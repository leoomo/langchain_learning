# è£…å¤‡å¯¹æ¯”åˆ†æç³»ç»Ÿå¼€å‘æŒ‡å—

## ğŸ“‹ æ¨¡å—æ¦‚è¿°

### ğŸ¯ ç³»ç»Ÿå®šä½
è£…å¤‡å¯¹æ¯”åˆ†æç³»ç»Ÿæ˜¯æ™ºèƒ½é’“é±¼ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒåˆ†æå·¥å…·ï¼Œä¸“æ³¨äºä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å®¢è§‚ã€è¯¦ç»†çš„è£…å¤‡å¯¹æ¯”åˆ†ææœåŠ¡ã€‚è¯¥ç³»ç»ŸåŸºäºå¤šç»´åº¦çš„è£…å¤‡å‚æ•°ã€æ€§èƒ½æ•°æ®ã€ç”¨æˆ·è¯„ä»·ç­‰ä¿¡æ¯ï¼Œé€šè¿‡æ™ºèƒ½ç®—æ³•è¿›è¡Œæ·±åº¦åˆ†æï¼Œå¸®åŠ©ç”¨æˆ·åšå‡ºæœ€ä¼˜çš„è£…å¤‡é€‰æ‹©å†³ç­–ã€‚

### ğŸ” æ ¸å¿ƒä»·å€¼ä¸»å¼ 
- **ä¸“ä¸šå¯¹æ¯”**: åŸºäºæŠ€æœ¯è§„æ ¼å’Œæ€§èƒ½æ•°æ®çš„å®¢è§‚å¯¹æ¯”åˆ†æ
- **å¤šç»´è¯„ä¼°**: ä»æ€§èƒ½ã€ä»·æ ¼ã€å“ç‰Œã€ç”¨æˆ·å£ç¢‘ç­‰å¤šä¸ªç»´åº¦ç»¼åˆè¯„ä¼°
- **å†³ç­–æ”¯æŒ**: æä¾›æ•°æ®é©±åŠ¨çš„è´­ä¹°å»ºè®®å’Œå‡çº§æŒ‡å¯¼
- **ä»·å€¼åˆ†æ**: æ·±åº¦åˆ†æè£…å¤‡çš„æ€§ä»·æ¯”å’Œé•¿æœŸä»·å€¼

### ğŸ—ï¸ ç³»ç»Ÿä¾èµ–
- **åŸºç¡€è®¾æ–½å±‚**: å…±äº«æ•°æ®åº“ç®¡ç†ã€ç¼“å­˜ç³»ç»Ÿã€é…ç½®ç®¡ç†
- **è£…å¤‡æ¨èç³»ç»Ÿ**: æä¾›è£…å¤‡åŸºç¡€ä¿¡æ¯å’Œæ¨èæ•°æ®
- **å¤–éƒ¨æ•°æ®æº**: è£…å¤‡è§„æ ¼æ•°æ®ã€ä»·æ ¼ä¿¡æ¯ã€ç”¨æˆ·è¯„ä»·ç­‰

## ğŸ¯ é¢†åŸŸæ¨¡å‹è®¾è®¡

### æ ¸å¿ƒå®ä½“æ¨¡å‹

#### 1. å¯¹æ¯”åˆ†æå®ä½“ (ComparisonAnalysis)
```python
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any, Tuple
from enum import Enum
from datetime import datetime

class ComparisonType(Enum):
    """å¯¹æ¯”ç±»å‹æšä¸¾"""
    HEAD_TO_HEAD = "head_to_head"              # æ­£é¢å¯¹æ¯”
    MULTI_COMPARISON = "multi_comparison"       # å¤šè£…å¤‡å¯¹æ¯”
    CATEGORY_ANALYSIS = "category_analysis"    # ç±»åˆ«åˆ†æ
    PRICE_RANGE = "price_range"                # ä»·æ ¼åŒºé—´å¯¹æ¯”
    BRAND_COMPARISON = "brand_comparison"      # å“ç‰Œå¯¹æ¯”

class AnalysisDimension(Enum):
    """åˆ†æç»´åº¦æšä¸¾"""
    PERFORMANCE = "performance"                # æ€§èƒ½ç»´åº¦
    PRICE = "price"                          # ä»·æ ¼ç»´åº¦
    USABILITY = "usability"                   # æ˜“ç”¨æ€§ç»´åº¦
    DURABILITY = "durability"                 # è€ç”¨æ€§ç»´åº¦
    BRAND_VALUE = "brand_value"              # å“ç‰Œä»·å€¼ç»´åº¦
    USER_SATISFACTION = "user_satisfaction"   # ç”¨æˆ·æ»¡æ„åº¦ç»´åº¦

@dataclass
class ComparisonMetric:
    """å¯¹æ¯”æŒ‡æ ‡"""
    name: str                                # æŒ‡æ ‡åç§°
    category: AnalysisDimension              # æŒ‡æ ‡ç±»åˆ«
    weight: float                           # æƒé‡ (0-1)
    unit: Optional[str] = None              # å•ä½
    description: str = ""                   # æè¿°
    higher_is_better: bool = True           # æ•°å€¼è¶Šå¤§è¶Šå¥½

@dataclass
class EquipmentComparison:
    """è£…å¤‡å¯¹æ¯”æ•°æ®"""
    equipment_id: str                       # è£…å¤‡ID
    metrics: Dict[str, float]               # æŒ‡æ ‡æ•°å€¼
    normalized_scores: Dict[str, float]     # æ ‡å‡†åŒ–è¯„åˆ†
    strengths: List[str]                    # ä¼˜åŠ¿é¡¹ç›®
    weaknesses: List[str]                   # åŠ£åŠ¿é¡¹ç›®
    overall_score: float                    # ç»¼åˆè¯„åˆ†

@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœ"""
    comparison_id: str                      # å¯¹æ¯”ID
    comparison_type: ComparisonType         # å¯¹æ¯”ç±»å‹
    equipment_comparisons: List[EquipmentComparison]  # è£…å¤‡å¯¹æ¯”æ•°æ®
    dimension_scores: Dict[AnalysisDimension, Dict[str, float]]  # ç»´åº¦è¯„åˆ†
    ranking: List[Tuple[str, float]]        # æ’åç»“æœ
    recommendation: str                     # æ¨èå»ºè®®
    key_differences: List[Dict[str, Any]]   # å…³é”®å·®å¼‚
    value_analysis: Dict[str, Any]          # ä»·å€¼åˆ†æ
    generated_at: datetime = field(default_factory=datetime.now)
```

#### 2. æ€§èƒ½è¯„åˆ†æ¨¡å‹ (PerformanceScore)
```python
@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†"""
    category: str                           # è£…å¤‡ç±»åˆ«
    level: str                             # æ€§èƒ½ç­‰çº§
    min_score: float                       # æœ€ä½åˆ†
    max_score: float                       # æœ€é«˜åˆ†
    description: str                       # æè¿°

@dataclass
class DetailedPerformanceScore:
    """è¯¦ç»†æ€§èƒ½è¯„åˆ†"""
    overall_score: float                    # ç»¼åˆè¯„åˆ†
    dimension_scores: Dict[AnalysisDimension, float]  # ç»´åº¦è¯„åˆ†
    sub_metrics: Dict[str, float]          # å­æŒ‡æ ‡è¯„åˆ†
    benchmark_comparison: Dict[str, str]   # åŸºå‡†å¯¹æ¯”
    confidence_level: float                # ç½®ä¿¡åº¦
    last_updated: datetime = field(default_factory=datetime.now)

class PerformanceScorer:
    """æ€§èƒ½è¯„åˆ†å™¨"""

    def __init__(self):
        self.benchmarks = self._initialize_benchmarks()
        self.scoring_weights = self._initialize_scoring_weights()
        self.normalization_methods = self._initialize_normalization_methods()

    def score_equipment(self, equipment: 'Equipment') -> DetailedPerformanceScore:
        """å¯¹è£…å¤‡è¿›è¡Œæ€§èƒ½è¯„åˆ†"""
        try:
            # æå–è£…å¤‡æŒ‡æ ‡
            raw_metrics = self._extract_metrics(equipment)

            # æ ‡å‡†åŒ–æŒ‡æ ‡
            normalized_metrics = self._normalize_metrics(raw_metrics, equipment.spec.category)

            # è®¡ç®—ç»´åº¦è¯„åˆ†
            dimension_scores = self._calculate_dimension_scores(normalized_metrics)

            # è®¡ç®—ç»¼åˆè¯„åˆ†
            overall_score = self._calculate_overall_score(dimension_scores)

            # åŸºå‡†å¯¹æ¯”
            benchmark_comparison = self._compare_to_benchmarks(
                overall_score, equipment.spec.category, equipment.spec.level
            )

            return DetailedPerformanceScore(
                overall_score=overall_score,
                dimension_scores=dimension_scores,
                sub_metrics=normalized_metrics,
                benchmark_comparison=benchmark_comparison,
                confidence_level=self._calculate_confidence_level(equipment)
            )

        except Exception as e:
            logger.error(f"è£…å¤‡æ€§èƒ½è¯„åˆ†å¤±è´¥: {e}")
            return DetailedPerformanceScore(
                overall_score=50.0,
                dimension_scores={},
                sub_metrics={},
                benchmark_comparison={},
                confidence_level=0.0
            )

    def _extract_metrics(self, equipment: 'Equipment') -> Dict[str, float]:
        """æå–è£…å¤‡æŒ‡æ ‡"""
        metrics = {}

        # åŸºç¡€æ€§èƒ½æŒ‡æ ‡
        metrics['performance_score'] = equipment.performance_score
        metrics['popularity_score'] = equipment.popularity_score
        metrics['user_rating'] = equipment.user_rating * 20  # è½¬æ¢ä¸º100åˆ†åˆ¶

        # ä»·æ ¼ç›¸å…³æŒ‡æ ‡
        avg_price = self._get_category_average_price(equipment.spec.category)
        metrics['price competitiveness'] = 1 - (equipment.spec.price / avg_price - 1) * 0.5
        metrics['price competitiveness'] = max(0, min(100, metrics['price competitiveness'] * 100))

        # è¯„ä»·æ•°é‡æŒ‡æ ‡
        metrics['review_confidence'] = min(100, equipment.review_count / 10)  # æ¯10ä¸ªè¯„ä»·åŠ 1åˆ†ï¼Œæœ€é«˜100åˆ†

        # ä»è§„æ ¼å‚æ•°ä¸­æå–å…·ä½“æŒ‡æ ‡
        specs = equipment.spec.specifications
        category = equipment.spec.category

        if category == EquipmentCategory.FISHING_ROD:
            metrics.update(self._extract_rod_metrics(specs))
        elif category == EquipmentCategory.FISHING_REEL:
            metrics.update(self._extract_reel_metrics(specs))
        elif category == EquipmentCategory.FISHING_LINE:
            metrics.update(self._extract_line_metrics(specs))
        elif category == EquipmentCategory.FISHING_LURE:
            metrics.update(self._extract_lure_metrics(specs))

        return metrics

    def _extract_rod_metrics(self, specs: Dict[str, Any]) -> Dict[str, float]:
        """æå–é±¼ç«¿æŒ‡æ ‡"""
        metrics = {}

        # è°ƒæ€§è¯„åˆ†
        if 'action' in specs:
            action_scores = {
                'ultralight': 30, 'light': 50, 'medium_light': 70,
                'medium': 85, 'medium_heavy': 90, 'heavy': 75, 'extra_heavy': 60
            }
            metrics['action_score'] = action_scores.get(specs['action'].lower(), 50)

        # é•¿åº¦è¯„åˆ†
        if 'length' in specs:
            length = specs['length']
            # ç†æƒ³é•¿åº¦èŒƒå›´2.1-2.7ç±³
            if 210 <= length <= 270:
                metrics['length_score'] = 100
            elif 180 <= length < 210 or 270 < length <= 300:
                metrics['length_score'] = 85
            else:
                metrics['length_score'] = 70

        # æè´¨è¯„åˆ†
        if 'material' in specs:
            material_scores = {
                'carbon': 95, 'carbon_fiber': 95, 'glass': 60, 'composite': 75,
                'bamboo': 40, 'fiberglass': 55
            }
            material = specs['material'].lower()
            metrics['material_score'] = max(30, material_scores.get(material, 50))

        # é‡é‡è¯„åˆ†
        if 'weight' in specs:
            weight = specs['weight']
            # è½»é‡åŒ–è¯„åˆ†ï¼Œè¶Šè½»è¶Šå¥½
            metrics['weight_score'] = max(30, 100 - (weight - 50) * 0.5)

        return metrics

    def _extract_reel_metrics(self, specs: Dict[str, Any]) -> Dict[str, float]:
        """æå–é±¼è½®æŒ‡æ ‡"""
        metrics = {}

        # è½´æ‰¿æ•°é‡è¯„åˆ†
        if 'bearings' in specs:
            bearings = specs['bearings']
            if bearings >= 10:
                metrics['bearing_score'] = 100
            elif bearings >= 7:
                metrics['bearing_score'] = 85
            elif bearings >= 4:
                metrics['bearing_score'] = 70
            else:
                metrics['bearing_score'] = 50

        # çº¿å®¹é‡è¯„åˆ†
        if 'line_capacity' in specs:
            # æ ¹æ®çº¿å®¹é‡èŒƒå›´è¯„åˆ†
            metrics['capacity_score'] = 80  # ç®€åŒ–è¯„åˆ†

        # æè´¨è¯„åˆ†
        if 'body_material' in specs:
            material_scores = {
                'aluminum': 85, 'carbon': 95, 'graphite': 90,
                'plastic': 50, 'metal': 75, 'titanium': 100
            }
            material = specs['body_material'].lower()
            metrics['body_material_score'] = max(40, material_scores.get(material, 60))

        # é½¿è½®æ¯”è¯„åˆ†
        if 'gear_ratio' in specs:
            gear_ratio = specs['gear_ratio']
            # ç†æƒ³é½¿è½®æ¯”èŒƒå›´5.0-6.0
            if 5.0 <= gear_ratio <= 6.0:
                metrics['gear_ratio_score'] = 100
            elif 4.5 <= gear_ratio < 5.0 or 6.0 < gear_ratio <= 7.0:
                metrics['gear_ratio_score'] = 85
            else:
                metrics['gear_ratio_score'] = 70

        return metrics

    def _extract_line_metrics(self, specs: Dict[str, Any]) -> Dict[str, float]:
        """æå–é±¼çº¿æŒ‡æ ‡"""
        metrics = {}

        # å¼ºåº¦è¯„åˆ†
        if 'strength' in specs:
            strength = specs['strength']
            # æ ¹æ®å¼ºåº¦èŒƒå›´è¯„åˆ†
            metrics['strength_score'] = min(100, strength / 10)  # ç®€åŒ–è¯„åˆ†

        # ç›´å¾„è¯„åˆ†
        if 'diameter' in specs:
            diameter = specs['diameter']
            # çº¿å¾„è¯„åˆ†ï¼Œè¶Šç»†è¶Šå¥½ï¼ˆåœ¨åŒç­‰å¼ºåº¦ä¸‹ï¼‰
            metrics['diameter_score'] = max(50, 100 - (diameter - 0.2) * 100)

        # æè´¨è¯„åˆ†
        if 'material' in specs:
            material_scores = {
                'pe': 95, 'nylon': 75, 'fluorocarbon': 85,
                'braided': 90, 'mono': 70, 'copolymer': 80
            }
            material = specs['material'].lower()
            metrics['material_score'] = max(50, material_scores.get(material, 60))

        return metrics

    def _extract_lure_metrics(self, specs: Dict[str, Any]) -> Dict[str, float]:
        """æå–æ‹Ÿé¥µæŒ‡æ ‡"""
        metrics = {}

        # é‡é‡è¯„åˆ†
        if 'weight' in specs:
            weight = specs['weight']
            # æ ¹æ®é‡é‡èŒƒå›´è¯„åˆ†
            if 3 <= weight <= 15:  # ç†æƒ³é‡é‡èŒƒå›´
                metrics['weight_score'] = 100
            elif 1 <= weight < 3 or 15 < weight <= 25:
                metrics['weight_score'] = 80
            else:
                metrics['weight_score'] = 60

        # ç±»å‹è¯„åˆ†
        if 'type' in specs:
            type_scores = {
                'minnow': 90, 'crankbait': 85, 'spinnerbait': 80,
                'jig': 85, 'soft_plastic': 75, 'topwater': 90,
                'vibrating': 80, 'spoon': 70
            }
            lure_type = specs['type'].lower()
            metrics['type_score'] = type_scores.get(lure_type, 70)

        return metrics

    def _normalize_metrics(self, metrics: Dict[str, float], category: str) -> Dict[str, float]:
        """æ ‡å‡†åŒ–æŒ‡æ ‡"""
        normalized = {}

        for metric_name, value in metrics.items():
            if metric_name in self.normalization_methods[category]:
                method = self.normalization_methods[category][metric_name]
                normalized[metric_name] = self._apply_normalization(value, method)
            else:
                # é»˜è®¤æ ‡å‡†åŒ–æ–¹æ³•
                normalized[metric_name] = max(0, min(100, value))

        return normalized

    def _apply_normalization(self, value: float, method: Dict[str, Any]) -> float:
        """åº”ç”¨æ ‡å‡†åŒ–æ–¹æ³•"""
        normalization_type = method.get('type', 'linear')

        if normalization_type == 'linear':
            min_val = method.get('min', 0)
            max_val = method.get('max', 100)
            return ((value - min_val) / (max_val - min_val)) * 100
        elif normalization_type == 'logarithmic':
            return min(100, math.log(value + 1) * method.get('scale', 20))
        elif normalization_type == 'inverse':
            optimal = method.get('optimal', 50)
            deviation = abs(value - optimal)
            max_deviation = method.get('max_deviation', 50)
            return max(0, 100 - (deviation / max_deviation) * 100)

        return max(0, min(100, value))

    def _calculate_dimension_scores(self, metrics: Dict[str, float]) -> Dict[AnalysisDimension, float]:
        """è®¡ç®—ç»´åº¦è¯„åˆ†"""
        dimension_scores = {}

        for dimension, weight in self.scoring_weights.items():
            # è·å–è¯¥ç»´åº¦ç›¸å…³çš„æŒ‡æ ‡
            dimension_metrics = self._get_dimension_metrics(dimension)

            if dimension_metrics:
                # è®¡ç®—ç»´åº¦å¹³å‡åˆ†
                relevant_scores = [
                    metrics[metric] for metric in dimension_metrics
                    if metric in metrics
                ]

                if relevant_scores:
                    dimension_scores[dimension] = sum(relevant_scores) / len(relevant_scores)
                else:
                    dimension_scores[dimension] = 50.0  # é»˜è®¤åˆ†æ•°
            else:
                dimension_scores[dimension] = 50.0

        return dimension_scores

    def _get_dimension_metrics(self, dimension: AnalysisDimension) -> List[str]:
        """è·å–ç»´åº¦ç›¸å…³æŒ‡æ ‡"""
        dimension_mapping = {
            AnalysisDimension.PERFORMANCE: ['performance_score', 'action_score', 'bearing_score', 'strength_score'],
            AnalysisDimension.PRICE: ['price competitiveness'],
            AnalysisDimension.USABILITY: ['length_score', 'weight_score', 'gear_ratio_score'],
            AnalysisDimension.DURABILITY: ['material_score', 'body_material_score'],
            AnalysisDimension.BRAND_VALUE: ['popularity_score'],
            AnalysisDimension.USER_SATISFACTION: ['user_rating', 'review_confidence']
        }
        return dimension_mapping.get(dimension, [])

    def _calculate_overall_score(self, dimension_scores: Dict[AnalysisDimension, float]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        total_score = 0.0
        total_weight = 0.0

        for dimension, score in dimension_scores.items():
            weight = self.scoring_weights.get(dimension, 0.1)
            total_score += score * weight
            total_weight += weight

        return total_score / total_weight if total_weight > 0 else 50.0

    def _compare_to_benchmarks(self, score: float, category: str, level: str) -> Dict[str, str]:
        """ä¸åŸºå‡†å¯¹æ¯”"""
        benchmarks = self.benchmarks.get(category, [])

        for benchmark in benchmarks:
            if benchmark.level.lower() == level.lower():
                if score >= benchmark.max_score:
                    return {'level': 'excellent', 'description': 'è¶…è¶ŠåŒçº§äº§å“'}
                elif score >= (benchmark.min_score + benchmark.max_score) / 2:
                    return {'level': 'good', 'description': 'ä¼˜äºåŒçº§äº§å“'}
                elif score >= benchmark.min_score:
                    return {'level': 'average', 'description': 'ç¬¦åˆåŒçº§æ ‡å‡†'}
                else:
                    return {'level': 'below_average', 'description': 'ä½äºåŒçº§æ ‡å‡†'}

        return {'level': 'unknown', 'description': 'æ— æ³•è¿›è¡ŒåŸºå‡†å¯¹æ¯”'}

    def _calculate_confidence_level(self, equipment: 'Equipment') -> float:
        """è®¡ç®—ç½®ä¿¡åº¦"""
        confidence_factors = []

        # è¯„ä»·æ•°é‡å› å­
        review_confidence = min(1.0, equipment.review_count / 100)
        confidence_factors.append(review_confidence)

        # æ•°æ®å®Œæ•´æ€§å› å­
        spec_completeness = len(equipment.spec.specifications) / 10  # å‡è®¾10ä¸ªå…³é”®è§„æ ¼
        confidence_factors.append(min(1.0, spec_completeness))

        # ç”¨æˆ·è¯„åˆ†ä¸€è‡´æ€§å› å­
        if equipment.user_rating > 0:
            # ç®€åŒ–çš„ä¸€è‡´æ€§è®¡ç®—
            rating_consistency = 0.8
        else:
            rating_consistency = 0.3
        confidence_factors.append(rating_consistency)

        return sum(confidence_factors) / len(confidence_factors)

    def _initialize_benchmarks(self) -> Dict[str, List[PerformanceBenchmark]]:
        """åˆå§‹åŒ–æ€§èƒ½åŸºå‡†"""
        return {
            'fishing_rod': [
                PerformanceBenchmark('fishing_rod', 'beginner', 60, 75, 'å…¥é—¨çº§é±¼ç«¿'),
                PerformanceBenchmark('fishing_rod', 'intermediate', 75, 85, 'è¿›é˜¶çº§é±¼ç«¿'),
                PerformanceBenchmark('fishing_rod', 'advanced', 85, 92, 'é«˜çº§é±¼ç«¿'),
                PerformanceBenchmark('fishing_rod', 'professional', 92, 98, 'ä¸“ä¸šçº§é±¼ç«¿'),
            ],
            'fishing_reel': [
                PerformanceBenchmark('fishing_reel', 'beginner', 65, 78, 'å…¥é—¨çº§é±¼è½®'),
                PerformanceBenchmark('fishing_reel', 'intermediate', 78, 88, 'è¿›é˜¶çº§é±¼è½®'),
                PerformanceBenchmark('fishing_reel', 'advanced', 88, 94, 'é«˜çº§é±¼è½®'),
                PerformanceBenchmark('fishing_reel', 'professional', 94, 99, 'ä¸“ä¸šçº§é±¼è½®'),
            ],
            # å…¶ä»–ç±»åˆ«åŸºå‡†...
        }

    def _initialize_scoring_weights(self) -> Dict[AnalysisDimension, float]:
        """åˆå§‹åŒ–è¯„åˆ†æƒé‡"""
        return {
            AnalysisDimension.PERFORMANCE: 0.30,
            AnalysisDimension.PRICE: 0.20,
            AnalysisDimension.USABILITY: 0.15,
            AnalysisDimension.DURABILITY: 0.15,
            AnalysisDimension.BRAND_VALUE: 0.10,
            AnalysisDimension.USER_SATISFACTION: 0.10
        }

    def _initialize_normalization_methods(self) -> Dict[str, Dict[str, Dict[str, Any]]]:
        """åˆå§‹åŒ–æ ‡å‡†åŒ–æ–¹æ³•"""
        return {
            'fishing_rod': {
                'action_score': {'type': 'linear', 'min': 0, 'max': 100},
                'length_score': {'type': 'linear', 'min': 0, 'max': 100},
                'material_score': {'type': 'linear', 'min': 0, 'max': 100},
                'weight_score': {'type': 'linear', 'min': 0, 'max': 100},
            },
            'fishing_reel': {
                'bearing_score': {'type': 'linear', 'min': 0, 'max': 100},
                'capacity_score': {'type': 'linear', 'min': 0, 'max': 100},
                'body_material_score': {'type': 'linear', 'min': 0, 'max': 100},
                'gear_ratio_score': {'type': 'linear', 'min': 0, 'max': 100},
            },
            # å…¶ä»–ç±»åˆ«æ ‡å‡†åŒ–æ–¹æ³•...
        }

    def _get_category_average_price(self, category: str) -> float:
        """è·å–ç±»åˆ«å¹³å‡ä»·æ ¼"""
        price_ranges = {
            'fishing_rod': 500,
            'fishing_reel': 400,
            'fishing_line': 50,
            'fishing_lure': 30,
            'fishing_hook': 20,
            'accessories': 100
        }
        return price_ranges.get(category, 200)
```

## ğŸ”§ æœåŠ¡å±‚å®ç°

### å¯¹æ¯”åˆ†æå¼•æ“ (ComparisonEngine)
```python
class ComparisonEngine:
    """å¯¹æ¯”åˆ†æå¼•æ“"""

    def __init__(self, equipment_repository: 'EquipmentRepository',
                 performance_scorer: PerformanceScorer,
                 spec_analyzer: 'SpecificationAnalyzer',
                 upgrade_advisor: 'UpgradeAdvisor'):
        self.equipment_repository = equipment_repository
        self.performance_scorer = performance_scorer
        self.spec_analyzer = spec_analyzer
        self.upgrade_advisor = upgrade_advisor
        self.comparison_cache = ComparisonCache()

    def compare_equipment(self, equipment_ids: List[str],
                         comparison_type: ComparisonType = ComparisonType.HEAD_TO_HEAD) -> ComparisonResult:
        """å¯¹æ¯”è£…å¤‡"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(equipment_ids, comparison_type)
            cached_result = self.comparison_cache.get(cache_key)
            if cached_result:
                return cached_result

            # è·å–è£…å¤‡ä¿¡æ¯
            equipments = []
            for equipment_id in equipment_ids:
                equipment = self.equipment_repository.find_by_id(equipment_id)
                if equipment:
                    equipments.append(equipment)
                else:
                    raise ValueError(f"è£…å¤‡ {equipment_id} ä¸å­˜åœ¨")

            if len(equipments) < 2:
                raise ValueError("è‡³å°‘éœ€è¦2ä¸ªè£…å¤‡è¿›è¡Œå¯¹æ¯”")

            # ç”Ÿæˆå¯¹æ¯”ID
            comparison_id = self._generate_comparison_id(equipment_ids)

            # è®¡ç®—æ€§èƒ½è¯„åˆ†
            equipment_comparisons = []
            for equipment in equipments:
                performance_score = self.performance_scorer.score_equipment(equipment)
                comparison_data = self._create_equipment_comparison(equipment, performance_score)
                equipment_comparisons.append(comparison_data)

            # åˆ†æç»´åº¦è¯„åˆ†
            dimension_scores = self._analyze_dimension_scores(equipment_comparisons)

            # ç”Ÿæˆæ’å
            ranking = self._generate_ranking(equipment_comparisons)

            # åˆ†æå…³é”®å·®å¼‚
            key_differences = self._analyze_key_differences(equipments, equipment_comparisons)

            # ä»·å€¼åˆ†æ
            value_analysis = self._analyze_value(equipment_comparisons)

            # ç”Ÿæˆæ¨èå»ºè®®
            recommendation = self._generate_recommendation(
                equipment_comparisons, dimension_scores, comparison_type
            )

            # åˆ›å»ºå¯¹æ¯”ç»“æœ
            result = ComparisonResult(
                comparison_id=comparison_id,
                comparison_type=comparison_type,
                equipment_comparisons=equipment_comparisons,
                dimension_scores=dimension_scores,
                ranking=ranking,
                recommendation=recommendation,
                key_differences=key_differences,
                value_analysis=value_analysis
            )

            # ç¼“å­˜ç»“æœ
            self.comparison_cache.set(cache_key, result)

            return result

        except Exception as e:
            logger.error(f"è£…å¤‡å¯¹æ¯”åˆ†æå¤±è´¥: {e}")
            raise ComparisonError(f"æ— æ³•å®Œæˆè£…å¤‡å¯¹æ¯”: {e}")

    def compare_multiple_equipment(self, category: str,
                                 filters: Dict[str, Any] = None,
                                 limit: int = 10) -> ComparisonResult:
        """å¯¹æ¯”å¤šæ¬¾è£…å¤‡"""
        try:
            # è·å–è£…å¤‡åˆ—è¡¨
            if filters:
                equipments = self.equipment_repository.search_equipment("", filters)
            else:
                equipments = self.equipment_repository.find_by_category(EquipmentCategory(category))

            # é™åˆ¶æ•°é‡
            equipments = equipments[:limit]

            if len(equipments) < 2:
                raise ValueError("æ‰¾åˆ°çš„è£…å¤‡æ•°é‡ä¸è¶³")

            equipment_ids = [equipment.id for equipment in equipments]
            return self.compare_equipment(
                equipment_ids, ComparisonType.MULTI_COMPARISON
            )

        except Exception as e:
            logger.error(f"å¤šè£…å¤‡å¯¹æ¯”å¤±è´¥: {e}")
            raise ComparisonError(f"æ— æ³•è¿›è¡Œå¤šè£…å¤‡å¯¹æ¯”: {e}")

    def analyze_upgrade_value(self, current_equipment_id: str,
                            target_equipment_id: str) -> Dict[str, Any]:
        """åˆ†æå‡çº§ä»·å€¼"""
        try:
            current_equipment = self.equipment_repository.find_by_id(current_equipment_id)
            target_equipment = self.equipment_repository.find_by_id(target_equipment_id)

            if not current_equipment or not target_equipment:
                raise ValueError("è£…å¤‡ä¸å­˜åœ¨")

            # è®¡ç®—æ€§èƒ½æå‡
            current_score = self.performance_scorer.score_equipment(current_equipment)
            target_score = self.performance_scorer.score_equipment(target_equipment)

            performance_improvement = target_score.overall_score - current_score.overall_score

            # åˆ†æè§„æ ¼å·®å¼‚
            spec_differences = self.spec_analyzer.compare_specifications(
                current_equipment.spec, target_equipment.spec
            )

            # è®¡ç®—æ€§ä»·æ¯”
            price_difference = target_equipment.spec.price - current_equipment.spec.price
            value_ratio = performance_improvement / (price_difference / 100) if price_difference > 0 else float('inf')

            # ç”Ÿæˆå‡çº§å»ºè®®
            upgrade_recommendation = self.upgrade_advisor.analyze_upgrade(
                current_equipment, target_equipment, performance_improvement, price_difference
            )

            return {
                'current_equipment': {
                    'id': current_equipment.id,
                    'name': f"{current_equipment.spec.brand} {current_equipment.spec.model}",
                    'overall_score': current_score.overall_score
                },
                'target_equipment': {
                    'id': target_equipment.id,
                    'name': f"{target_equipment.spec.brand} {target_equipment.spec.model}",
                    'overall_score': target_score.overall_score
                },
                'performance_improvement': performance_improvement,
                'price_difference': price_difference,
                'value_ratio': value_ratio,
                'spec_differences': spec_differences,
                'upgrade_recommendation': upgrade_recommendation,
                'is_recommended': performance_improvement > 10 and value_ratio > 0.5
            }

        except Exception as e:
            logger.error(f"å‡çº§ä»·å€¼åˆ†æå¤±è´¥: {e}")
            raise ComparisonError(f"æ— æ³•åˆ†æå‡çº§ä»·å€¼: {e}")

    def _create_equipment_comparison(self, equipment: 'Equipment',
                                   performance_score: DetailedPerformanceScore) -> EquipmentComparison:
        """åˆ›å»ºè£…å¤‡å¯¹æ¯”æ•°æ®"""
        # æå–æŒ‡æ ‡
        metrics = self.performance_scorer._extract_metrics(equipment)

        # è¯†åˆ«ä¼˜åŠ¿å’ŒåŠ£åŠ¿
        strengths, weaknesses = self._identify_strengths_weaknesses(
            metrics, performance_score.dimension_scores
        )

        return EquipmentComparison(
            equipment_id=equipment.id,
            metrics=metrics,
            normalized_scores=performance_score.sub_metrics,
            strengths=strengths,
            weaknesses=weaknesses,
            overall_score=performance_score.overall_score
        )

    def _identify_strengths_weaknesses(self, metrics: Dict[str, float],
                                     dimension_scores: Dict[AnalysisDimension, float]) -> Tuple[List[str], List[str]]:
        """è¯†åˆ«ä¼˜åŠ¿å’ŒåŠ£åŠ¿"""
        strengths = []
        weaknesses = []

        # åŸºäºç»´åº¦è¯„åˆ†è¯†åˆ«
        for dimension, score in dimension_scores.items():
            if score >= 85:
                strengths.append(f"{dimension.value}è¡¨ç°å‡ºè‰² (è¯„åˆ†: {score:.1f})")
            elif score <= 60:
                weaknesses.append(f"{dimension.value}æœ‰å¾…æ”¹è¿› (è¯„åˆ†: {score:.1f})")

        # åŸºäºå…·ä½“æŒ‡æ ‡è¯†åˆ«
        for metric, value in metrics.items():
            if value >= 90:
                strengths.append(f"{metric}æŒ‡æ ‡ä¼˜ç§€ (æ•°å€¼: {value:.1f})")
            elif value <= 40:
                weaknesses.append(f"{metric}æŒ‡æ ‡è¾ƒä½ (æ•°å€¼: {value:.1f})")

        return strengths[:3], weaknesses[:3]  # æœ€å¤šè¿”å›3ä¸ª

    def _analyze_dimension_scores(self, equipment_comparisons: List[EquipmentComparison]) -> Dict[AnalysisDimension, Dict[str, float]]:
        """åˆ†æç»´åº¦è¯„åˆ†"""
        dimension_scores = {}

        # æ”¶é›†æ‰€æœ‰ç»´åº¦
        all_dimensions = set()
        for comparison in equipment_comparisons:
            # è¿™é‡Œéœ€è¦ä»metricsä¸­åæ¨ç»´åº¦ï¼Œç®€åŒ–å¤„ç†
            for metric_name in comparison.metrics.keys():
                dimension = self._map_metric_to_dimension(metric_name)
                if dimension:
                    all_dimensions.add(dimension)

        # ä¸ºæ¯ä¸ªç»´åº¦è®¡ç®—è¯„åˆ†
        for dimension in all_dimensions:
            dimension_scores[dimension] = {}
            for comparison in equipment_comparisons:
                score = self._calculate_dimension_score_for_equipment(comparison, dimension)
                dimension_scores[dimension][comparison.equipment_id] = score

        return dimension_scores

    def _map_metric_to_dimension(self, metric_name: str) -> Optional[AnalysisDimension]:
        """å°†æŒ‡æ ‡æ˜ å°„åˆ°ç»´åº¦"""
        metric_mapping = {
            'performance_score': AnalysisDimension.PERFORMANCE,
            'price competitiveness': AnalysisDimension.PRICE,
            'user_rating': AnalysisDimension.USER_SATISFACTION,
            'popularity_score': AnalysisDimension.BRAND_VALUE,
            'material_score': AnalysisDimension.DURABILITY,
            'action_score': AnalysisDimension.PERFORMANCE,
            'bearing_score': AnalysisDimension.PERFORMANCE,
            'weight_score': AnalysisDimension.USABILITY,
        }
        return metric_mapping.get(metric_name)

    def _calculate_dimension_score_for_equipment(self, comparison: EquipmentComparison,
                                               dimension: AnalysisDimension) -> float:
        """è®¡ç®—è£…å¤‡çš„ç‰¹å®šç»´åº¦è¯„åˆ†"""
        # è·å–è¯¥ç»´åº¦ç›¸å…³çš„æŒ‡æ ‡
        dimension_metrics = self.performance_scorer._get_dimension_metrics(dimension)

        relevant_scores = []
        for metric in dimension_metrics:
            if metric in comparison.normalized_scores:
                relevant_scores.append(comparison.normalized_scores[metric])

        if relevant_scores:
            return sum(relevant_scores) / len(relevant_scores)
        else:
            return 50.0  # é»˜è®¤åˆ†æ•°

    def _generate_ranking(self, equipment_comparisons: List[EquipmentComparison]) -> List[Tuple[str, float]]:
        """ç”Ÿæˆæ’å"""
        sorted_comparisons = sorted(
            equipment_comparisons,
            key=lambda x: x.overall_score,
            reverse=True
        )

        return [(comp.equipment_id, comp.overall_score) for comp in sorted_comparisons]

    def _analyze_key_differences(self, equipments: List['Equipment'],
                               equipment_comparisons: List[EquipmentComparison]) -> List[Dict[str, Any]]:
        """åˆ†æå…³é”®å·®å¼‚"""
        key_differences = []

        if len(equipments) != 2:
            return key_differences

        eq1, eq2 = equipments[0], equipments[1]
        comp1, comp2 = equipment_comparisons[0], equipment_comparisons[1]

        # ä»·æ ¼å·®å¼‚
        price_diff = abs(eq1.spec.price - eq2.spec.price)
        if price_diff > 100:  # ä»·æ ¼å·®å¼‚è¶…è¿‡100å…ƒ
            key_differences.append({
                'type': 'price',
                'description': f"ä»·æ ¼å·®å¼‚ {price_diff:.0f} å…ƒ",
                'impact': 'high' if price_diff > 500 else 'medium'
            })

        # æ€§èƒ½å·®å¼‚
        performance_diff = abs(comp1.overall_score - comp2.overall_score)
        if performance_diff > 10:  # æ€§èƒ½å·®å¼‚è¶…è¿‡10åˆ†
            key_differences.append({
                'type': 'performance',
                'description': f"æ€§èƒ½è¯„åˆ†å·®å¼‚ {performance_diff:.1f} åˆ†",
                'impact': 'high' if performance_diff > 20 else 'medium'
            })

        # å“ç‰Œå·®å¼‚
        if eq1.spec.brand != eq2.spec.brand:
            key_differences.append({
                'type': 'brand',
                'description': f"å“ç‰Œå·®å¼‚ï¼š{eq1.spec.brand} vs {eq2.spec.brand}",
                'impact': 'medium'
            })

        # è§„æ ¼å·®å¼‚
        spec_diffs = self.spec_analyzer.identify_key_spec_differences(
            eq1.spec.specifications, eq2.spec.specifications
        )
        for diff in spec_diffs[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ªå…³é”®è§„æ ¼å·®å¼‚
            key_differences.append({
                'type': 'specification',
                'description': diff,
                'impact': 'medium'
            })

        return key_differences

    def _analyze_value(self, equipment_comparisons: List[EquipmentComparison]) -> Dict[str, Any]:
        """åˆ†æä»·å€¼"""
        total_scores = [comp.overall_score for comp in equipment_comparisons]
        avg_score = sum(total_scores) / len(total_scores)

        # ä»·å€¼ç­‰çº§åˆ†æ
        value_analysis = {}
        for comp in equipment_comparisons:
            equipment = self.equipment_repository.find_by_id(comp.equipment_id)
            if equipment:
                price = equipment.spec.price
                value_score = comp.overall_score / (price / 100)  # æ¯100å…ƒçš„æ€§èƒ½è¯„åˆ†

                value_analysis[comp.equipment_id] = {
                    'value_score': value_score,
                    'value_rank': 'high' if value_score > avg_score / (price / 100) * 1.2 else 'medium' if value_score > avg_score / (price / 100) * 0.8 else 'low',
                    'price_performance_ratio': f"{value_score:.2f}"
                }

        return {
            'individual_values': value_analysis,
            'best_value_equipment': max(value_analysis.keys(), key=lambda k: value_analysis[k]['value_score']),
            'value_range': {
                'min': min(v['value_score'] for v in value_analysis.values()),
                'max': max(v['value_score'] for v in value_analysis.values()),
                'average': sum(v['value_score'] for v in value_analysis.values()) / len(value_analysis)
            }
        }

    def _generate_recommendation(self, equipment_comparisons: List[EquipmentComparison],
                               dimension_scores: Dict[AnalysisDimension, Dict[str, float]],
                               comparison_type: ComparisonType) -> str:
        """ç”Ÿæˆæ¨èå»ºè®®"""
        if not equipment_comparisons:
            return "æ— æ³•ç”Ÿæˆæ¨èå»ºè®®"

        # æ‰¾å‡ºæœ€ä½³è£…å¤‡
        best_comparison = max(equipment_comparisons, key=lambda x: x.overall_score)
        best_equipment = self.equipment_repository.find_by_id(best_comparison.equipment_id)

        if not best_equipment:
            return "æ— æ³•è·å–è£…å¤‡ä¿¡æ¯"

        # åˆ†ææ¨èç†ç”±
        reasons = []

        # ç»¼åˆæ€§èƒ½ä¼˜åŠ¿
        if best_comparison.overall_score >= 90:
            reasons.append("ç»¼åˆæ€§èƒ½è¡¨ç°å“è¶Š")
        elif best_comparison.overall_score >= 80:
            reasons.append("ç»¼åˆæ€§èƒ½è¡¨ç°ä¼˜ç§€")

        # ä¼˜åŠ¿é¡¹ç›®
        if best_comparison.strengths:
            reasons.append(f"åœ¨ {best_comparison.strengths[0].split('(')[0].strip()} æ–¹é¢è¡¨ç°çªå‡º")

        # æ€§ä»·æ¯”ä¼˜åŠ¿
        value_scores = [comp.overall_score / (self.equipment_repository.find_by_id(comp.equipment_id).spec.price / 100)
                       for comp in equipment_comparisons
                       if self.equipment_repository.find_by_id(comp.equipment_id)]

        if value_scores:
            best_value = max(value_scores)
            current_value = best_comparison.overall_score / (best_equipment.spec.price / 100)
            if current_value >= best_value * 0.95:
                reasons.append("å…·æœ‰è‰¯å¥½çš„æ€§ä»·æ¯”")

        # ç”Ÿæˆæ¨èæ–‡æœ¬
        recommendation = f"æ¨èé€‰æ‹© {best_equipment.spec.brand} {best_equipment.spec.model}"

        if reasons:
            recommendation += f"ï¼Œå› ä¸º{'ï¼Œ'.join(reasons[:2])}"  # æœ€å¤š2ä¸ªç†ç”±

        # æ ¹æ®å¯¹æ¯”ç±»å‹æ·»åŠ ç‰¹å®šå»ºè®®
        if comparison_type == ComparisonType.HEAD_TO_HEAD:
            recommendation += "ã€‚åœ¨ç›´æ¥å¯¹æ¯”ä¸­ï¼Œè¿™æ¬¾è£…å¤‡å±•ç°å‡ºæ˜æ˜¾ä¼˜åŠ¿ã€‚"
        elif comparison_type == ComparisonType.MULTI_COMPARISON:
            recommendation += f"ã€‚åœ¨ {len(equipment_comparisons)} æ¬¾è£…å¤‡ä¸­è„±é¢–è€Œå‡ºã€‚"

        return recommendation

    def _generate_cache_key(self, equipment_ids: List[str], comparison_type: ComparisonType) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        sorted_ids = sorted(equipment_ids)
        return f"comparison:{'_'.join(sorted_ids)}:{comparison_type.value}"

    def _generate_comparison_id(self, equipment_ids: List[str]) -> str:
        """ç”Ÿæˆå¯¹æ¯”ID"""
        sorted_ids = sorted(equipment_ids)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"cmp_{timestamp}_{'_'.join(sorted_ids[:3])}"  # æœ€å¤š3ä¸ªID

class ComparisonError(Exception):
    """å¯¹æ¯”åˆ†æå¼‚å¸¸"""
    pass
```

### è§„æ ¼åˆ†æå™¨ (SpecificationAnalyzer)
```python
class SpecificationAnalyzer:
    """è§„æ ¼åˆ†æå™¨"""

    def __init__(self):
        self.comparison_rules = self._initialize_comparison_rules()
        self.spec_weights = self._initialize_spec_weights()

    def compare_specifications(self, spec1: EquipmentSpec, spec2: EquipmentSpec) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸¤ä¸ªè£…å¤‡çš„è§„æ ¼"""
        if spec1.category != spec2.category:
            raise ValueError("åªèƒ½å¯¹æ¯”åŒç±»åˆ«è£…å¤‡çš„è§„æ ¼")

        comparison_result = {
            'category': spec1.category.value,
            'differences': [],
            'similarities': [],
            'winner_metrics': {},
            'overall_similarity': 0.0
        }

        # è·å–è¯¥ç±»åˆ«çš„å¯¹æ¯”è§„åˆ™
        rules = self.comparison_rules.get(spec1.category.value, {})

        # å¯¹æ¯”å…³é”®è§„æ ¼
        all_specs = set(spec1.specifications.keys()) | set(spec2.specifications.keys())

        similarity_scores = []

        for spec_key in all_specs:
            if spec_key in rules:
                spec_comparison = self._compare_spec_value(
                    spec_key,
                    spec1.specifications.get(spec_key),
                    spec2.specifications.get(spec_key),
                    rules[spec_key]
                )

                comparison_result['differences'].append(spec_comparison)

                if spec_comparison['similarity'] > 0.8:
                    comparison_result['similarities'].append(spec_key)

                if spec_comparison.get('winner'):
                    comparison_result['winner_metrics'][spec_key] = spec_comparison['winner']

                similarity_scores.append(spec_comparison['similarity'])

        # è®¡ç®—æ€»ä½“ç›¸ä¼¼åº¦
        if similarity_scores:
            comparison_result['overall_similarity'] = sum(similarity_scores) / len(similarity_scores)

        # æ·»åŠ åŸºç¡€ä¿¡æ¯å¯¹æ¯”
        comparison_result['basic_info'] = {
            'brand_comparison': self._compare_brands(spec1.brand, spec2.brand),
            'level_comparison': self._compare_levels(spec1.level, spec2.level),
            'price_difference': abs(spec1.price - spec2.price),
            'weight_difference': abs((spec1.weight or 0) - (spec2.weight or 0))
        }

        return comparison_result

    def identify_key_spec_differences(self, specs1: Dict[str, Any],
                                    specs2: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«å…³é”®è§„æ ¼å·®å¼‚"""
        differences = []

        # å®šä¹‰å…³é”®è§„æ ¼
        key_specs = [
            'length', 'weight', 'material', 'action',  # é±¼ç«¿
            'bearings', 'gear_ratio', 'line_capacity',  # é±¼è½®
            'strength', 'diameter', 'material',         # é±¼çº¿
            'weight', 'type', 'diving_depth'            # æ‹Ÿé¥µ
        ]

        for spec in key_specs:
            if spec in specs1 and spec in specs2:
                value1, value2 = specs1[spec], specs2[spec]
                if not self._are_spec_values_similar(spec, value1, value2):
                    differences.append(f"{spec}: {value1} vs {value2}")
            elif spec in specs1 or spec in specs2:
                value = specs1.get(spec, specs2.get(spec, 'N/A'))
                differences.append(f"{spec}: {value} vs N/A")

        return differences[:5]  # æœ€å¤šè¿”å›5ä¸ªå…³é”®å·®å¼‚

    def _compare_spec_value(self, spec_name: str, value1: Any, value2: Any,
                          rule: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”è§„æ ¼å€¼"""
        comparison = {
            'spec_name': spec_name,
            'value1': value1,
            'value2': value2,
            'similarity': 0.0,
            'significance': rule.get('significance', 'medium'),
            'description': ''
        }

        if value1 is None or value2 is None:
            comparison['similarity'] = 0.0
            comparison['description'] = "å…¶ä¸­ä¸€ä¸ªè£…å¤‡ç¼ºå°‘æ­¤è§„æ ¼ä¿¡æ¯"
            return comparison

        comparison_type = rule.get('type', 'numeric')

        if comparison_type == 'numeric':
            comparison.update(self._compare_numeric_values(value1, value2, rule))
        elif comparison_type == 'categorical':
            comparison.update(self._compare_categorical_values(value1, value2, rule))
        elif comparison_type == 'range':
            comparison.update(self._compare_range_values(value1, value2, rule))

        return comparison

    def _compare_numeric_values(self, value1: float, value2: float,
                              rule: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”æ•°å€¼å‹è§„æ ¼"""
        result = {}

        # è®¡ç®—ç›¸ä¼¼åº¦
        max_val = max(value1, value2)
        min_val = min(value1, value2)

        if max_val > 0:
            similarity = min_val / max_val
        else:
            similarity = 1.0 if value1 == value2 else 0.0

        result['similarity'] = similarity

        # ç¡®å®šä¼˜èƒœè€…
        higher_is_better = rule.get('higher_is_better', True)
        if higher_is_better:
            winner = 'equipment1' if value1 > value2 else 'equipment2' if value2 > value1 else 'equal'
        else:
            winner = 'equipment1' if value1 < value2 else 'equipment2' if value2 < value1 else 'equal'

        result['winner'] = winner

        # ç”Ÿæˆæè¿°
        diff_percentage = abs(value1 - value2) / max_val * 100 if max_val > 0 else 0

        if diff_percentage < 5:
            description = f"æ•°å€¼éå¸¸æ¥è¿‘ ({value1} vs {value2})"
        elif diff_percentage < 15:
            description = f"æ•°å€¼ç›¸è¿‘ ({value1} vs {value2})"
        else:
            description = f"æ•°å€¼å·®å¼‚è¾ƒå¤§ ({value1} vs {value2})"

        result['description'] = description

        return result

    def _compare_categorical_values(self, value1: str, value2: str,
                                  rule: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”ç±»åˆ«å‹è§„æ ¼"""
        result = {}

        # æ ‡å‡†åŒ–å€¼
        normalized_value1 = str(value1).lower().strip()
        normalized_value2 = str(value2).lower().strip()

        # è®¡ç®—ç›¸ä¼¼åº¦
        if normalized_value1 == normalized_value2:
            similarity = 1.0
            winner = 'equal'
        else:
            # æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¹‰è¯æˆ–ç›¸ä¼¼ç±»åˆ«
            similarity = self._calculate_categorical_similarity(
                normalized_value1, normalized_value2, rule
            )
            winner = 'different'  # ç±»åˆ«å‹è§„æ ¼æ²¡æœ‰ä¼˜èƒœè€…æ¦‚å¿µ

        result['similarity'] = similarity
        result['winner'] = winner

        # ç”Ÿæˆæè¿°
        if similarity >= 0.9:
            description = f"ç›¸åŒç±»å‹ ({value1})"
        elif similarity >= 0.7:
            description = f"ç›¸ä¼¼ç±»å‹ ({value1} vs {value2})"
        else:
            description = f"ä¸åŒç±»å‹ ({value1} vs {value2})"

        result['description'] = description

        return result

    def _compare_range_values(self, value1: str, value2: str,
                            rule: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”èŒƒå›´å‹è§„æ ¼"""
        result = {}

        # è§£æèŒƒå›´å€¼ (ä¾‹å¦‚ "2.1-2.4m")
        range1 = self._parse_range_value(value1)
        range2 = self._parse_range_value(value2)

        if range1 and range2:
            # è®¡ç®—èŒƒå›´é‡å åº¦
            overlap = self._calculate_range_overlap(range1, range2)
            similarity = overlap

            # ç¡®å®šä¼˜èƒœè€…ï¼ˆåŸºäºèŒƒå›´ä¸­ç‚¹ï¼‰
            midpoint1 = (range1[0] + range1[1]) / 2
            midpoint2 = (range2[0] + range2[1]) / 2
            higher_is_better = rule.get('higher_is_better', True)

            if higher_is_better:
                winner = 'equipment1' if midpoint1 > midpoint2 else 'equipment2' if midpoint2 > midpoint1 else 'equal'
            else:
                winner = 'equipment1' if midpoint1 < midpoint2 else 'equipment2' if midpoint2 < midpoint1 else 'equal'

            result['winner'] = winner
        else:
            similarity = 0.0
            winner = 'different'

        result['similarity'] = similarity

        # ç”Ÿæˆæè¿°
        if similarity >= 0.8:
            description = f"èŒƒå›´éå¸¸æ¥è¿‘ ({value1} vs {value2})"
        elif similarity >= 0.5:
            description = f"èŒƒå›´éƒ¨åˆ†é‡å  ({value1} vs {value2})"
        else:
            description = f"èŒƒå›´å·®å¼‚è¾ƒå¤§ ({value1} vs {value2})"

        result['description'] = description

        return result

    def _parse_range_value(self, value: str) -> Optional[Tuple[float, float]]:
        """è§£æèŒƒå›´å€¼"""
        import re

        # åŒ¹é…æ ¼å¼å¦‚ "2.1-2.4m" æˆ– "2.1~2.4"
        range_pattern = r'(\d+\.?\d*)\s*[-~]\s*(\d+\.?\d*)'
        match = re.search(range_pattern, str(value))

        if match:
            min_val = float(match.group(1))
            max_val = float(match.group(2))
            return (min_val, max_val)

        # å°è¯•åŒ¹é…å•ä¸ªæ•°å€¼
        number_pattern = r'(\d+\.?\d*)'
        match = re.search(number_pattern, str(value))
        if match:
            single_val = float(match.group(1))
            return (single_val, single_val)

        return None

    def _calculate_range_overlap(self, range1: Tuple[float, float],
                               range2: Tuple[float, float]) -> float:
        """è®¡ç®—èŒƒå›´é‡å åº¦"""
        min1, max1 = range1
        min2, max2 = range2

        # è®¡ç®—é‡å åŒºé—´
        overlap_min = max(min1, min2)
        overlap_max = min(max1, max2)

        if overlap_min > overlap_max:
            return 0.0  # æ²¡æœ‰é‡å 

        # è®¡ç®—é‡å é•¿åº¦
        overlap_length = overlap_max - overlap_min

        # è®¡ç®—æ€»é•¿åº¦
        total_length = max(max1, max2) - min(min1, min2)

        if total_length == 0:
            return 1.0  # ä¸¤ä¸ªèŒƒå›´éƒ½æ˜¯ç‚¹ä¸”ç›¸åŒ

        return overlap_length / total_length

    def _calculate_categorical_similarity(self, value1: str, value2: str,
                                        rule: Dict[str, Any]) -> float:
        """è®¡ç®—ç±»åˆ«ç›¸ä¼¼åº¦"""
        # æ£€æŸ¥åŒä¹‰è¯æ˜ å°„
        synonyms = rule.get('synonyms', {})

        normalized_values = set()
        for value in [value1, value2]:
            if value in synonyms:
                normalized_values.add(synonyms[value])
            else:
                normalized_values.add(value)

        if len(normalized_values) == 1:
            return 1.0  # åŒä¹‰è¯æˆ–ç›¸åŒå€¼

        # æ£€æŸ¥ç›¸ä¼¼ç±»åˆ«æ˜ å°„
        similarity_groups = rule.get('similarity_groups', [])
        for group in similarity_groups:
            if value1 in group and value2 in group:
                return 0.8  # ç›¸ä¼¼ç±»åˆ«

        # æ£€æŸ¥æ˜¯å¦æœ‰å…±åŒçš„çˆ¶ç±»åˆ«
        parent_categories = rule.get('parent_categories', {})
        parent1 = parent_categories.get(value1)
        parent2 = parent_categories.get(value2)

        if parent1 and parent1 == parent2:
            return 0.6  # ç›¸åŒçˆ¶ç±»åˆ«

        return 0.0  # å®Œå…¨ä¸åŒ

    def _are_spec_values_similar(self, spec_name: str, value1: Any, value2: Any) -> bool:
        """åˆ¤æ–­è§„æ ¼å€¼æ˜¯å¦ç›¸ä¼¼"""
        if value1 is None or value2 is None:
            return False

        try:
            # æ•°å€¼å‹è§„æ ¼
            if isinstance(value1, (int, float)) and isinstance(value2, (int, float)):
                max_val = max(value1, value2)
                if max_val > 0:
                    similarity = min(value1, value2) / max_val
                    return similarity > 0.9
                else:
                    return value1 == value2

            # å­—ç¬¦ä¸²å‹è§„æ ¼
            value1_str = str(value1).lower().strip()
            value2_str = str(value2).lower().strip()
            return value1_str == value2_str

        except (ValueError, TypeError):
            return False

    def _compare_brands(self, brand1: str, brand2: str) -> Dict[str, Any]:
        """å¯¹æ¯”å“ç‰Œ"""
        brand_tiers = {
            'premium': ['ç¦§ç›è¯º', 'è¾¾äº¿ç“¦', 'stella', 'exist'],
            'mid_range': ['å…‰å¨', 'åŒ–æ°', 'å®é£é¾™', 'å¤©å…ƒ'],
            'budget': ['è¿ªä½³', 'é±¼ç‹', 'ç¾äººé±¼']
        }

        tier1 = self._get_brand_tier(brand1, brand_tiers)
        tier2 = self._get_brand_tier(brand2, brand_tiers)

        return {
            'brands': [brand1, brand2],
            'tiers': [tier1, tier2],
            'similarity': 1.0 if tier1 == tier2 else 0.5,
            'premium_diff': abs(tier1 - tier2)
        }

    def _compare_levels(self, level1: EquipmentLevel, level2: EquipmentLevel) -> Dict[str, Any]:
        """å¯¹æ¯”ç­‰çº§"""
        level_hierarchy = {
            EquipmentLevel.BEGINNER: 1,
            EquipmentLevel.INTERMEDIATE: 2,
            EquipmentLevel.ADVANCED: 3,
            EquipmentLevel.PROFESSIONAL: 4
        }

        rank1 = level_hierarchy[level1]
        rank2 = level_hierarchy[level2]

        return {
            'levels': [level1.value, level2.value],
            'ranks': [rank1, rank2],
            'difference': abs(rank1 - rank2)
        }

    def _get_brand_tier(self, brand: str, brand_tiers: Dict[str, List[str]]) -> int:
        """è·å–å“ç‰Œæ¡£æ¬¡"""
        brand_lower = brand.lower()

        for tier, brands in brand_tiers.items():
            for b in brands:
                if b.lower() in brand_lower or brand_lower in b.lower():
                    return list(brand_tiers.keys()).index(tier) + 1

        return 2  # é»˜è®¤ä¸­æ¡£

    def _initialize_comparison_rules(self) -> Dict[str, Dict[str, Dict[str, Any]]]:
        """åˆå§‹åŒ–å¯¹æ¯”è§„åˆ™"""
        return {
            'fishing_rod': {
                'length': {
                    'type': 'range',
                    'significance': 'high',
                    'higher_is_better': False  # ä¸æ˜¯è¶Šé«˜è¶Šå¥½ï¼Œè€Œæ˜¯é€‚ä¸­
                },
                'weight': {
                    'type': 'numeric',
                    'significance': 'medium',
                    'higher_is_better': False  # è½»é‡åŒ–æ›´å¥½
                },
                'material': {
                    'type': 'categorical',
                    'significance': 'high',
                    'synonyms': {
                        'carbon': 'ç¢³çº¤ç»´',
                        'carbon fiber': 'ç¢³çº¤ç»´',
                        'graphite': 'ç¢³çº¤ç»´'
                    },
                    'similarity_groups': [
                        ['carbon', 'carbon fiber', 'graphite'],
                        ['glass', 'fiberglass', 'ç»ç’ƒçº¤ç»´']
                    ]
                },
                'action': {
                    'type': 'categorical',
                    'significance': 'medium',
                    'synonyms': {
                        'ml': 'medium light',
                        'mh': 'medium heavy'
                    }
                }
            },
            'fishing_reel': {
                'bearings': {
                    'type': 'numeric',
                    'significance': 'medium',
                    'higher_is_better': True
                },
                'gear_ratio': {
                    'type': 'numeric',
                    'significance': 'high',
                    'higher_is_better': True
                },
                'line_capacity': {
                    'type': 'categorical',
                    'significance': 'medium'
                }
            },
            # å…¶ä»–ç±»åˆ«è§„åˆ™...
        }

    def _initialize_spec_weights(self) -> Dict[str, Dict[str, float]]:
        """åˆå§‹åŒ–è§„æ ¼æƒé‡"""
        return {
            'fishing_rod': {
                'length': 0.20,
                'weight': 0.15,
                'material': 0.25,
                'action': 0.20,
                'power': 0.20
            },
            'fishing_reel': {
                'bearings': 0.20,
                'gear_ratio': 0.25,
                'line_capacity': 0.20,
                'weight': 0.15,
                'material': 0.20
            },
            # å…¶ä»–ç±»åˆ«æƒé‡...
        }
```

## ğŸ› ï¸ LangChainå·¥å…·é›†æˆ

### è£…å¤‡å¯¹æ¯”åˆ†æå·¥å…·å‡½æ•°
```python
from langchain_core.tools import tool
from typing import Dict, Any, List, Optional

@tool
def compare_multiple_equipment(
    equipment_list: List[Dict[str, str]],
    comparison_focus: Optional[str] = None
) -> Dict[str, Any]:
    """
    å¯¹æ¯”å¤šæ¬¾é’“é±¼è£…å¤‡çš„è¯¦ç»†å‚æ•°å’Œæ€§èƒ½

    Args:
        equipment_list: è£…å¤‡åˆ—è¡¨ï¼Œæ¯ä¸ªè£…å¤‡åŒ…å«brandå’Œmodel
        comparison_focus: å¯¹æ¯”é‡ç‚¹ (performance/price/value/overall)

    Returns:
        Dict: åŒ…å«è¯¦ç»†å¯¹æ¯”åˆ†æã€æ€§èƒ½æ’åã€ä¼˜åŠ£åŠ¿åˆ†æç­‰
    """
    try:
        if len(equipment_list) < 2:
            return {
                "success": False,
                "message": "è‡³å°‘éœ€è¦2ä¸ªè£…å¤‡è¿›è¡Œå¯¹æ¯”"
            }

        # è·å–è£…å¤‡ä¿¡æ¯
        service_container = get_service_container()
        equipment_repository = service_container.get_service('equipment_repository')
        comparison_engine = service_container.get_service('comparison_engine')

        equipment_ids = []
        equipment_details = []

        for equipment_info in equipment_list:
            brand = equipment_info.get('brand', '')
            model = equipment_info.get('model', '')

            # æœç´¢è£…å¤‡
            search_results = equipment_repository.search_equipment(
                f"{brand} {model}", {'limit': 1}
            )

            if search_results:
                equipment = search_results[0]
                equipment_ids.append(equipment.id)
                equipment_details.append({
                    "id": equipment.id,
                    "brand": equipment.spec.brand,
                    "model": equipment.spec.model,
                    "price": equipment.spec.price,
                    "level": equipment.spec.level.value
                })

        if len(equipment_ids) < 2:
            return {
                "success": False,
                "message": "æ— æ³•æ‰¾åˆ°è¶³å¤Ÿçš„è£…å¤‡ä¿¡æ¯è¿›è¡Œå¯¹æ¯”"
            }

        # æ‰§è¡Œå¯¹æ¯”åˆ†æ
        comparison_type = ComparisonType.MULTI_COMPARISON
        result = comparison_engine.compare_equipment(equipment_ids, comparison_type)

        # æ ¼å¼åŒ–è¿”å›ç»“æœ
        formatted_result = {
            "success": True,
            "comparison_id": result.comparison_id,
            "equipment_count": len(result.equipment_comparisons),
            "equipments": equipment_details,
            "ranking": [
                {
                    "rank": i + 1,
                    "equipment_id": equipment_id,
                    "overall_score": f"{score:.1f}/100"
                }
                for i, (equipment_id, score) in enumerate(result.ranking)
            ],
            "dimension_analysis": {
                dimension.value: {
                    equipment_id: f"{score:.1f}/100"
                    for equipment_id, score in scores.items()
                }
                for dimension, scores in result.dimension_scores.items()
            },
            "key_differences": result.key_differences,
            "value_analysis": {
                "best_value": result.value_analysis.get('best_value_equipment'),
                "value_range": {
                    "min": f"{result.value_analysis['value_range']['min']:.2f}",
                    "max": f"{result.value_analysis['value_range']['max']:.2f}",
                    "average": f"{result.value_analysis['value_range']['average']:.2f}"
                }
            },
            "recommendation": result.recommendation,
            "detailed_scores": {
                comp.equipment_id: {
                    "overall_score": f"{comp.overall_score:.1f}/100",
                    "strengths": comp.strengths,
                    "weaknesses": comp.weaknesses,
                    "key_metrics": {
                        metric: f"{value:.1f}"
                        for metric, value in list(comp.metrics.items())[:5]
                    }
                }
                for comp in result.equipment_comparisons
            }
        }

        # æ ¹æ®å¯¹æ¯”é‡ç‚¹è°ƒæ•´ç»“æœ
        if comparison_focus:
            formatted_result["focus_analysis"] = _generate_focus_analysis(
                result, comparison_focus
            )

        return formatted_result

    except Exception as e:
        logger.error(f"å¤šè£…å¤‡å¯¹æ¯”å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "å¾ˆæŠ±æ­‰ï¼Œæ— æ³•å®Œæˆè£…å¤‡å¯¹æ¯”åˆ†æ"
        }

@tool
def analyze_upgrade_value(
    current_equipment: Dict[str, str],
    target_equipment: Dict[str, str]
) -> Dict[str, Any]:
    """
    åˆ†æè£…å¤‡å‡çº§çš„ä»·å€¼å’Œæ€§ä»·æ¯”

    Args:
        current_equipment: å½“å‰è£…å¤‡ {brand, model}
        target_equipment: ç›®æ ‡è£…å¤‡ {brand, model}

    Returns:
        Dict: åŒ…å«å‡çº§ä»·å€¼åˆ†æã€æ€§èƒ½æå‡ã€æ€§ä»·æ¯”è¯„ä¼°ç­‰
    """
    try:
        # è·å–è£…å¤‡ä¿¡æ¯
        service_container = get_service_container()
        equipment_repository = service_container.get_service('equipment_repository')
        comparison_engine = service_container.get_service('comparison_engine')

        # æŸ¥æ‰¾è£…å¤‡
        current_eq = _find_equipment_by_brand_model(
            equipment_repository, current_equipment['brand'], current_equipment['model']
        )
        target_eq = _find_equipment_by_brand_model(
            equipment_repository, target_equipment['brand'], target_equipment['model']
        )

        if not current_eq or not target_eq:
            return {
                "success": False,
                "message": "æ— æ³•æ‰¾åˆ°æŒ‡å®šçš„è£…å¤‡ä¿¡æ¯"
            }

        # æ‰§è¡Œå‡çº§ä»·å€¼åˆ†æ
        upgrade_analysis = comparison_engine.analyze_upgrade_value(
            current_eq.id, target_eq.id
        )

        # æ ¼å¼åŒ–ç»“æœ
        return {
            "success": True,
            "current_equipment": {
                "name": f"{current_eq.spec.brand} {current_eq.spec.model}",
                "price": current_eq.spec.price,
                "level": current_eq.spec.level.value,
                "overall_score": f"{upgrade_analysis['current_equipment']['overall_score']:.1f}/100"
            },
            "target_equipment": {
                "name": f"{target_eq.spec.brand} {target_eq.spec.model}",
                "price": target_eq.spec.price,
                "level": target_eq.spec.level.value,
                "overall_score": f"{upgrade_analysis['target_equipment']['overall_score']:.1f}/100"
            },
            "upgrade_analysis": {
                "performance_improvement": f"{upgrade_analysis['performance_improvement']:.1f} åˆ†",
                "price_difference": f"{upgrade_analysis['price_difference']:.0f} å…ƒ",
                "value_ratio": f"{upgrade_analysis['value_ratio']:.2f}",
                "is_recommended": upgrade_analysis['is_recommended']
            },
            "spec_improvements": upgrade_analysis['spec_differences'],
            "recommendation": upgrade_analysis['upgrade_recommendation'],
            "upgrade_suggestion": _generate_upgrade_suggestion(upgrade_analysis)
        }

    except Exception as e:
        logger.error(f"å‡çº§ä»·å€¼åˆ†æå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "å¾ˆæŠ±æ­‰ï¼Œæ— æ³•åˆ†æå‡çº§ä»·å€¼"
        }

@tool
def get_equipment_performance_scores(
    category: str,
    brands: Optional[List[str]] = None,
    price_range: Optional[Dict[str, float]] = None
) -> Dict[str, Any]:
    """
    è·å–ç‰¹å®šç±»åˆ«è£…å¤‡çš„æ€§èƒ½è¯„åˆ†æ’å

    Args:
        category: è£…å¤‡ç±»åˆ«
        brands: ç­›é€‰å“ç‰Œåˆ—è¡¨ (å¯é€‰)
        price_range: ä»·æ ¼èŒƒå›´ {min, max} (å¯é€‰)

    Returns:
        Dict: åŒ…å«æ€§èƒ½è¯„åˆ†æ’åå’Œè¯¦ç»†åˆ†æ
    """
    try:
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filters = {'category': category}

        if brands:
            filters['brands'] = brands

        if price_range:
            filters['min_price'] = price_range.get('min', 0)
            filters['max_price'] = price_range.get('max', float('inf'))

        # è·å–è£…å¤‡å¯¹æ¯”ç»“æœ
        service_container = get_service_container()
        comparison_engine = service_container.get_service('comparison_engine')

        comparison_result = comparison_engine.compare_multiple_equipment(
            category, filters, limit=15
        )

        # æ ¼å¼åŒ–æ€§èƒ½æ’å
        performance_ranking = []
        for rank, (equipment_id, score) in enumerate(comparison_result.ranking, 1):
            equipment_info = None
            for comp in comparison_result.equipment_comparisons:
                if comp.equipment_id == equipment_id:
                    # è·å–è£…å¤‡è¯¦ç»†ä¿¡æ¯
                    equipment_repository = service_container.get_service('equipment_repository')
                    equipment = equipment_repository.find_by_id(equipment_id)
                    if equipment:
                        equipment_info = {
                            "brand": equipment.spec.brand,
                            "model": equipment.spec.model,
                            "price": equipment.spec.price,
                            "level": equipment.spec.level.value
                        }
                    break

            if equipment_info:
                performance_ranking.append({
                    "rank": rank,
                    "overall_score": f"{score:.1f}/100",
                    "performance_grade": _get_performance_grade(score),
                    **equipment_info
                })

        # ç»´åº¦åˆ†æ
        dimension_analysis = {}
        for dimension, scores in comparison_result.dimension_scores.items():
            # æŒ‰è¯„åˆ†æ’åº
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            dimension_analysis[dimension.value] = [
                {
                    "rank": i + 1,
                    "equipment_id": equipment_id,
                    "score": f"{score:.1f}/100"
                }
                for i, (equipment_id, score) in enumerate(sorted_scores[:5])
            ]

        return {
            "success": True,
            "category": category,
            "total_equipment": len(performance_ranking),
            "performance_ranking": performance_ranking,
            "dimension_analysis": dimension_analysis,
            "analysis_summary": _generate_performance_summary(comparison_result),
            "best_value_equipment": comparison_result.value_analysis.get('best_value_equipment')
        }

    except Exception as e:
        logger.error(f"æ€§èƒ½è¯„åˆ†è·å–å¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "å¾ˆæŠ±æ­‰ï¼Œæ— æ³•è·å–è£…å¤‡æ€§èƒ½è¯„åˆ†"
        }

# è¾…åŠ©å‡½æ•°
def _find_equipment_by_brand_model(repository, brand: str, model: str):
    """æ ¹æ®å“ç‰Œå‹å·æŸ¥æ‰¾è£…å¤‡"""
    search_results = repository.search_equipment(f"{brand} {model}", {'limit': 1})
    return search_results[0] if search_results else None

def _generate_focus_analysis(result: ComparisonResult, focus: str) -> Dict[str, Any]:
    """ç”Ÿæˆé‡ç‚¹åˆ†æ"""
    focus_analysis = {}

    if focus == "performance":
        # æ€§èƒ½é‡ç‚¹åˆ†æ
        best_performance = max(result.equipment_comparisons, key=lambda x: x.overall_score)
        focus_analysis = {
            "focus_type": "performance",
            "best_performer": best_performance.equipment_id,
            "analysis": f"æœ€ä½³æ€§èƒ½è£…å¤‡è¯„åˆ†ä¸º {best_performance.overall_score:.1f} åˆ†",
            "recommendation": "å¦‚æœæ‚¨ä¼˜å…ˆè€ƒè™‘æ€§èƒ½ï¼Œå»ºè®®é€‰æ‹©è¿™æ¬¾è£…å¤‡"
        }
    elif focus == "price":
        # ä»·æ ¼é‡ç‚¹åˆ†æ
        service_container = get_service_container()
        equipment_repository = service_container.get_service('equipment_repository')

        prices = {}
        for comp in result.equipment_comparisons:
            equipment = equipment_repository.find_by_id(comp.equipment_id)
            if equipment:
                prices[comp.equipment_id] = equipment.spec.price

        if prices:
            cheapest_id = min(prices.keys(), key=lambda k: prices[k])
            focus_analysis = {
                "focus_type": "price",
                "cheapest_option": cheapest_id,
                "analysis": f"æœ€ç»æµé€‰é¡¹ä»·æ ¼ä¸º {prices[cheapest_id]:.0f} å…ƒ",
                "recommendation": "å¦‚æœæ‚¨é¢„ç®—æœ‰é™ï¼Œå»ºè®®é€‰æ‹©è¿™æ¬¾ç»æµå‹è£…å¤‡"
            }
    elif focus == "value":
        # æ€§ä»·æ¯”é‡ç‚¹åˆ†æ
        best_value_id = result.value_analysis.get('best_value_equipment')
        if best_value_id:
            focus_analysis = {
                "focus_type": "value",
                "best_value": best_value_id,
                "analysis": "æœ€ä½³æ€§ä»·æ¯”é€‰æ‹©",
                "recommendation": "å¦‚æœæ‚¨è¿½æ±‚æ€§ä»·æ¯”ï¼Œè¿™æ¬¾è£…å¤‡æ˜¯æœ€ä½³é€‰æ‹©"
            }

    return focus_analysis

def _generate_upgrade_suggestion(upgrade_analysis: Dict[str, Any]) -> str:
    """ç”Ÿæˆå‡çº§å»ºè®®"""
    performance_improvement = upgrade_analysis['performance_improvement']
    price_difference = upgrade_analysis['price_difference']
    is_recommended = upgrade_analysis['is_recommended']

    if is_recommended:
        if performance_improvement > 20:
            return "å¼ºçƒˆæ¨èå‡çº§ï¼æ€§èƒ½æå‡æ˜¾è‘—ï¼Œæ€§ä»·æ¯”ä¼˜ç§€ã€‚"
        elif performance_improvement > 10:
            return "æ¨èå‡çº§ã€‚æ€§èƒ½æœ‰æ˜æ˜¾æå‡ï¼Œå€¼å¾—æŠ•èµ„ã€‚"
        else:
            return "å¯ä»¥è€ƒè™‘å‡çº§ã€‚æ€§èƒ½ç•¥æœ‰æå‡ï¼ŒæŒ‰éœ€é€‰æ‹©ã€‚"
    else:
        if price_difference > 1000:
            return "ä¸æ¨èå‡çº§ã€‚ä»·æ ¼è¿‡é«˜ï¼Œæ€§èƒ½æå‡æœ‰é™ã€‚"
        else:
            return "å‡çº§ä»·å€¼ä¸é«˜ã€‚å»ºè®®ç»§ç»­ä½¿ç”¨å½“å‰è£…å¤‡æˆ–è€ƒè™‘å…¶ä»–é€‰æ‹©ã€‚"

def _get_performance_grade(score: float) -> str:
    """è·å–æ€§èƒ½ç­‰çº§"""
    if score >= 95:
        return "A+ (å“è¶Š)"
    elif score >= 90:
        return "A (ä¼˜ç§€)"
    elif score >= 85:
        return "B+ (è‰¯å¥½)"
    elif score >= 80:
        return "B (ä¸­ç­‰åä¸Š)"
    elif score >= 75:
        return "C+ (ä¸­ç­‰)"
    elif score >= 70:
        return "C (ä¸­ç­‰åä¸‹)"
    else:
        return "D (éœ€è¦æ”¹è¿›)"

def _generate_performance_summary(result: ComparisonResult) -> str:
    """ç”Ÿæˆæ€§èƒ½æ€»ç»“"""
    if not result.equipment_comparisons:
        return "æ— æ•°æ®"

    scores = [comp.overall_score for comp in result.equipment_comparisons]
    avg_score = sum(scores) / len(scores)
    max_score = max(scores)
    min_score = min(scores)

    summary = f"å…±å¯¹æ¯” {len(scores)} æ¬¾è£…å¤‡ï¼Œå¹³å‡è¯„åˆ† {avg_score:.1f} åˆ†ã€‚"

    if max_score - min_score > 20:
        summary += "è£…å¤‡é—´æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œå»ºè®®ä»”ç»†å¯¹æ¯”é€‰æ‹©ã€‚"
    else:
        summary += "è£…å¤‡é—´æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ®ä»·æ ¼å’Œå“ç‰Œåå¥½é€‰æ‹©ã€‚"

    return summary

def get_service_container():
    """è·å–æœåŠ¡å®¹å™¨"""
    from shared.infrastructure.service_manager import get_service_manager
    return get_service_manager()
```

## ğŸ¯ å¼€å‘å®æ–½æŒ‡å—

### å¼€å‘ä¼˜å…ˆçº§å’Œé‡Œç¨‹ç¢‘

#### ç¬¬ä¸€é˜¶æ®µï¼šæ ¸å¿ƒå¯¹æ¯”å¼•æ“ï¼ˆ2å‘¨ï¼‰
**ç›®æ ‡**ï¼šå»ºç«‹åŸºç¡€çš„è£…å¤‡å¯¹æ¯”åˆ†æèƒ½åŠ›

**Week 1: æ€§èƒ½è¯„åˆ†ç³»ç»Ÿ**
- [ ] å®ç°PerformanceScoreræ€§èƒ½è¯„åˆ†å™¨
- [ ] å¼€å‘å¤šç»´åº¦è¯„åˆ†ç®—æ³•
- [ ] å»ºç«‹æ€§èƒ½åŸºå‡†ä½“ç³»
- [ ] å®ç°è£…å¤‡æŒ‡æ ‡æå–å’Œæ ‡å‡†åŒ–

**Week 2: å¯¹æ¯”åˆ†æå¼•æ“**
- [ ] å®ç°ComparisonEngineå¯¹æ¯”åˆ†æå¼•æ“
- [ ] å¼€å‘è£…å¤‡å¯¹æ¯”æ ¸å¿ƒç®—æ³•
- [ ] å®ç°æ’åå’Œæ¨èç”Ÿæˆ
- [ ] åˆ›å»ºåŸºç¡€ç¼“å­˜æœºåˆ¶

#### ç¬¬äºŒé˜¶æ®µï¼šè§„æ ¼åˆ†æç³»ç»Ÿï¼ˆ1-2å‘¨ï¼‰
**ç›®æ ‡**ï¼šå®ç°è¯¦ç»†çš„è§„æ ¼å‚æ•°å¯¹æ¯”åˆ†æ

**Week 3: è§„æ ¼åˆ†æå™¨**
- [ ] å®ç°SpecificationAnalyzerè§„æ ¼åˆ†æå™¨
- [ ] å¼€å‘å¤šç±»å‹è§„æ ¼å¯¹æ¯”ç®—æ³•
- [ ] å»ºç«‹è§„æ ¼å¯¹æ¯”è§„åˆ™åº“
- [ ] å®ç°ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•

**Week 4: å‡çº§ä»·å€¼åˆ†æ**
- [ ] å®ç°UpgradeAdvisorå‡çº§é¡¾é—®
- [ ] å¼€å‘å‡çº§ä»·å€¼è¯„ä¼°ç®—æ³•
- [ ] å®ç°æ€§ä»·æ¯”åˆ†æåŠŸèƒ½
- [ ] åˆ›å»ºå‡çº§å»ºè®®ç”Ÿæˆå™¨

#### ç¬¬ä¸‰é˜¶æ®µï¼šå·¥å…·é›†æˆå’Œä¼˜åŒ–ï¼ˆ1å‘¨ï¼‰
**ç›®æ ‡**ï¼šå®ŒæˆLangChainå·¥å…·é›†æˆå’Œç³»ç»Ÿä¼˜åŒ–

**Week 5: å·¥å…·é›†æˆ**
- [ ] å¼€å‘LangChainå·¥å…·å‡½æ•°
- [ ] å®ç°é”™è¯¯å¤„ç†å’Œå¼‚å¸¸ç®¡ç†
- [ ] åˆ›å»ºAPIæ¥å£å’Œæ–‡æ¡£
- [ ] è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•å’Œæ€§èƒ½ä¼˜åŒ–

### æŠ€æœ¯å®æ–½è¦ç‚¹

#### 1. æ€§èƒ½è¯„åˆ†ç®—æ³•
- **å¤šç»´åº¦è¯„ä¼°**: ä»æ€§èƒ½ã€ä»·æ ¼ã€æ˜“ç”¨æ€§ã€è€ç”¨æ€§ç­‰å¤šç»´åº¦è¯„ä¼°
- **æƒé‡è°ƒä¼˜**: æ ¹æ®ç”¨æˆ·åé¦ˆå’Œå¸‚åœºéœ€æ±‚è°ƒæ•´è¯„åˆ†æƒé‡
- **åŸºå‡†å¯¹æ¯”**: ä¸åŒçº§äº§å“è¿›è¡ŒåŸºå‡†å¯¹æ¯”ï¼Œæä¾›ç›¸å¯¹æ€§èƒ½è¯„ä¼°
- **ç½®ä¿¡åº¦è®¡ç®—**: è¯„ä¼°è¯„åˆ†ç»“æœçš„å¯ä¿¡åº¦

#### 2. è§„æ ¼å¯¹æ¯”åˆ†æ
- **ç±»å‹è¯†åˆ«**: åŒºåˆ†æ•°å€¼å‹ã€ç±»åˆ«å‹ã€èŒƒå›´å‹ç­‰ä¸åŒè§„æ ¼ç±»å‹
- **ç›¸ä¼¼åº¦è®¡ç®—**: é’ˆå¯¹ä¸åŒç±»å‹é‡‡ç”¨åˆé€‚çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•
- **å…³é”®å·®å¼‚è¯†åˆ«**: è¯†åˆ«å½±å“ç”¨æˆ·å†³ç­–çš„å…³é”®è§„æ ¼å·®å¼‚
- **æ ‡å‡†åŒ–å¤„ç†**: å°†ä¸åŒè§„æ ¼æ ‡å‡†åŒ–åˆ°ç»Ÿä¸€çš„è¯„åˆ†ä½“ç³»

#### 3. å¯¹æ¯”ç»“æœå±•ç¤º
- **å¤šå±‚æ¬¡å±•ç¤º**: æä¾›æ¦‚è§ˆã€è¯¦ç»†åˆ†æã€ä¸“ä¸šå»ºè®®ç­‰å¤šå±‚æ¬¡ä¿¡æ¯
- **å¯è§†åŒ–æ”¯æŒ**: æ”¯æŒå›¾è¡¨ã€é›·è¾¾å›¾ç­‰å¯è§†åŒ–å¯¹æ¯”å±•ç¤º
- **ä¸ªæ€§åŒ–æ¨è**: åŸºäºç”¨æˆ·éœ€æ±‚æä¾›ä¸ªæ€§åŒ–çš„è´­ä¹°å»ºè®®
- **å†³ç­–æ”¯æŒ**: æä¾›æ˜ç¡®çš„å†³ç­–ä¾æ®å’Œè´­ä¹°æŒ‡å¯¼

#### 4. ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–
- **ç¼“å­˜ç­–ç•¥**: å¯¹å¸¸ç”¨å¯¹æ¯”ç»“æœè¿›è¡Œç¼“å­˜
- **å¼‚æ­¥å¤„ç†**: å¤æ‚åˆ†æè®¡ç®—ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡
- **æ•°æ®åº“ä¼˜åŒ–**: ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½å’Œç´¢å¼•è®¾è®¡
- **è´Ÿè½½å‡è¡¡**: æ”¯æŒé«˜å¹¶å‘å¯¹æ¯”è¯·æ±‚

### æµ‹è¯•ç­–ç•¥

#### å•å…ƒæµ‹è¯•
```python
import pytest
from unittest.mock import Mock

class TestPerformanceScorer:
    """æ€§èƒ½è¯„åˆ†å™¨æµ‹è¯•"""

    def setup_method(self):
        self.scorer = PerformanceScorer()

    def test_score_fishing_rod(self):
        """æµ‹è¯•é±¼ç«¿è¯„åˆ†"""
        equipment = Equipment(
            id="test_rod",
            spec=EquipmentSpec(
                brand="TestBrand",
                model="TestModel",
                category=EquipmentCategory.FISHING_ROD,
                level=EquipmentLevel.INTERMEDIATE,
                price=500,
                specifications={
                    'length': 240,
                    'weight': 80,
                    'material': 'carbon',
                    'action': 'medium'
                },
                compatibility=[]
            ),
            performance_score=85,
            popularity_score=80,
            user_rating=4.2,
            review_count=150,
            availability=True
        )

        result = self.scorer.score_equipment(equipment)

        assert result.overall_score > 70
        assert len(result.dimension_scores) > 0
        assert result.confidence_level > 0.5

    def test_extract_rod_metrics(self):
        """æµ‹è¯•é±¼ç«¿æŒ‡æ ‡æå–"""
        specs = {
            'length': 240,
            'weight': 80,
            'material': 'carbon',
            'action': 'medium'
        }

        metrics = self.scorer._extract_rod_metrics(specs)

        assert 'length_score' in metrics
        assert 'weight_score' in metrics
        assert 'material_score' in metrics
        assert 'action_score' in metrics
        assert all(0 <= score <= 100 for score in metrics.values())

class TestComparisonEngine:
    """å¯¹æ¯”åˆ†æå¼•æ“æµ‹è¯•"""

    def setup_method(self):
        self.mock_repository = Mock(spec=EquipmentRepository)
        self.mock_scorer = Mock(spec=PerformanceScorer)
        self.mock_analyzer = Mock(spec=SpecificationAnalyzer)
        self.mock_advisor = Mock(spec=UpgradeAdvisor)

        self.engine = ComparisonEngine(
            self.mock_repository,
            self.mock_scorer,
            self.mock_analyzer,
            self.mock_advisor
        )

    def test_compare_two_equipment(self):
        """æµ‹è¯•ä¸¤æ¬¾è£…å¤‡å¯¹æ¯”"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        equipment1 = self._create_test_equipment("rod1", 500)
        equipment2 = self._create_test_equipment("rod2", 600)

        self.mock_repository.find_by_id.side_effect = [equipment1, equipment2]

        # è®¾ç½®æ€§èƒ½è¯„åˆ†è¿”å›å€¼
        mock_score = DetailedPerformanceScore(
            overall_score=85.0,
            dimension_scores={AnalysisDimension.PERFORMANCE: 90.0},
            sub_metrics={'performance_score': 85},
            benchmark_comparison={},
            confidence_level=0.8
        )
        self.mock_scorer.score_equipment.return_value = mock_score

        # æ‰§è¡Œå¯¹æ¯”
        result = self.engine.compare_equipment(["rod1", "rod2"])

        # éªŒè¯ç»“æœ
        assert result is not None
        assert len(result.equipment_comparisons) == 2
        assert len(result.ranking) == 2
        assert result.recommendation is not None

    def _create_test_equipment(self, equipment_id: str, price: float) -> Equipment:
        """åˆ›å»ºæµ‹è¯•è£…å¤‡"""
        return Equipment(
            id=equipment_id,
            spec=EquipmentSpec(
                brand="TestBrand",
                model=f"TestModel{equipment_id}",
                category=EquipmentCategory.FISHING_ROD,
                level=EquipmentLevel.INTERMEDIATE,
                price=price,
                specifications={},
                compatibility=[]
            ),
            performance_score=85,
            popularity_score=80,
            user_rating=4.2,
            review_count=100,
            availability=True
        )
```

#### é›†æˆæµ‹è¯•
```python
class TestComparisonIntegration:
    """å¯¹æ¯”ç³»ç»Ÿé›†æˆæµ‹è¯•"""

    def setup_method(self):
        # ä½¿ç”¨æµ‹è¯•æ•°æ®åº“
        self.test_db = create_test_database()
        self.setup_test_data()

        # åˆ›å»ºçœŸå®çš„æœåŠ¡å®ä¾‹
        self.equipment_repository = DatabaseEquipmentRepository(self.test_db)
        self.performance_scorer = PerformanceScorer()
        self.spec_analyzer = SpecificationAnalyzer()
        self.upgrade_advisor = UpgradeAdvisor()

        self.comparison_engine = ComparisonEngine(
            self.equipment_repository,
            self.performance_scorer,
            self.spec_analyzer,
            self.upgrade_advisor
        )

    def test_full_comparison_workflow(self):
        """æµ‹è¯•å®Œæ•´å¯¹æ¯”å·¥ä½œæµ"""
        # è·å–æµ‹è¯•è£…å¤‡
        equipments = self.equipment_repository.find_by_category(
            EquipmentCategory.FISHING_ROD
        )[:3]

        if len(equipments) < 2:
            pytest.skip("éœ€è¦è‡³å°‘2ä¸ªæµ‹è¯•è£…å¤‡")

        equipment_ids = [equipment.id for equipment in equipments]

        # æ‰§è¡Œå¯¹æ¯”
        result = self.comparison_engine.compare_equipment(
            equipment_ids, ComparisonType.MULTI_COMPARISON
        )

        # éªŒè¯ç»“æœ
        assert result is not None
        assert len(result.equipment_comparisons) == len(equipments)
        assert len(result.ranking) == len(equipments)
        assert result.overall_score > 0

        # éªŒè¯æ’ååˆç†æ€§
        scores = [score for _, score in result.ranking]
        assert scores == sorted(scores, reverse=True)

        # éªŒè¯ç»´åº¦åˆ†æ
        assert len(result.dimension_scores) > 0
        for dimension, scores in result.dimension_scores.items():
            assert len(scores) == len(equipments)
```

### éƒ¨ç½²å’Œè¿ç»´

#### 1. æ€§èƒ½ç›‘æ§
- **å“åº”æ—¶é—´**: ç›‘æ§å¯¹æ¯”åˆ†æçš„å“åº”æ—¶é—´
- **å‡†ç¡®ç‡**: ç›‘æ§æ¨èç»“æœçš„ç”¨æˆ·æ»¡æ„åº¦
- **å¹¶å‘é‡**: ç›‘æ§ç³»ç»Ÿçš„å¹¶å‘å¤„ç†èƒ½åŠ›
- **ç¼“å­˜å‘½ä¸­ç‡**: ç›‘æ§ç¼“å­˜ç³»ç»Ÿçš„æ•ˆç‡

#### 2. æ•°æ®è´¨é‡
- **æ•°æ®å®Œæ•´æ€§**: ç¡®ä¿è£…å¤‡è§„æ ¼æ•°æ®çš„å®Œæ•´æ€§
- **æ•°æ®å‡†ç¡®æ€§**: å®šæœŸéªŒè¯è£…å¤‡ä¿¡æ¯çš„å‡†ç¡®æ€§
- **æ•°æ®æ›´æ–°**: å»ºç«‹è£…å¤‡ä¿¡æ¯çš„å®šæœŸæ›´æ–°æœºåˆ¶
- **å¼‚å¸¸æ£€æµ‹**: ç›‘æ§å’Œæ£€æµ‹å¼‚å¸¸æ•°æ®

#### 3. ç”¨æˆ·ä½“éªŒ
- **ç»“æœè´¨é‡**: æŒç»­ä¼˜åŒ–å¯¹æ¯”åˆ†æç»“æœçš„è´¨é‡
- **å“åº”é€Ÿåº¦**: ä¼˜åŒ–ç³»ç»Ÿå“åº”é€Ÿåº¦
- **ç•Œé¢å‹å¥½**: æä¾›ç›´è§‚æ˜“ç”¨çš„å¯¹æ¯”ç»“æœå±•ç¤º
- **ä¸ªæ€§åŒ–**: åŸºäºç”¨æˆ·åé¦ˆä¼˜åŒ–ä¸ªæ€§åŒ–æ¨è

---

## ğŸ“ å¼€å‘æ€»ç»“

è£…å¤‡å¯¹æ¯”åˆ†æç³»ç»Ÿä¸ºé’“é±¼çˆ±å¥½è€…æä¾›ä¸“ä¸šã€å®¢è§‚ã€è¯¦ç»†çš„è£…å¤‡å¯¹æ¯”åˆ†ææœåŠ¡ã€‚é€šè¿‡å¤šç»´åº¦çš„æ€§èƒ½è¯„ä¼°ã€è¯¦ç»†çš„è§„æ ¼å¯¹æ¯”ã€æ™ºèƒ½çš„å‡çº§ä»·å€¼åˆ†æï¼Œå¸®åŠ©ç”¨æˆ·åšå‡ºæœ€ä¼˜çš„è£…å¤‡é€‰æ‹©å†³ç­–ã€‚

### æ ¸å¿ƒèƒ½åŠ›
- **ä¸“ä¸šæ€§èƒ½è¯„åˆ†**: åŸºäºå¤šç»´åº¦æŒ‡æ ‡çš„ç§‘å­¦è¯„åˆ†ä½“ç³»
- **è¯¦ç»†è§„æ ¼å¯¹æ¯”**: æ·±åº¦åˆ†æè£…å¤‡è§„æ ¼å‚æ•°çš„å·®å¼‚å’Œå½±å“
- **æ™ºèƒ½æ’åæ¨è**: æä¾›å®¢è§‚çš„æ’åå’Œä¸ªæ€§åŒ–æ¨èå»ºè®®
- **å‡çº§ä»·å€¼åˆ†æ**: è¯„ä¼°è£…å¤‡å‡çº§çš„ä»·å€¼å’Œæ€§ä»·æ¯”

### æŠ€æœ¯ç‰¹è‰²
- **ç§‘å­¦è¯„åˆ†ç®—æ³•**: åŸºäºåŸºå‡†å¯¹æ¯”å’Œå¤šç»´åº¦è¯„ä¼°çš„è¯„åˆ†ä½“ç³»
- **æ™ºèƒ½è§„æ ¼åˆ†æ**: æ”¯æŒå¤šç§è§„æ ¼ç±»å‹çš„æ™ºèƒ½å¯¹æ¯”åˆ†æ
- **é«˜æ€§èƒ½ç¼“å­˜**: ä¼˜åŒ–å¸¸ç”¨å¯¹æ¯”ç»“æœçš„ç¼“å­˜ç­–ç•¥
- **LangChainé›†æˆ**: æ— ç¼é›†æˆåˆ°æ™ºèƒ½ä½“å¯¹è¯ç³»ç»Ÿ

è¯¥ç³»ç»Ÿä¸ºé’“é±¼è£…å¤‡é€‰æ‹©æä¾›ä¸“ä¸šçº§çš„æ•°æ®æ”¯æŒï¼Œæ˜¾è‘—æå‡ç”¨æˆ·çš„è´­ä¹°å†³ç­–è´¨é‡å’Œæ»¡æ„åº¦ã€‚