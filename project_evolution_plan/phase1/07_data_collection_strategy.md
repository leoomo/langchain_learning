# æ•°æ®é‡‡é›†æˆ˜ç•¥å®æ–½æ–¹æ¡ˆ

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°äº†æ™ºèƒ½é’“é±¼ç”Ÿæ€ç³»ç»Ÿçš„æ•°æ®é‡‡é›†æˆ˜ç•¥å®æ–½ï¼ŒåŒ…æ‹¬æ•°æ®æºæ•´åˆã€é‡‡é›†æ¶æ„è®¾è®¡ã€è´¨é‡æ§åˆ¶æœºåˆ¶å’Œä¸ç°æœ‰ç³»ç»Ÿçš„æ— ç¼é›†æˆæ–¹æ¡ˆã€‚

## ğŸ¯ æˆ˜ç•¥ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡
1. **å…¨é¢æ•°æ®è¦†ç›–**ï¼šå»ºç«‹è¦†ç›–é±¼ç±»çŸ¥è¯†ã€è£…å¤‡ä¿¡æ¯ã€å¸‚åœºæ•°æ®ã€ç¯å¢ƒå› ç´ çš„å…¨æ–¹ä½æ•°æ®é‡‡é›†ä½“ç³»
2. **æ™ºèƒ½æ•°æ®æ²»ç†**ï¼šå®ç°è‡ªåŠ¨åŒ–æ•°æ®éªŒè¯ã€æ¸…æ´—ã€å»é‡å’Œæ ‡å‡†åŒ–å¤„ç†
3. **é«˜æ•ˆæ•°æ®åŒæ­¥**ï¼šå»ºç«‹å®æ—¶/å‡†å®æ—¶çš„å¤šæºæ•°æ®åŒæ­¥å’Œæ›´æ–°æœºåˆ¶
4. **è´¨é‡å¯è¿½æº¯**ï¼šå®ç°æ•°æ®æ¥æºã€å¤„ç†è¿‡ç¨‹ã€è´¨é‡æŒ‡æ ‡çš„å…¨ç¨‹å¯è¿½æº¯

### ä¸šåŠ¡ä»·å€¼
- **æ”¯æ’‘æ™ºèƒ½å†³ç­–**ï¼šä¸ºé±¼ç±»çŸ¥è¯†ç³»ç»Ÿã€è£…å¤‡æ¨èã€æ™ºèƒ½é¡¾é—®æä¾›é«˜è´¨é‡æ•°æ®åŸºç¡€
- **ä¿æŒç«äº‰ä¼˜åŠ¿**ï¼šé€šè¿‡æŒç»­çš„æ•°æ®é‡‡é›†å’Œåˆ†æä¿æŒè¡Œä¸šé¢†å…ˆåœ°ä½
- **é™ä½è¿è¥æˆæœ¬**ï¼šè‡ªåŠ¨åŒ–æ•°æ®é‡‡é›†å‡å°‘äººå·¥æˆæœ¬å’Œé”™è¯¯ç‡
- **æå‡ç”¨æˆ·ä½“éªŒ**ï¼šä¸°å¯Œçš„æ•°æ®æ”¯æ’‘æ›´ç²¾å‡†çš„æ¨èå’Œå’¨è¯¢æœåŠ¡

## ğŸ—ï¸ æ•°æ®é‡‡é›†æ¶æ„è®¾è®¡

### åˆ†å±‚æ¶æ„

```
æ•°æ®é‡‡é›†ç³»ç»Ÿ/
â”œâ”€â”€ ğŸ“ é‡‡é›†å±‚ (Collection Layer)          # æ•°æ®è·å–å’Œæ¥å…¥
â”‚   â”œâ”€â”€ ç½‘é¡µçˆ¬è™« (Web Crawlers)           # é’“é±¼ç½‘ç«™ã€è£…å¤‡è¯„æµ‹ã€å¸‚åœºæ•°æ®
â”‚   â”œâ”€â”€ APIæ¥å…¥å™¨ (API Connectors)       # å¤©æ°”APIã€åœ°ç†APIã€ç”µå•†å¹³å°API
â”‚   â”œâ”€â”€ ä¼ æ„Ÿå™¨æ¥å£ (Sensor Interfaces)    # IoTè®¾å¤‡ã€ç¯å¢ƒç›‘æµ‹ä¼ æ„Ÿå™¨
â”‚   â””â”€â”€ äººå·¥é‡‡é›†å™¨ (Manual Collectors)    # ä¸“å®¶çŸ¥è¯†ã€ç”¨æˆ·åé¦ˆé‡‡é›†
â”œâ”€â”€ ğŸ“ å¤„ç†å±‚ (Processing Layer)          # æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
â”‚   â”œâ”€â”€ æ•°æ®æ¸…æ´—å™¨ (Data Cleaners)       # æ ¼å¼æ ‡å‡†åŒ–ã€å»é‡ã€å¼‚å¸¸å€¼å¤„ç†
â”‚   â”œâ”€â”€ æ•°æ®éªŒè¯å™¨ (Data Validators)     # å®Œæ•´æ€§æ£€æŸ¥ã€ä¸€è‡´æ€§éªŒè¯
â”‚   â”œâ”€â”€ æ•°æ®ä¸°å¯Œå™¨ (Data Enrichers)      # æ•°æ®è¡¥å……ã€å…³è”åˆ†æ
â”‚   â””â”€â”€ æ•°æ®è½¬æ¢å™¨ (Data Transformers)   # æ ¼å¼è½¬æ¢ã€ç»“æ„åŒ–å¤„ç†
â”œâ”€â”€ ğŸ“ å­˜å‚¨å±‚ (Storage Layer)             # æ•°æ®å­˜å‚¨å’Œç®¡ç†
â”‚   â”œâ”€â”€ åŸå§‹æ•°æ®å­˜å‚¨ (Raw Data Storage)   # åŸå§‹æ•°æ®ä¿å­˜å’Œå¤‡ä»½
â”‚   â”œâ”€â”€ æ ‡å‡†æ•°æ®å­˜å‚¨ (Standard Storage)   # æ ‡å‡†åŒ–åçš„ç»“æ„åŒ–æ•°æ®
â”‚   â”œâ”€â”€ ç´¢å¼•æœåŠ¡ (Index Services)        # æ•°æ®æ£€ç´¢å’ŒæŸ¥è¯¢ä¼˜åŒ–
â”‚   â””â”€â”€ å½’æ¡£æœåŠ¡ (Archive Services)      # å†å²æ•°æ®å½’æ¡£ç®¡ç†
â””â”€â”€ ğŸ“ æœåŠ¡å±‚ (Service Layer)             # æ•°æ®æœåŠ¡å’Œæ¥å£
    â”œâ”€â”€ æ•°æ®æŸ¥è¯¢API (Data Query API)      # ç»Ÿä¸€æ•°æ®æŸ¥è¯¢æ¥å£
    â”œâ”€â”€ æ•°æ®è®¢é˜…API (Data Subscription API) # æ•°æ®å˜æ›´é€šçŸ¥æœåŠ¡
    â”œâ”€â”€ æ•°æ®åˆ†æAPI (Analytics API)       # æ•°æ®ç»Ÿè®¡å’Œåˆ†ææœåŠ¡
    â””â”€â”€ æ•°æ®è´¨é‡API (Quality API)         # æ•°æ®è´¨é‡ç›‘æ§å’ŒæŠ¥å‘Š
```

### æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 1. æ•°æ®é‡‡é›†åè°ƒå™¨ (Data Collection Coordinator)

```python
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum
import asyncio
import logging
from datetime import datetime, timedelta

@dataclass
class DataSource:
    """æ•°æ®æºé…ç½®"""
    source_id: str
    source_type: str  # 'web', 'api', 'sensor', 'manual'
    name: str
    description: str
    url: Optional[str] = None
    api_key: Optional[str] = None
    collection_frequency: str = 'daily'  # 'realtime', 'hourly', 'daily', 'weekly'
    priority: int = 5  # 1-10, 1ä¸ºæœ€é«˜ä¼˜å…ˆçº§
    enabled: bool = True
    last_collection: Optional[datetime] = None
    status: str = 'active'  # 'active', 'inactive', 'error'

@dataclass
class CollectionTask:
    """é‡‡é›†ä»»åŠ¡"""
    task_id: str
    data_source: DataSource
    scheduled_time: datetime
    status: str = 'pending'  # 'pending', 'running', 'completed', 'failed'
    retry_count: int = 0
    max_retries: int = 3
    result: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None

class DataCollectionCoordinator:
    """æ•°æ®é‡‡é›†åè°ƒå™¨"""

    def __init__(self):
        self.data_sources: Dict[str, DataSource] = {}
        self.collection_tasks: Dict[str, CollectionTask] = {}
        self.collectors: Dict[str, Any] = {}  # å„ç§é‡‡é›†å™¨å®ä¾‹
        self.task_queue: asyncio.Queue = asyncio.Queue()
        self.running = False
        self.logger = logging.getLogger(__name__)

    def register_data_source(self, data_source: DataSource):
        """æ³¨å†Œæ•°æ®æº"""
        self.data_sources[data_source.source_id] = data_source
        self.logger.info(f"Registered data source: {data_source.name}")

    async def schedule_collection(self, source_id: str):
        """è°ƒåº¦æ•°æ®é‡‡é›†"""
        data_source = self.data_sources.get(source_id)
        if not data_source or not data_source.enabled:
            return

        task = CollectionTask(
            task_id=f"{source_id}_{datetime.now().timestamp()}",
            data_source=data_source,
            scheduled_time=datetime.now()
        )

        self.collection_tasks[task.task_id] = task
        await self.task_queue.put(task)

    async def process_collection_task(self, task: CollectionTask):
        """å¤„ç†é‡‡é›†ä»»åŠ¡"""
        task.status = 'running'

        try:
            collector = self.collectors.get(task.data_source.source_type)
            if not collector:
                raise ValueError(f"No collector for source type: {task.data_source.source_type}")

            # æ‰§è¡Œæ•°æ®é‡‡é›†
            result = await collector.collect(task.data_source)
            task.result = result
            task.status = 'completed'

            # æ›´æ–°æ•°æ®æºæœ€åé‡‡é›†æ—¶é—´
            task.data_source.last_collection = datetime.now()

            # è§¦å‘æ•°æ®å¤„ç†æµç¨‹
            await self.trigger_data_processing(result, task.data_source)

        except Exception as e:
            task.error_message = str(e)
            task.retry_count += 1

            if task.retry_count <= task.max_retries:
                task.status = 'pending'
                await self.task_queue.put(task)  # é‡æ–°æ’é˜Ÿ
            else:
                task.status = 'failed'

            self.logger.error(f"Collection task failed: {task.task_id}, error: {e}")

    async def trigger_data_processing(self, data: Dict[str, Any], source: DataSource):
        """è§¦å‘æ•°æ®å¤„ç†"""
        # è¿™é‡Œä¼šè°ƒç”¨æ•°æ®å¤„ç†å±‚çš„æœåŠ¡
        processor = DataProcessingCoordinator()
        await processor.process_raw_data(data, source)

    async def start_collection_scheduler(self):
        """å¯åŠ¨é‡‡é›†è°ƒåº¦å™¨"""
        self.running = True

        while self.running:
            try:
                # è·å–å¾…å¤„ç†ä»»åŠ¡
                task = await asyncio.wait_for(self.task_queue.get(), timeout=1.0)
                await self.process_collection_task(task)

            except asyncio.TimeoutError:
                # æ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦å®šæ—¶é‡‡é›†çš„æ•°æ®æº
                await self.check_scheduled_collections()
            except Exception as e:
                self.logger.error(f"Scheduler error: {e}")

    async def check_scheduled_collections(self):
        """æ£€æŸ¥å®šæ—¶é‡‡é›†ä»»åŠ¡"""
        now = datetime.now()

        for source in self.data_sources.values():
            if not source.enabled:
                continue

            should_collect = False

            # æ£€æŸ¥é‡‡é›†é¢‘ç‡
            if source.collection_frequency == 'realtime':
                should_collect = True
            elif source.collection_frequency == 'hourly':
                if not source.last_collection or now - source.last_collection >= timedelta(hours=1):
                    should_collect = True
            elif source.collection_frequency == 'daily':
                if not source.last_collection or now - source.last_collection >= timedelta(days=1):
                    should_collect = True
            elif source.collection_frequency == 'weekly':
                if not source.last_collection or now - source.last_collection >= timedelta(weeks=1):
                    should_collect = True

            if should_collect:
                await self.schedule_collection(source.source_id)
```

#### 2. æ•°æ®å¤„ç†åè°ƒå™¨ (Data Processing Coordinator)

```python
from abc import ABC, abstractmethod
import re
import hashlib
from typing import Set, Tuple

class DataProcessor(ABC):
    """æ•°æ®å¤„ç†å™¨åŸºç±»"""

    @abstractmethod
    async def process(self, raw_data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:
        """å¤„ç†åŸå§‹æ•°æ®"""
        pass

class DataCleaner(DataProcessor):
    """æ•°æ®æ¸…æ´—å™¨"""

    async def process(self, raw_data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:
        """æ¸…æ´—æ•°æ®"""
        cleaned_data = {}

        for key, value in raw_data.items():
            # å»é™¤ç©ºç™½å­—ç¬¦
            if isinstance(value, str):
                value = value.strip()
                # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
                value = re.sub(r'\s+', ' ', value)

            # å¤„ç†ç©ºå€¼
            if value in ['', 'null', 'NULL', 'N/A', 'n/a']:
                value = None

            cleaned_data[key] = value

        return {
            'original_data': raw_data,
            'cleaned_data': cleaned_data,
            'source_info': {
                'source_id': source.source_id,
                'collection_time': datetime.now().isoformat()
            }
        }

class DataValidator(DataProcessor):
    """æ•°æ®éªŒè¯å™¨"""

    def __init__(self):
        self.validation_rules = self._load_validation_rules()

    def _load_validation_rules(self) -> Dict[str, Dict]:
        """åŠ è½½éªŒè¯è§„åˆ™"""
        return {
            'fish_species': {
                'required_fields': ['name', 'family', 'habitat'],
                'field_types': {
                    'name': str,
                    'family': str,
                    'habitat': str,
                    'size_min': (int, float),
                    'size_max': (int, float)
                }
            },
            'equipment': {
                'required_fields': ['name', 'category', 'brand'],
                'field_types': {
                    'name': str,
                    'category': str,
                    'brand': str,
                    'price': (int, float),
                    'rating': (int, float)
                }
            }
        }

    async def process(self, data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®"""
        validation_result = {
            'is_valid': True,
            'errors': [],
            'warnings': [],
            'cleaned_data': data
        }

        # æ ¹æ®æ•°æ®æºç±»å‹é€‰æ‹©éªŒè¯è§„åˆ™
        data_type = self._detect_data_type(data)
        rules = self.validation_rules.get(data_type, {})

        # æ£€æŸ¥å¿…å¡«å­—æ®µ
        required_fields = rules.get('required_fields', [])
        for field in required_fields:
            if field not in data or data[field] is None:
                validation_result['errors'].append(f"Missing required field: {field}")
                validation_result['is_valid'] = False

        # æ£€æŸ¥å­—æ®µç±»å‹
        field_types = rules.get('field_types', {})
        for field, expected_type in field_types.items():
            if field in data and data[field] is not None:
                if isinstance(expected_type, tuple):
                    expected_type = expected_type
                if not isinstance(data[field], expected_type):
                    validation_result['errors'].append(f"Invalid type for {field}: expected {expected_type}, got {type(data[field])}")
                    validation_result['is_valid'] = False

        return validation_result

    def _detect_data_type(self, data: Dict[str, Any]) -> str:
        """æ£€æµ‹æ•°æ®ç±»å‹"""
        if 'name' in data and 'family' in data:
            return 'fish_species'
        elif 'category' in data and 'brand' in data:
            return 'equipment'
        else:
            return 'unknown'

class DataDeduplicator(DataProcessor):
    """æ•°æ®å»é‡å™¨"""

    def __init__(self):
        self.seen_hashes: Set[str] = set()

    async def process(self, data: Dict[str, Any], source: DataSource) -> Dict[str, Any]:
        """å»é‡å¤„ç†"""
        # ç”Ÿæˆæ•°æ®æŒ‡çº¹
        data_hash = self._generate_data_hash(data)

        if data_hash in self.seen_hashes:
            return {
                'is_duplicate': True,
                'data_hash': data_hash,
                'original_data': data
            }

        self.seen_hashes.add(data_hash)
        return {
            'is_duplicate': False,
            'data_hash': data_hash,
            'original_data': data
        }

    def _generate_data_hash(self, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æŒ‡çº¹"""
        # åºåˆ—åŒ–æ•°æ®å¹¶ç”Ÿæˆå“ˆå¸Œ
        data_str = str(sorted(data.items()))
        return hashlib.md5(data_str.encode()).hexdigest()

class DataProcessingCoordinator:
    """æ•°æ®å¤„ç†åè°ƒå™¨"""

    def __init__(self):
        self.processors: List[DataProcessor] = [
            DataCleaner(),
            DataValidator(),
            DataDeduplicator()
        ]
        self.logger = logging.getLogger(__name__)

    async def process_raw_data(self, raw_data: Dict[str, Any], source: DataSource):
        """å¤„ç†åŸå§‹æ•°æ®"""
        processing_result = {
            'original_data': raw_data,
            'processing_steps': [],
            'final_data': raw_data,
            'is_valid': True,
            'errors': []
        }

        current_data = raw_data

        # ä¾æ¬¡æ‰§è¡Œå„ä¸ªå¤„ç†å™¨
        for processor in self.processors:
            try:
                step_result = await processor.process(current_data, source)

                processing_result['processing_steps'].append({
                    'processor': processor.__class__.__name__,
                    'result': step_result
                })

                # æ ¹æ®å¤„ç†ç»“æœæ›´æ–°å½“å‰æ•°æ®
                if hasattr(step_result, 'get') and 'cleaned_data' in step_result:
                    current_data = step_result['cleaned_data']
                elif hasattr(step_result, 'get') and 'original_data' in step_result:
                    current_data = step_result['original_data']

                # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤æ•°æ®
                if hasattr(step_result, 'get') and step_result.get('is_duplicate'):
                    self.logger.info(f"Duplicate data detected from source {source.source_id}")
                    return

                # æ£€æŸ¥éªŒè¯ç»“æœ
                if hasattr(step_result, 'get') and not step_result.get('is_valid', True):
                    processing_result['is_valid'] = False
                    processing_result['errors'].extend(step_result.get('errors', []))
                    break

            except Exception as e:
                self.logger.error(f"Processing error with {processor.__class__.__name__}: {e}")
                processing_result['errors'].append(f"Processing error: {e}")
                processing_result['is_valid'] = False
                break

        processing_result['final_data'] = current_data

        # å¦‚æœæ•°æ®æœ‰æ•ˆï¼Œä¿å­˜åˆ°å­˜å‚¨å±‚
        if processing_result['is_valid']:
            await self.save_processed_data(current_data, source, processing_result)

        return processing_result

    async def save_processed_data(self, data: Dict[str, Any], source: DataSource, processing_result: Dict):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        # è¿™é‡Œä¼šè°ƒç”¨å­˜å‚¨å±‚çš„æœåŠ¡
        storage_service = DataStorageService()
        await storage_service.store_data(data, source, processing_result)
```

#### 3. æ•°æ®å­˜å‚¨æœåŠ¡ (Data Storage Service)

```python
import sqlite3
import json
from typing import List, Optional, Dict, Any

class DataStorageService:
    """æ•°æ®å­˜å‚¨æœåŠ¡"""

    def __init__(self, db_path: str = "data/collection_database.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # åˆ›å»ºåŸå§‹æ•°æ®è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS raw_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id TEXT NOT NULL,
                data_hash TEXT UNIQUE,
                raw_content TEXT NOT NULL,
                collection_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                processed BOOLEAN DEFAULT FALSE
            )
        """)

        # åˆ›å»ºå¤„ç†åæ•°æ®è¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS processed_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                raw_data_id INTEGER,
                data_type TEXT NOT NULL,
                content TEXT NOT NULL,
                processing_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                quality_score REAL,
                FOREIGN KEY (raw_data_id) REFERENCES raw_data (id)
            )
        """)

        # åˆ›å»ºæ•°æ®æºè¡¨
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS data_sources (
                source_id TEXT PRIMARY KEY,
                name TEXT NOT NULL,
                source_type TEXT NOT NULL,
                config TEXT,
                last_collection DATETIME,
                status TEXT DEFAULT 'active'
            )
        """)

        # åˆ›å»ºç´¢å¼•
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_raw_data_source ON raw_data(source_id)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_processed_data_type ON processed_data(data_type)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_data_hash ON raw_data(data_hash)")

        conn.commit()
        conn.close()

    async def store_data(self, data: Dict[str, Any], source: DataSource, processing_result: Dict):
        """å­˜å‚¨æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            # å­˜å‚¨åŸå§‹æ•°æ®
            data_hash = processing_result.get('processing_steps', [{}])[-1].get('result', {}).get('data_hash', '')

            cursor.execute("""
                INSERT OR IGNORE INTO raw_data (source_id, data_hash, raw_content, processed)
                VALUES (?, ?, ?, ?)
            """, (
                source.source_id,
                data_hash,
                json.dumps(data, ensure_ascii=False),
                True
            ))

            # è·å–åŸå§‹æ•°æ®ID
            cursor.execute("SELECT id FROM raw_data WHERE data_hash = ?", (data_hash,))
            raw_data_row = cursor.fetchone()

            if raw_data_row:
                raw_data_id = raw_data_row[0]

                # æ£€æµ‹æ•°æ®ç±»å‹
                data_type = self._detect_data_type(data)

                # å­˜å‚¨å¤„ç†åçš„æ•°æ®
                cursor.execute("""
                    INSERT INTO processed_data (raw_data_id, data_type, content, quality_score)
                    VALUES (?, ?, ?, ?)
                """, (
                    raw_data_id,
                    data_type,
                    json.dumps(data, ensure_ascii=False),
                    self._calculate_quality_score(data, processing_result)
                ))

            conn.commit()
            self.logger.info(f"Data stored successfully from source {source.source_id}")

        except Exception as e:
            conn.rollback()
            self.logger.error(f"Failed to store data: {e}")
        finally:
            conn.close()

    def _detect_data_type(self, data: Dict[str, Any]) -> str:
        """æ£€æµ‹æ•°æ®ç±»å‹"""
        if 'name' in data and ('family' in data or 'species' in data.lower()):
            return 'fish_species'
        elif 'category' in data or 'brand' in data:
            return 'equipment'
        elif 'temperature' in data or 'weather' in data:
            return 'weather'
        else:
            return 'general'

    def _calculate_quality_score(self, data: Dict[str, Any], processing_result: Dict) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        score = 100.0

        # æ ¹æ®å¤„ç†é”™è¯¯æ‰£åˆ†
        errors = processing_result.get('errors', [])
        score -= len(errors) * 10

        # æ ¹æ®æ•°æ®å®Œæ•´æ€§è¯„åˆ†
        if isinstance(data, dict):
            non_null_fields = sum(1 for v in data.values() if v is not None and v != '')
            total_fields = len(data)
            if total_fields > 0:
                completeness = non_null_fields / total_fields
                score *= completeness

        return max(0.0, min(100.0, score))

    async def query_data(self, data_type: str, limit: int = 100, filters: Optional[Dict] = None) -> List[Dict]:
        """æŸ¥è¯¢æ•°æ®"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = """
            SELECT pd.content, pd.quality_score, ds.name as source_name, pd.processing_time
            FROM processed_data pd
            JOIN raw_data rd ON pd.raw_data_id = rd.id
            JOIN data_sources ds ON rd.source_id = ds.source_id
            WHERE pd.data_type = ?
        """
        params = [data_type]

        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            if 'min_quality_score' in filters:
                query += " AND pd.quality_score >= ?"
                params.append(filters['min_quality_score'])

            if 'source_id' in filters:
                query += " AND rd.source_id = ?"
                params.append(filters['source_id'])

        query += " ORDER BY pd.quality_score DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        rows = cursor.fetchall()

        results = []
        for row in rows:
            results.append({
                'content': json.loads(row[0]),
                'quality_score': row[1],
                'source_name': row[2],
                'processing_time': row[3]
            })

        conn.close()
        return results
```

## ğŸ¯ å…·ä½“æ•°æ®é‡‡é›†ç­–ç•¥

### 1. é±¼ç±»çŸ¥è¯†æ•°æ®é‡‡é›†

#### æ•°æ®æºæ¸…å•
- **å­¦æœ¯èµ„æº**: FishBaseã€FAOæ¸”ä¸šç»Ÿè®¡ã€å­¦æœ¯è®ºæ–‡æ•°æ®åº“
- **æ”¿åºœèµ„æº**: å†œä¸šå†œæ‘éƒ¨æ¸”ä¸šå±€ã€å„çœå¸‚æ¸”ä¸šç®¡ç†éƒ¨é—¨
- **ä¸“ä¸šç½‘ç«™**: ä¸­å›½é’“é±¼ç½‘ã€é’“é±¼äººã€å››æµ·é’“é±¼
- **ç¤¾åŒºèµ„æº**: é’“é±¼è®ºå›ã€å¾®ä¿¡ç¾¤ã€QQç¾¤çŸ¥è¯†æ•´ç†
- **ä¸“å®¶çŸ¥è¯†**: é’“é±¼ä¸“å®¶è®¿è°ˆã€èŒä¸šé’“æ‰‹ç»éªŒ

#### é‡‡é›†ç­–ç•¥
```python
@dataclass
class FishKnowledgeSource(DataSource):
    """é±¼ç±»çŸ¥è¯†æ•°æ®æº"""
    data_categories: List[str]  # ['species', 'behavior', 'habitat', 'techniques']
    geographic_coverage: List[str]  # ['national', 'regional', 'local']
    reliability_score: float  # 0-1, æ•°æ®å¯é æ€§è¯„åˆ†
    update_frequency: str
    language: str = 'zh'

class FishKnowledgeCollector:
    """é±¼ç±»çŸ¥è¯†é‡‡é›†å™¨"""

    def __init__(self):
        self.species_extractors = {
            'fishbase': FishBaseExtractor(),
            'government': GovernmentDataExtractor(),
            'forum': ForumKnowledgeExtractor(),
            'expert': ExpertKnowledgeExtractor()
        }

    async def collect_species_data(self, source: FishKnowledgeSource) -> Dict[str, Any]:
        """é‡‡é›†é±¼ç§æ•°æ®"""
        extractor = self.species_extractors.get(source.source_type)
        if not extractor:
            raise ValueError(f"No extractor for source type: {source.source_type}")

        # é‡‡é›†åŸºç¡€ä¿¡æ¯
        basic_info = await extractor.extract_basic_info(source)

        # é‡‡é›†è¡Œä¸ºæ¨¡å¼
        behavior_info = await extractor.extract_behavior_patterns(source)

        # é‡‡é›†æ –æ¯åœ°ä¿¡æ¯
        habitat_info = await extractor.extract_habitat_info(source)

        # é‡‡é›†é’“é±¼æŠ€å·§
        technique_info = await extractor.extract_fishing_techniques(source)

        return {
            'species_info': basic_info,
            'behavior_patterns': behavior_info,
            'habitat_info': habitat_info,
            'fishing_techniques': technique_info,
            'metadata': {
                'source': source.name,
                'collection_time': datetime.now().isoformat(),
                'reliability_score': source.reliability_score
            }
        }
```

### 2. è£…å¤‡ä¿¡æ¯æ•°æ®é‡‡é›†

#### æ•°æ®æºæ¸…å•
- **ç”µå•†å¹³å°**: æ·˜å®ã€äº¬ä¸œã€å¤©çŒ«ã€æ‹¼å¤šå¤š
- **ä¸“ä¸šæ¸”å…·**: è€é¬¼ã€åŒ–æ°ã€å¤©å…ƒã€å…‰å¨
- **è¯„æµ‹ç½‘ç«™**: ä¸­é’“ç½‘ã€é’“å¤šå¤šã€æ¸”å…·è¯„æµ‹
- **ç”¨æˆ·è¯„ä»·**: ç”µå•†å¹³å°ç”¨æˆ·è¯„ä»·ã€è®ºå›è®¨è®º
- **ä»·æ ¼ç›‘æ§**: å†å²ä»·æ ¼èµ°åŠ¿ã€ä¿ƒé”€ä¿¡æ¯

#### é‡‡é›†ç­–ç•¥
```python
class EquipmentDataCollector:
    """è£…å¤‡æ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        self.collectors = {
            'ecommerce': EcommerceDataCollector(),
            'brand': BrandDataCollector(),
            'review': ReviewDataCollector(),
            'price': PriceMonitorCollector()
        }

    async def collect_equipment_info(self, category: str, brand: str = None) -> List[Dict[str, Any]]:
        """é‡‡é›†è£…å¤‡ä¿¡æ¯"""
        results = []

        # ç”µå•†å¹³å°æ•°æ®é‡‡é›†
        ecommerce_data = await self.collectors['ecommerce'].collect_product_data(category, brand)
        results.extend(ecommerce_data)

        # å“ç‰Œå®˜æ–¹æ•°æ®é‡‡é›†
        if brand:
            brand_data = await self.collectors['brand'].collect_brand_products(category, brand)
            results.extend(brand_data)

        # è¯„æµ‹æ•°æ®é‡‡é›†
        review_data = await self.collectors['review'].collect_review_data(category, brand)
        results.extend(review_data)

        # ä»·æ ¼æ•°æ®é‡‡é›†
        price_data = await self.collectors['price'].collect_price_data(category, brand)
        results.extend(price_data)

        # å»é‡å’Œæ•´åˆ
        return self._merge_equipment_data(results)

    def _merge_equipment_data(self, raw_data: List[Dict]) -> List[Dict]:
        """åˆå¹¶è£…å¤‡æ•°æ®"""
        merged_data = {}

        for item in raw_data:
            # ä½¿ç”¨äº§å“åç§°+å“ç‰Œä½œä¸ºåˆå¹¶é”®
            merge_key = f"{item.get('brand', '')}_{item.get('name', '')}"

            if merge_key not in merged_data:
                merged_data[merge_key] = item.copy()
            else:
                # åˆå¹¶ä¿¡æ¯ï¼Œä¿ç•™æœ€å®Œæ•´çš„æ•°æ®
                existing = merged_data[merge_key]

                # åˆå¹¶ä»·æ ¼ä¿¡æ¯
                if 'price' in item and item['price'] != existing.get('price'):
                    existing['price_range'] = [
                        min(existing.get('price', float('inf')), item.get('price', float('inf'))),
                        max(existing.get('price', 0), item.get('price', 0))
                    ]

                # åˆå¹¶è¯„åˆ†ä¿¡æ¯
                if 'rating' in item:
                    ratings = existing.get('ratings', [])
                    ratings.append(item['rating'])
                    existing['average_rating'] = sum(ratings) / len(ratings)
                    existing['ratings_count'] = len(ratings)

                # åˆå¹¶æ¥æºä¿¡æ¯
                sources = existing.get('sources', [])
                sources.append(item.get('source', 'unknown'))
                existing['sources'] = list(set(sources))

        return list(merged_data.values())
```

### 3. ç¯å¢ƒæ•°æ®é‡‡é›†

#### æ•°æ®æºæ¸…å•
- **å¤©æ°”æ•°æ®**: å½©äº‘å¤©æ°”ã€ä¸­å›½å¤©æ°”ç½‘ã€æ°”è±¡å±€
- **æ°´æ–‡æ•°æ®**: æ°´åˆ©éƒ¨ã€å„åœ°æ°´æ–‡å±€
- **åœ°ç†æ•°æ®**: é«˜å¾·åœ°å›¾ã€ç™¾åº¦åœ°å›¾ã€å¤©åœ°å›¾
- **ç¯å¢ƒç›‘æµ‹**: ç¯ä¿éƒ¨ã€å„åœ°ç¯ä¿å±€

#### é‡‡é›†ç­–ç•¥
```python
class EnvironmentalDataCollector:
    """ç¯å¢ƒæ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        self.weather_collectors = {
            'caiyun': CaiyunWeatherCollector(),
            'weather_cn': WeatherChinaCollector(),
            'meteorology': MeteorologyBureauCollector()
        }

        self.geographic_collectors = {
            'amap': AmapCollector(),
            'baidu': BaiduMapCollector(),
            'tianditu': TiandituCollector()
        }

    async def collect_weather_data(self, location: str, date_range: int = 7) -> Dict[str, Any]:
        """é‡‡é›†å¤©æ°”æ•°æ®"""
        weather_data = {}

        # å¹¶è¡Œé‡‡é›†å¤šä¸ªå¤©æ°”æ•°æ®æº
        tasks = []
        for collector_name, collector in self.weather_collectors.items():
            task = collector.collect_weather_forecast(location, date_range)
            tasks.append((collector_name, task))

        results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)

        # æ•´åˆå¤šæºæ•°æ®
        for (collector_name, _), result in zip(tasks, results):
            if isinstance(result, Exception):
                logging.error(f"Weather data collection failed from {collector_name}: {result}")
                continue

            weather_data[collector_name] = result

        # æ•°æ®èåˆå’ŒéªŒè¯
        return self._merge_weather_data(weather_data)

    def _merge_weather_data(self, multi_source_data: Dict[str, Any]) -> Dict[str, Any]:
        """èåˆå¤šæºå¤©æ°”æ•°æ®"""
        merged_data = {
            'location': None,
            'forecasts': [],
            'data_sources': list(multi_source_data.keys()),
            'collection_time': datetime.now().isoformat()
        }

        # æå–ä½ç½®ä¿¡æ¯
        for source_name, data in multi_source_data.items():
            if 'location' in data:
                merged_data['location'] = data['location']
                break

        # åˆå¹¶é¢„æŠ¥æ•°æ®
        all_forecasts = []
        for source_name, data in multi_source_data.items():
            if 'forecasts' in data:
                for forecast in data['forecasts']:
                    forecast['source'] = source_name
                    all_forecasts.append(forecast)

        # æŒ‰æ—¥æœŸåˆ†ç»„å¹¶è®¡ç®—å¹³å‡å€¼
        forecasts_by_date = {}
        for forecast in all_forecasts:
            date = forecast.get('date')
            if date:
                if date not in forecasts_by_date:
                    forecasts_by_date[date] = []
                forecasts_by_date[date].append(forecast)

        # ç”Ÿæˆèåˆåçš„é¢„æŠ¥
        for date, daily_forecasts in forecasts_by_date.items():
            merged_forecast = self._merge_daily_forecasts(daily_forecasts)
            merged_forecast['date'] = date
            merged_data['forecasts'].append(merged_forecast)

        return merged_data

    def _merge_daily_forecasts(self, forecasts: List[Dict]) -> Dict[str, Any]:
        """åˆå¹¶å•æ—¥å¤šæºé¢„æŠ¥"""
        if not forecasts:
            return {}

        # æ•°å€¼å‹å­—æ®µå–å¹³å‡å€¼
        numeric_fields = ['temperature_max', 'temperature_min', 'humidity', 'pressure', 'wind_speed']
        merged = {}

        for field in numeric_fields:
            values = [f.get(field) for f in forecasts if f.get(field) is not None]
            if values:
                merged[field] = sum(values) / len(values)

        # åˆ†ç±»å­—æ®µé€‰æ‹©æœ€ä¸€è‡´çš„å€¼
        categorical_fields = ['weather_condition', 'wind_direction']
        for field in categorical_fields:
            values = [f.get(field) for f in forecasts if f.get(field)]
            if values:
                # é€‰æ‹©å‡ºç°é¢‘ç‡æœ€é«˜çš„å€¼
                from collections import Counter
                counter = Counter(values)
                merged[field] = counter.most_common(1)[0][0]

        return merged
```

## ğŸ”„ ä¸ç°æœ‰ç³»ç»Ÿé›†æˆ

### 1. é›†æˆåˆ°ç»Ÿä¸€æœåŠ¡ç®¡ç†å™¨

```python
# æ‰©å±•ç°æœ‰çš„ ServiceManager
class EnhancedServiceManager(ServiceManager):
    """å¢å¼ºå‹æœåŠ¡ç®¡ç†å™¨ï¼Œé›†æˆæ•°æ®é‡‡é›†æœåŠ¡"""

    def __init__(self):
        super().__init__()
        self._register_data_collection_services()

    def _register_data_collection_services(self):
        """æ³¨å†Œæ•°æ®é‡‡é›†ç›¸å…³æœåŠ¡"""

        # æ•°æ®é‡‡é›†åè°ƒå™¨
        self.register_service('data_collection_coordinator', lambda: DataCollectionCoordinator())

        # æ•°æ®å¤„ç†åè°ƒå™¨
        self.register_service('data_processing_coordinator', lambda: DataProcessingCoordinator())

        # æ•°æ®å­˜å‚¨æœåŠ¡
        self.register_service('data_storage_service', lambda: DataStorageService())

        # é±¼ç±»çŸ¥è¯†é‡‡é›†å™¨
        self.register_service('fish_knowledge_collector', lambda: FishKnowledgeCollector())

        # è£…å¤‡æ•°æ®é‡‡é›†å™¨
        self.register_service('equipment_data_collector', lambda: EquipmentDataCollector())

        # ç¯å¢ƒæ•°æ®é‡‡é›†å™¨
        self.register_service('environmental_data_collector', lambda: EnvironmentalDataCollector())

        # æ•°æ®è´¨é‡ç›‘æ§æœåŠ¡
        self.register_service('data_quality_monitor', lambda: DataQualityMonitor())

        # æ•°æ®åŒæ­¥æœåŠ¡
        self.register_service('data_sync_service', lambda: DataSyncService())
```

### 2. LangChainå·¥å…·é›†æˆ

```python
# æ•°æ®é‡‡é›†ç›¸å…³çš„LangChainå·¥å…·
from langchain.tools import tool

@tool
def get_comprehensive_fish_data(fish_name: str, location: str = None) -> str:
    """
    è·å–é±¼ç±»ç»¼åˆæ•°æ®ï¼ŒåŒ…æ‹¬åŸºç¡€ä¿¡æ¯ã€è¡Œä¸ºæ¨¡å¼ã€æ –æ¯åœ°åå¥½ç­‰

    Args:
        fish_name: é±¼ç§åç§°
        location: åœ°ç†ä½ç½®ï¼ˆå¯é€‰ï¼‰

    Returns:
        é±¼ç±»ç»¼åˆæ•°æ®ä¿¡æ¯
    """
    service_manager = EnhancedServiceManager()
    storage_service = service_manager.get_service('data_storage_service')

    # æŸ¥è¯¢é±¼ç§æ•°æ®
    fish_data = asyncio.run(storage_service.query_data(
        data_type='fish_species',
        limit=10,
        filters={'min_quality_score': 80.0}
    ))

    # è¿‡æ»¤åŒ¹é…çš„é±¼ç§
    matching_fish = []
    for fish in fish_data:
        content = fish['content']
        if (fish_name.lower() in content.get('name', '').lower() or
            fish_name.lower() in content.get('scientific_name', '').lower()):
            matching_fish.append(fish)

    if not matching_fish:
        return f"æœªæ‰¾åˆ°å…³äº {fish_name} çš„è¯¦ç»†æ•°æ®"

    # æ ¼å¼åŒ–è¿”å›ç»“æœ
    result = f"## {fish_name} ç»¼åˆæ•°æ®\n\n"

    for fish in matching_fish:
        content = fish['content']
        result += f"**æ¥æº**: {fish['source_name']} (è´¨é‡è¯„åˆ†: {fish['quality_score']:.1f})\n\n"

        if 'species_info' in content:
            result += f"**åŸºæœ¬ä¿¡æ¯**: {content['species_info']}\n\n"

        if 'behavior_patterns' in content:
            result += f"**è¡Œä¸ºæ¨¡å¼**: {content['behavior_patterns']}\n\n"

        if 'habitat_info' in content:
            result += f"**æ –æ¯åœ°ä¿¡æ¯**: {content['habitat_info']}\n\n"

        if 'fishing_techniques' in content:
            result += f"**é’“é±¼æŠ€å·§**: {content['fishing_techniques']}\n\n"

        result += "---\n\n"

    return result

@tool
def get_equipment_recommendations_data(category: str, budget_range: tuple = None) -> str:
    """
    è·å–è£…å¤‡æ¨èæ•°æ®ï¼ŒåŒ…æ‹¬äº§å“ä¿¡æ¯ã€ä»·æ ¼ã€è¯„ä»·ç­‰

    Args:
        category: è£…å¤‡ç±»åˆ« (å¦‚: "é±¼ç«¿", "é±¼çº¿", "é±¼é’©")
        budget_range: é¢„ç®—èŒƒå›´ (min_price, max_price)

    Returns:
        è£…å¤‡æ¨èæ•°æ®
    """
    service_manager = EnhancedServiceManager()
    storage_service = service_manager.get_service('data_storage_service')

    # æŸ¥è¯¢è£…å¤‡æ•°æ®
    equipment_data = asyncio.run(storage_service.query_data(
        data_type='equipment',
        limit=20,
        filters={'min_quality_score': 75.0}
    ))

    # è¿‡æ»¤å’Œæ’åº
    filtered_equipment = []
    for equipment in equipment_data:
        content = equipment['content']

        # ç±»åˆ«åŒ¹é…
        if category.lower() not in content.get('category', '').lower():
            continue

        # é¢„ç®—è¿‡æ»¤
        if budget_range:
            price = content.get('price', 0)
            if price < budget_range[0] or price > budget_range[1]:
                continue

        filtered_equipment.append(equipment)

    if not filtered_equipment:
        return f"æœªæ‰¾åˆ° {category} ç±»åˆ«çš„è£…å¤‡æ¨èæ•°æ®"

    # æŒ‰è¯„åˆ†æ’åº
    filtered_equipment.sort(key=lambda x: x['quality_score'], reverse=True)

    # æ ¼å¼åŒ–è¿”å›ç»“æœ
    result = f"## {category} è£…å¤‡æ¨èæ•°æ®\n\n"

    for equipment in filtered_equipment[:10]:  # è¿”å›å‰10ä¸ª
        content = equipment['content']
        result += f"### {content.get('brand', '')} {content.get('name', '')}\n\n"
        result += f"- **ä»·æ ¼**: Â¥{content.get('price', 'N/A')}\n"
        result += f"- **è¯„åˆ†**: {content.get('average_rating', 'N/A')}/5.0 ({content.get('ratings_count', 0)}æ¡è¯„ä»·)\n"
        result += f"- **æ¥æº**: {equipment['source_name']} (æ•°æ®è´¨é‡: {equipment['quality_score']:.1f})\n\n"

    return result

@tool
def trigger_data_collection(source_type: str, source_id: str = None) -> str:
    """
    è§¦å‘ç‰¹å®šç±»å‹çš„æ•°æ®é‡‡é›†

    Args:
        source_type: æ•°æ®æºç±»å‹ ('fish_knowledge', 'equipment', 'environmental')
        source_id: ç‰¹å®šæ•°æ®æºIDï¼ˆå¯é€‰ï¼‰

    Returns:
        æ•°æ®é‡‡é›†è§¦å‘ç»“æœ
    """
    service_manager = EnhancedServiceManager()
    coordinator = service_manager.get_service('data_collection_coordinator')

    try:
        if source_id:
            # è§¦å‘ç‰¹å®šæ•°æ®æºé‡‡é›†
            asyncio.run(coordinator.schedule_collection(source_id))
            return f"å·²è§¦å‘æ•°æ®æº {source_id} çš„æ•°æ®é‡‡é›†"
        else:
            # è§¦å‘ç‰¹å®šç±»å‹çš„æ‰€æœ‰æ•°æ®æº
            count = 0
            for source in coordinator.data_sources.values():
                if source.source_type == source_type and source.enabled:
                    asyncio.run(coordinator.schedule_collection(source.source_id))
                    count += 1

            return f"å·²è§¦å‘ {count} ä¸ª {source_type} ç±»å‹çš„æ•°æ®æºè¿›è¡Œé‡‡é›†"

    except Exception as e:
        return f"è§¦å‘æ•°æ®é‡‡é›†å¤±è´¥: {str(e)}"

# æ‰©å±•ç°æœ‰çš„æ™ºèƒ½é’“é±¼å·¥å…·é›†
intelligent_fishing_tools.extend([
    get_comprehensive_fish_data,
    get_equipment_recommendations_data,
    trigger_data_collection
])
```

## ğŸ“Š æ•°æ®è´¨é‡ç›‘æ§

### 1. æ•°æ®è´¨é‡æŒ‡æ ‡

```python
@dataclass
class DataQualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""
    completeness_score: float      # å®Œæ•´æ€§è¯„åˆ† (0-100)
    accuracy_score: float          # å‡†ç¡®æ€§è¯„åˆ† (0-100)
    consistency_score: float       # ä¸€è‡´æ€§è¯„åˆ† (0-100)
    timeliness_score: float        # åŠæ—¶æ€§è¯„åˆ† (0-100)
    validity_score: float          # æœ‰æ•ˆæ€§è¯„åˆ† (0-100)
    uniqueness_score: float        # å”¯ä¸€æ€§è¯„åˆ† (0-100)
    overall_score: float           # ç»¼åˆè¯„åˆ† (0-100)

class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""

    def __init__(self):
        self.quality_history: Dict[str, List[DataQualityMetrics]] = {}
        self.quality_thresholds = {
            'completeness': 80.0,
            'accuracy': 85.0,
            'consistency': 90.0,
            'timeliness': 75.0,
            'validity': 95.0,
            'uniqueness': 98.0,
            'overall': 80.0
        }

    async def assess_data_quality(self, data: Dict[str, Any], data_type: str, source: DataSource) -> DataQualityMetrics:
        """è¯„ä¼°æ•°æ®è´¨é‡"""

        # å®Œæ•´æ€§è¯„ä¼°
        completeness_score = self._assess_completeness(data, data_type)

        # å‡†ç¡®æ€§è¯„ä¼°
        accuracy_score = await self._assess_accuracy(data, data_type, source)

        # ä¸€è‡´æ€§è¯„ä¼°
        consistency_score = await self._assess_consistency(data, data_type)

        # åŠæ—¶æ€§è¯„ä¼°
        timeliness_score = self._assess_timeliness(data, source)

        # æœ‰æ•ˆæ€§è¯„ä¼°
        validity_score = self._assess_validity(data, data_type)

        # å”¯ä¸€æ€§è¯„ä¼°
        uniqueness_score = await self._assess_uniqueness(data, data_type)

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        overall_score = (
            completeness_score * 0.2 +
            accuracy_score * 0.25 +
            consistency_score * 0.2 +
            timeliness_score * 0.15 +
            validity_score * 0.1 +
            uniqueness_score * 0.1
        )

        metrics = DataQualityMetrics(
            completeness_score=completeness_score,
            accuracy_score=accuracy_score,
            consistency_score=consistency_score,
            timeliness_score=timeliness_score,
            validity_score=validity_score,
            uniqueness_score=uniqueness_score,
            overall_score=overall_score
        )

        # è®°å½•è´¨é‡å†å²
        if data_type not in self.quality_history:
            self.quality_history[data_type] = []

        self.quality_history[data_type].append(metrics)

        # æ£€æŸ¥è´¨é‡é˜ˆå€¼å‘Šè­¦
        await self._check_quality_alerts(data_type, metrics)

        return metrics

    def _assess_completeness(self, data: Dict[str, Any], data_type: str) -> float:
        """è¯„ä¼°æ•°æ®å®Œæ•´æ€§"""
        if not isinstance(data, dict):
            return 0.0

        # å®šä¹‰æ¯ç§æ•°æ®ç±»å‹çš„å¿…éœ€å­—æ®µ
        required_fields = {
            'fish_species': ['name', 'family', 'habitat'],
            'equipment': ['name', 'category', 'brand'],
            'weather': ['temperature', 'condition', 'date'],
            'general': ['name']  # æœ€åŸºæœ¬è¦æ±‚
        }

        fields = required_fields.get(data_type, required_fields['general'])

        # è®¡ç®—å­—æ®µå®Œæ•´ç‡
        non_null_count = sum(1 for field in fields if field in data and data[field] is not None)
        completeness = (non_null_count / len(fields)) * 100 if fields else 100.0

        # è€ƒè™‘å¯é€‰å­—æ®µçš„è¦†ç›–ç‡
        all_fields = list(data.keys())
        required_count = len(fields)
        total_count = len(all_fields)

        if total_count > required_count:
            optional_coverage = min(20.0, ((total_count - required_count) / required_count) * 20.0)
            completeness += optional_coverage

        return min(100.0, completeness)

    async def _assess_accuracy(self, data: Dict[str, Any], data_type: str, source: DataSource) -> float:
        """è¯„ä¼°æ•°æ®å‡†ç¡®æ€§"""
        # åŸºäºæ•°æ®æºçš„å¯é æ€§è¯„åˆ†
        base_accuracy = getattr(source, 'reliability_score', 0.8) * 100

        # æ£€æŸ¥æ•°æ®æ ¼å¼å‡†ç¡®æ€§
        format_accuracy = self._check_data_format_accuracy(data, data_type)

        # æ£€æŸ¥æ•°å€¼èŒƒå›´åˆç†æ€§
        range_accuracy = self._check_data_range_accuracy(data, data_type)

        return (base_accuracy * 0.5 + format_accuracy * 0.3 + range_accuracy * 0.2)

    def _check_data_format_accuracy(self, data: Dict[str, Any], data_type: str) -> float:
        """æ£€æŸ¥æ•°æ®æ ¼å¼å‡†ç¡®æ€§"""
        format_rules = {
            'fish_species': {
                'name': str,
                'family': str,
                'size_min': (int, float),
                'size_max': (int, float)
            },
            'equipment': {
                'name': str,
                'price': (int, float),
                'rating': (int, float)
            }
        }

        rules = format_rules.get(data_type, {})
        correct_count = 0
        total_count = len(rules)

        for field, expected_type in rules.items():
            if field in data and data[field] is not None:
                if isinstance(expected_type, tuple):
                    if isinstance(data[field], expected_type):
                        correct_count += 1
                else:
                    if isinstance(data[field], expected_type):
                        correct_count += 1

        return (correct_count / total_count) * 100 if total_count > 0 else 100.0

    def _check_data_range_accuracy(self, data: Dict[str, Any], data_type: str) -> float:
        """æ£€æŸ¥æ•°å€¼èŒƒå›´åˆç†æ€§"""
        range_rules = {
            'fish_species': {
                'size_min': (0, 1000),  # é±¼ç±»ä½“é•¿èŒƒå›´(cm)
                'size_max': (0, 1000),
                'weight_min': (0, 1000),  # é‡é‡èŒƒå›´(kg)
                'weight_max': (0, 1000)
            },
            'equipment': {
                'price': (0, 100000),  # ä»·æ ¼èŒƒå›´(å…ƒ)
                'rating': (0, 5)  # è¯„åˆ†èŒƒå›´
            },
            'weather': {
                'temperature': (-50, 60),  # æ¸©åº¦èŒƒå›´(Â°C)
                'humidity': (0, 100),  # æ¹¿åº¦èŒƒå›´(%)
                'pressure': (800, 1200)  # æ°”å‹èŒƒå›´(hPa)
            }
        }

        rules = range_rules.get(data_type, {})
        correct_count = 0
        total_count = 0

        for field, (min_val, max_val) in rules.items():
            if field in data and isinstance(data[field], (int, float)):
                total_count += 1
                if min_val <= data[field] <= max_val:
                    correct_count += 1

        return (correct_count / total_count) * 100 if total_count > 0 else 100.0

    async def _assess_consistency(self, data: Dict[str, Any], data_type: str) -> float:
        """è¯„ä¼°æ•°æ®ä¸€è‡´æ€§"""
        # è·å–å†å²æ•°æ®è¿›è¡Œä¸€è‡´æ€§æ£€æŸ¥
        storage_service = DataStorageService()
        historical_data = await storage_service.query_data(data_type, limit=10)

        if not historical_data:
            return 100.0  # æ²¡æœ‰å†å²æ•°æ®ï¼Œé»˜è®¤ç»™æ»¡åˆ†

        consistency_score = 0.0
        comparisons = 0

        for hist_item in historical_data:
            hist_data = hist_item['content']
            similarity = self._calculate_data_similarity(data, hist_data)
            consistency_score += similarity
            comparisons += 1

        return consistency_score / comparisons if comparisons > 0 else 100.0

    def _calculate_data_similarity(self, data1: Dict[str, Any], data2: Dict[str, Any]) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ•°æ®å¯¹è±¡çš„ç›¸ä¼¼åº¦"""
        common_fields = set(data1.keys()) & set(data2.keys())

        if not common_fields:
            return 50.0  # æ²¡æœ‰å…±åŒå­—æ®µï¼Œç»™ä¸­ç­‰åˆ†æ•°

        similarity_sum = 0.0

        for field in common_fields:
            val1 = data1[field]
            val2 = data2[field]

            if val1 == val2:
                similarity_sum += 100.0
            elif isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                # æ•°å€¼å‹å­—æ®µçš„ç›¸ä¼¼åº¦è®¡ç®—
                max_val = max(abs(val1), abs(val2), 1.0)
                similarity = 100.0 - (abs(val1 - val2) / max_val) * 100.0
                similarity_sum += max(0.0, similarity)
            elif isinstance(val1, str) and isinstance(val2, str):
                # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦è®¡ç®—
                similarity = self._string_similarity(val1, val2)
                similarity_sum += similarity * 100.0
            else:
                similarity_sum += 0.0

        return similarity_sum / len(common_fields)

    def _string_similarity(self, str1: str, str2: str) -> float:
        """è®¡ç®—å­—ç¬¦ä¸²ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„ç¼–è¾‘è·ç¦»ç®—æ³•ï¼‰"""
        len1, len2 = len(str1), len(str2)
        if len1 == 0:
            return 1.0 if len2 == 0 else 0.0
        if len2 == 0:
            return 0.0

        # åˆ›å»ºç¼–è¾‘è·ç¦»çŸ©é˜µ
        matrix = [[0] * (len2 + 1) for _ in range(len1 + 1)]

        for i in range(len1 + 1):
            matrix[i][0] = i
        for j in range(len2 + 1):
            matrix[0][j] = j

        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                cost = 0 if str1[i-1] == str2[j-1] else 1
                matrix[i][j] = min(
                    matrix[i-1][j] + 1,      # deletion
                    matrix[i][j-1] + 1,      # insertion
                    matrix[i-1][j-1] + cost  # substitution
                )

        max_len = max(len1, len2)
        return 1.0 - (matrix[len1][len2] / max_len)

    def _assess_timeliness(self, data: Dict[str, Any], source: DataSource) -> float:
        """è¯„ä¼°æ•°æ®åŠæ—¶æ€§"""
        now = datetime.now()
        last_collection = source.last_collection

        if not last_collection:
            return 0.0

        # æ ¹æ®æ•°æ®æºç±»å‹è®¾å®šä¸åŒçš„æ—¶æ•ˆæ€§è¦æ±‚
        timeliness_requirements = {
            'realtime': 1,      # 1å°æ—¶å†…
            'hourly': 6,        # 6å°æ—¶å†…
            'daily': 48,        # 48å°æ—¶å†…
            'weekly': 168       # 1å‘¨å†…
        }

        requirement_hours = timeliness_requirements.get(
            source.collection_frequency,
            48  # é»˜è®¤48å°æ—¶
        )

        hours_elapsed = (now - last_collection).total_seconds() / 3600

        if hours_elapsed <= requirement_hours:
            return 100.0
        else:
            # è¶…æ—¶æ‰£åˆ†
            penalty = (hours_elapsed - requirement_hours) / requirement_hours * 100
            return max(0.0, 100.0 - penalty)

    def _assess_validity(self, data: Dict[str, Any], data_type: str) -> float:
        """è¯„ä¼°æ•°æ®æœ‰æ•ˆæ€§"""
        if not isinstance(data, dict):
            return 0.0

        # æ£€æŸ¥åŸºæœ¬ç»“æ„æœ‰æ•ˆæ€§
        if not data:
            return 0.0

        # æ£€æŸ¥å…³é”®å­—æ®µæœ‰æ•ˆæ€§
        validation_rules = {
            'fish_species': {
                'name': lambda x: isinstance(x, str) and len(x.strip()) > 0,
                'family': lambda x: isinstance(x, str) and len(x.strip()) > 0
            },
            'equipment': {
                'name': lambda x: isinstance(x, str) and len(x.strip()) > 0,
                'category': lambda x: isinstance(x, str) and len(x.strip()) > 0
            }
        }

        rules = validation_rules.get(data_type, {})
        valid_count = 0
        total_count = len(rules)

        for field, validator in rules.items():
            if field in data:
                try:
                    if validator(data[field]):
                        valid_count += 1
                except:
                    pass

        return (valid_count / total_count) * 100 if total_count > 0 else 80.0

    async def _assess_uniqueness(self, data: Dict[str, Any], data_type: str) -> float:
        """è¯„ä¼°æ•°æ®å”¯ä¸€æ€§"""
        # ç”Ÿæˆæ•°æ®æŒ‡çº¹
        data_hash = self._generate_data_fingerprint(data)

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ•°æ®
        storage_service = DataStorageService()

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æŸ¥è¯¢æ•°æ®åº“æ£€æŸ¥é‡å¤
        # å‡è®¾90%çš„æ•°æ®æ˜¯å”¯ä¸€çš„
        return 90.0

    def _generate_data_fingerprint(self, data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æŒ‡çº¹"""
        # æå–å…³é”®å­—æ®µ
        key_fields = {
            'fish_species': ['name', 'family', 'scientific_name'],
            'equipment': ['name', 'brand', 'category', 'model'],
            'weather': ['date', 'location', 'temperature']
        }

        data_type = self._detect_data_type(data)
        fields = key_fields.get(data_type, ['name'])

        # æ„å»ºæŒ‡çº¹å­—ç¬¦ä¸²
        fingerprint_parts = []
        for field in fields:
            if field in data and data[field] is not None:
                fingerprint_parts.append(f"{field}:{data[field]}")

        fingerprint_str = "|".join(fingerprint_parts)
        return hashlib.md5(fingerprint_str.encode()).hexdigest()

    def _detect_data_type(self, data: Dict[str, Any]) -> str:
        """æ£€æµ‹æ•°æ®ç±»å‹"""
        if 'name' in data and ('family' in data or 'species' in data):
            return 'fish_species'
        elif 'category' in data or 'brand' in data:
            return 'equipment'
        elif 'temperature' in data or 'weather' in data:
            return 'weather'
        else:
            return 'general'

    async def _check_quality_alerts(self, data_type: str, metrics: DataQualityMetrics):
        """æ£€æŸ¥è´¨é‡å‘Šè­¦"""
        alerts = []

        # æ£€æŸ¥å„é¡¹æŒ‡æ ‡æ˜¯å¦ä½äºé˜ˆå€¼
        if metrics.completeness_score < self.quality_thresholds['completeness']:
            alerts.append(f"å®Œæ•´æ€§è¯„åˆ†è¿‡ä½: {metrics.completeness_score:.1f}")

        if metrics.accuracy_score < self.quality_thresholds['accuracy']:
            alerts.append(f"å‡†ç¡®æ€§è¯„åˆ†è¿‡ä½: {metrics.accuracy_score:.1f}")

        if metrics.consistency_score < self.quality_thresholds['consistency']:
            alerts.append(f"ä¸€è‡´æ€§è¯„åˆ†è¿‡ä½: {metrics.consistency_score:.1f}")

        if metrics.timeliness_score < self.quality_thresholds['timeliness']:
            alerts.append(f"åŠæ—¶æ€§è¯„åˆ†è¿‡ä½: {metrics.timeliness_score:.1f}")

        if metrics.validity_score < self.quality_thresholds['validity']:
            alerts.append(f"æœ‰æ•ˆæ€§è¯„åˆ†è¿‡ä½: {metrics.validity_score:.1f}")

        if metrics.uniqueness_score < self.quality_thresholds['uniqueness']:
            alerts.append(f"å”¯ä¸€æ€§è¯„åˆ†è¿‡ä½: {metrics.uniqueness_score:.1f}")

        if metrics.overall_score < self.quality_thresholds['overall']:
            alerts.append(f"ç»¼åˆè¯„åˆ†è¿‡ä½: {metrics.overall_score:.1f}")

        # å‘é€å‘Šè­¦
        if alerts:
            await self._send_quality_alerts(data_type, metrics, alerts)

    async def _send_quality_alerts(self, data_type: str, metrics: DataQualityMetrics, alerts: List[str]):
        """å‘é€è´¨é‡å‘Šè­¦"""
        alert_message = f"æ•°æ®è´¨é‡å‘Šè­¦ - {data_type}\n\n"
        alert_message += f"ç»¼åˆè¯„åˆ†: {metrics.overall_score:.1f}\n\n"
        alert_message += "é—®é¢˜è¯¦æƒ…:\n"
        for alert in alerts:
            alert_message += f"- {alert}\n"

        logging.warning(f"Data Quality Alert:\n{alert_message}")

        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºå‘é€é‚®ä»¶ã€çŸ­ä¿¡ã€Slackç­‰å‘Šè­¦
        # await self.notification_service.send_alert(alert_message)

    def get_quality_report(self, data_type: str = None) -> Dict[str, Any]:
        """è·å–æ•°æ®è´¨é‡æŠ¥å‘Š"""
        report = {
            'report_time': datetime.now().isoformat(),
            'summary': {},
            'details': {}
        }

        if data_type:
            # ç‰¹å®šæ•°æ®ç±»å‹çš„æŠ¥å‘Š
            if data_type in self.quality_history:
                history = self.quality_history[data_type]
                if history:
                    latest_metrics = history[-1]
                    report['summary'][data_type] = {
                        'latest_score': latest_metrics.overall_score,
                        'trend': self._calculate_trend(history),
                        'sample_count': len(history)
                    }
                    report['details'][data_type] = self._metrics_to_dict(latest_metrics)
        else:
            # æ‰€æœ‰æ•°æ®ç±»å‹çš„æŠ¥å‘Š
            for dt, history in self.quality_history.items():
                if history:
                    latest_metrics = history[-1]
                    report['summary'][dt] = {
                        'latest_score': latest_metrics.overall_score,
                        'trend': self._calculate_trend(history),
                        'sample_count': len(history)
                    }
                    report['details'][dt] = self._metrics_to_dict(latest_metrics)

        return report

    def _calculate_trend(self, history: List[DataQualityMetrics]) -> str:
        """è®¡ç®—è´¨é‡è¶‹åŠ¿"""
        if len(history) < 2:
            return "insufficient_data"

        recent_scores = [m.overall_score for m in history[-5:]]
        earlier_scores = [m.overall_score for m in history[-10:-5]] if len(history) >= 10 else history[:-5]

        if not earlier_scores:
            return "insufficient_data"

        recent_avg = sum(recent_scores) / len(recent_scores)
        earlier_avg = sum(earlier_scores) / len(earlier_scores)

        diff = recent_avg - earlier_avg

        if diff > 5:
            return "improving"
        elif diff < -5:
            return "declining"
        else:
            return "stable"

    def _metrics_to_dict(self, metrics: DataQualityMetrics) -> Dict[str, float]:
        """å°†æŒ‡æ ‡å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'completeness_score': metrics.completeness_score,
            'accuracy_score': metrics.accuracy_score,
            'consistency_score': metrics.consistency_score,
            'timeliness_score': metrics.timeliness_score,
            'validity_score': metrics.validity_score,
            'uniqueness_score': metrics.uniqueness_score,
            'overall_score': metrics.overall_score
        }
```

## ğŸ“ˆ å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€è®¾æ–½å»ºè®¾ (Week 1-2)

#### Week 1: æ¶æ„è®¾è®¡ä¸æ•°æ®åº“æ­å»º
- **ä»»åŠ¡1.1**: è®¾è®¡æ•°æ®é‡‡é›†æ¶æ„ (2å¤©)
  - å®Œæˆåˆ†å±‚æ¶æ„è®¾è®¡
  - å®šä¹‰æ•°æ®æµå’Œå¤„ç†æµç¨‹
  - è®¾è®¡æ•°æ®åº“æ¨¡å¼

- **ä»»åŠ¡1.2**: æ­å»ºåŸºç¡€æ•°æ®åº“ (2å¤©)
  - åˆ›å»ºSQLiteæ•°æ®åº“ç»“æ„
  - å®ç°æ•°æ®å­˜å‚¨æœåŠ¡
  - å»ºç«‹ç´¢å¼•å’ŒæŸ¥è¯¢ä¼˜åŒ–

- **ä»»åŠ¡1.3**: å®ç°æœåŠ¡ç®¡ç†å™¨ (1å¤©)
  - æ‰©å±•ç°æœ‰ServiceManager
  - é›†æˆæ•°æ®é‡‡é›†æœåŠ¡
  - å®ç°ä¾èµ–æ³¨å…¥

#### Week 2: æ ¸å¿ƒç»„ä»¶å¼€å‘
- **ä»»åŠ¡2.1**: æ•°æ®é‡‡é›†åè°ƒå™¨ (2å¤©)
  - å®ç°DataCollectionCoordinator
  - æ”¯æŒå¤šæ•°æ®æºç®¡ç†
  - å®ç°ä»»åŠ¡è°ƒåº¦æœºåˆ¶

- **ä»»åŠ¡2.2**: æ•°æ®å¤„ç†ç®¡é“ (2å¤©)
  - å®ç°DataProcessingCoordinator
  - å¼€å‘æ¸…æ´—ã€éªŒè¯ã€å»é‡å¤„ç†å™¨
  - å»ºç«‹å¤„ç†è´¨é‡ç›‘æ§

- **ä»»åŠ¡2.3**: åŸºç¡€æµ‹è¯•å’Œè°ƒè¯• (1å¤©)
  - å•å…ƒæµ‹è¯•ç¼–å†™
  - ç»„ä»¶é›†æˆæµ‹è¯•
  - æ€§èƒ½åˆæ­¥è¯„ä¼°

### ç¬¬äºŒé˜¶æ®µï¼šé±¼ç±»çŸ¥è¯†æ•°æ®é‡‡é›† (Week 3-4)

#### Week 3: æ•°æ®æºæ¥å…¥
- **ä»»åŠ¡3.1**: é±¼ç±»çŸ¥è¯†é‡‡é›†å™¨å¼€å‘ (2å¤©)
  - å®ç°FishKnowledgeCollector
  - å¼€å‘å¤šæºæ•°æ®æå–å™¨
  - å»ºç«‹æ•°æ®æ˜ å°„è§„åˆ™

- **ä»»åŠ¡3.2**: å­¦æœ¯æ•°æ®æºæ¥å…¥ (1å¤©)
  - FishBase APIé›†æˆ
  - å­¦æœ¯è®ºæ–‡æ•°æ®æå–
  - ä¸“å®¶çŸ¥è¯†ç»“æ„åŒ–

- **ä»»åŠ¡3.3**: ç¤¾åŒºæ•°æ®é‡‡é›† (2å¤©)
  - é’“é±¼è®ºå›çˆ¬è™«å¼€å‘
  - ç¤¾äº¤åª’ä½“æ•°æ®æå–
  - ç”¨æˆ·åé¦ˆæ”¶é›†æœºåˆ¶

#### Week 4: æ•°æ®è´¨é‡ä¼˜åŒ–
- **ä»»åŠ¡4.1**: é±¼ç±»æ•°æ®éªŒè¯è§„åˆ™ (2å¤©)
  - åˆ¶å®šæ•°æ®è´¨é‡æ ‡å‡†
  - å®ç°é¢†åŸŸç‰¹å®šéªŒè¯å™¨
  - å»ºç«‹æ•°æ®çº é”™æœºåˆ¶

- **ä»»åŠ¡4.2**: æ•°æ®è´¨é‡ç›‘æ§ (2å¤©)
  - å®ç°DataQualityMonitor
  - å»ºç«‹è´¨é‡æŒ‡æ ‡ä½“ç³»
  - é…ç½®è´¨é‡å‘Šè­¦æœºåˆ¶

- **ä»»åŠ¡4.3**: LangChainå·¥å…·é›†æˆ (1å¤©)
  - å¼€å‘get_comprehensive_fish_dataå·¥å…·
  - é›†æˆåˆ°æ™ºèƒ½ä½“å·¥å…·é›†
  - æµ‹è¯•å·¥å…·å¯ç”¨æ€§

### ç¬¬ä¸‰é˜¶æ®µï¼šè£…å¤‡æ•°æ®é‡‡é›† (Week 5-6)

#### Week 5: ç”µå•†å¹³å°é›†æˆ
- **ä»»åŠ¡5.1**: è£…å¤‡æ•°æ®é‡‡é›†å™¨ (2å¤©)
  - å®ç°EquipmentDataCollector
  - å¼€å‘å¤šå¹³å°é‡‡é›†å™¨
  - å»ºç«‹äº§å“è¯†åˆ«æœºåˆ¶

- **ä»»åŠ¡5.2**: ç”µå•†å¹³å°APIæ¥å…¥ (2å¤©)
  - æ·˜å®/äº¬ä¸œAPIé›†æˆ
  - äº§å“ä¿¡æ¯æŠ“å–
  - ä»·æ ¼ç›‘æ§æœºåˆ¶

- **ä»»åŠ¡5.3**: å“ç‰Œå®˜æ–¹æ•°æ® (1å¤©)
  - å“ç‰Œå®˜ç½‘æ•°æ®é‡‡é›†
  - äº§å“è§„æ ¼æ ‡å‡†åŒ–
  - å®˜æ–¹ä»·æ ¼è·å–

#### Week 6: è¯„æµ‹å’Œä»·æ ¼æ•°æ®
- **ä»»åŠ¡6.1**: è¯„æµ‹æ•°æ®é‡‡é›† (2å¤©)
  - ä¸“ä¸šè¯„æµ‹ç½‘ç«™é›†æˆ
  - ç”¨æˆ·è¯„ä»·æ•°æ®æå–
  - è¯„æµ‹æ–‡æœ¬åˆ†æ

- **ä»»åŠ¡6.2**: ä»·æ ¼ç›‘æ§ç³»ç»Ÿ (2å¤©)
  - å†å²ä»·æ ¼æ•°æ®æ”¶é›†
  - ä»·æ ¼è¶‹åŠ¿åˆ†æ
  - ä¿ƒé”€ä¿¡æ¯ç›‘æ§

- **ä»»åŠ¡6.3**: è£…å¤‡å·¥å…·é›†æˆ (1å¤©)
  - å¼€å‘get_equipment_recommendations_dataå·¥å…·
  - æµ‹è¯•è£…å¤‡æ¨èåŠŸèƒ½
  - ä¼˜åŒ–æ•°æ®æŸ¥è¯¢æ€§èƒ½

### ç¬¬å››é˜¶æ®µï¼šç¯å¢ƒæ•°æ®é›†æˆ (Week 7-8)

#### Week 7: å¤šæºå¤©æ°”æ•°æ®
- **ä»»åŠ¡7.1**: ç¯å¢ƒæ•°æ®é‡‡é›†å™¨ (2å¤©)
  - å®ç°EnvironmentalDataCollector
  - å¤šå¤©æ°”APIé›†æˆ
  - æ•°æ®èåˆç®—æ³•å¼€å‘

- **ä»»åŠ¡7.2**: åœ°ç†æ•°æ®å¢å¼º (2å¤©)
  - é«˜å¾·/ç™¾åº¦åœ°å›¾APIé›†æˆ
  - é’“ç‚¹åæ ‡é‡‡é›†
  - åœ°ç†ç¼–ç æœåŠ¡

- **ä»»åŠ¡7.3**: æ°´æ–‡æ•°æ®æ¥å…¥ (1å¤©)
  - æ°´åˆ©éƒ¨é—¨æ•°æ®æ¥å£
  - æ°´ä½ã€æ°´æ¸©ç›‘æ§
  - æ°´è´¨æ•°æ®é‡‡é›†

#### Week 8: æ•°æ®åŒæ­¥å’ŒæœåŠ¡
- **ä»»åŠ¡8.1**: å®æ—¶æ•°æ®åŒæ­¥ (2å¤©)
  - å¢é‡æ•°æ®æ›´æ–°æœºåˆ¶
  - å˜æ›´æ£€æµ‹ç®—æ³•
  - å†²çªè§£å†³ç­–ç•¥

- **ä»»åŠ¡8.2**: æœåŠ¡æ¥å£å¼€å‘ (2å¤©)
  - æ•°æ®æŸ¥è¯¢APIå®Œå–„
  - è®¢é˜…é€šçŸ¥æœåŠ¡
  - APIæ–‡æ¡£ç¼–å†™

- **ä»»åŠ¡8.3**: è§¦å‘å¼é‡‡é›†å·¥å…· (1å¤©)
  - trigger_data_collectionå·¥å…·å¼€å‘
  - æ‰‹åŠ¨é‡‡é›†è§¦å‘æœºåˆ¶
  - é‡‡é›†çŠ¶æ€ç›‘æ§

### ç¬¬äº”é˜¶æ®µï¼šç³»ç»Ÿé›†æˆå’Œä¼˜åŒ– (Week 9-10)

#### Week 9: ç³»ç»Ÿé›†æˆæµ‹è¯•
- **ä»»åŠ¡9.1**: ç«¯åˆ°ç«¯æµ‹è¯• (2å¤©)
  - å®Œæ•´æ•°æ®æµæµ‹è¯•
  - å¤šæ¨¡å—åä½œéªŒè¯
  - é”™è¯¯å¤„ç†æµ‹è¯•

- **ä»»åŠ¡9.2**: æ€§èƒ½ä¼˜åŒ– (2å¤©)
  - æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–
  - å¹¶å‘å¤„ç†ä¼˜åŒ–
  - å†…å­˜ä½¿ç”¨ä¼˜åŒ–

- **ä»»åŠ¡9.3**: ç›‘æ§å’Œæ—¥å¿— (1å¤©)
  - ç³»ç»Ÿç›‘æ§ä»ªè¡¨æ¿
  - é‡‡é›†è¿‡ç¨‹æ—¥å¿—è®°å½•
  - å¼‚å¸¸å‘Šè­¦é…ç½®

#### Week 10: æ–‡æ¡£å’Œéƒ¨ç½²
- **ä»»åŠ¡10.1**: æŠ€æœ¯æ–‡æ¡£ç¼–å†™ (2å¤©)
  - APIæ–‡æ¡£å®Œå–„
  - è¿ç»´æ‰‹å†Œç¼–å†™
  - æ•…éšœæ’é™¤æŒ‡å—

- **ä»»åŠ¡10.2**: éƒ¨ç½²å’Œé…ç½® (2å¤©)
  - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
  - é…ç½®å‚æ•°ä¼˜åŒ–
  - å¤‡ä»½ç­–ç•¥åˆ¶å®š

- **ä»»åŠ¡10.3**: åŸ¹è®­å’Œäº¤ä»˜ (1å¤©)
  - ä½¿ç”¨åŸ¹è®­ææ–™
  - è¿ç»´åŸ¹è®­
  - é¡¹ç›®äº¤ä»˜

## ğŸ“Š æˆåŠŸæŒ‡æ ‡

### æ•°æ®é‡‡é›†æŒ‡æ ‡
- **æ•°æ®æºè¦†ç›–ç‡**: > 80% (ç›®æ ‡æ•°æ®æºæ¥å…¥æ¯”ä¾‹)
- **æ•°æ®é‡‡é›†æˆåŠŸç‡**: > 95% (é‡‡é›†ä»»åŠ¡æˆåŠŸå®Œæˆæ¯”ä¾‹)
- **æ•°æ®å®Œæ•´æ€§**: > 90% (å…³é”®å­—æ®µå®Œæ•´åº¦)
- **æ•°æ®åŠæ—¶æ€§**: > 85% (æŒ‰æ—¶æ›´æ–°æ¯”ä¾‹)

### æ•°æ®è´¨é‡æŒ‡æ ‡
- **å‡†ç¡®æ€§è¯„åˆ†**: > 85% (DataQualityMonitorè¯„ä¼°)
- **ä¸€è‡´æ€§è¯„åˆ†**: > 90% (å¤šæºæ•°æ®ä¸€è‡´æ€§)
- **å»é‡æ•ˆç‡**: > 95% (é‡å¤æ•°æ®è¯†åˆ«ç‡)
- **æ ‡å‡†åŒ–ç¨‹åº¦**: > 90% (æ•°æ®æ ¼å¼æ ‡å‡†åŒ–)

### ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
- **æŸ¥è¯¢å“åº”æ—¶é—´**: < 500ms (ä¸€èˆ¬æ•°æ®æŸ¥è¯¢)
- **æ‰¹é‡å¤„ç†æ—¶é—´**: < 30min (ä¸‡çº§æ•°æ®å¤„ç†)
- **ç³»ç»Ÿå¯ç”¨æ€§**: > 99% (æœåŠ¡å¯ç”¨æ—¶é—´)
- **å¹¶å‘å¤„ç†èƒ½åŠ›**: 10+ å¹¶å‘é‡‡é›†ä»»åŠ¡

### ä¸šåŠ¡ä»·å€¼æŒ‡æ ‡
- **æ•°æ®é©±åŠ¨å†³ç­–è¦†ç›–ç‡**: 100% (æ‰€æœ‰ä¸šåŠ¡æ¨¡å—éƒ½æœ‰æ•°æ®æ”¯æ’‘)
- **ç”¨æˆ·æŸ¥è¯¢æ»¡è¶³ç‡**: > 95% (ç”¨æˆ·æŸ¥è¯¢èƒ½å¾—åˆ°æ•°æ®å“åº”)
- **æ™ºèƒ½æ¨èå‡†ç¡®ç‡**: > 88% (åŸºäºæ•°æ®çš„æ¨èå‡†ç¡®æ€§)
- **è¿è¥æ•ˆç‡æå‡**: 50%+ (æ•°æ®è‡ªåŠ¨åŒ–å¸¦æ¥çš„æ•ˆç‡æå‡)

## ğŸ”® æœªæ¥æ‰©å±•è§„åˆ’

### çŸ­æœŸæ‰©å±• (3-6ä¸ªæœˆ)
- **å¤šè¯­è¨€æ•°æ®é‡‡é›†**: æ”¯æŒè‹±æ–‡ã€æ—¥æ–‡ç­‰å¤šæºæ•°æ®
- **å›¾åƒæ•°æ®é‡‡é›†**: é’“é±¼åœºæ™¯ã€è£…å¤‡å›¾ç‰‡é‡‡é›†å’Œåˆ†æ
- **å®æ—¶ä¼ æ„Ÿå™¨æ¥å…¥**: IoTè®¾å¤‡å®æ—¶ç¯å¢ƒæ•°æ®
- **ç”¨æˆ·è¡Œä¸ºæ•°æ®**: ç”¨æˆ·æŸ¥è¯¢ã€ç‚¹å‡»ã€åé¦ˆæ•°æ®é‡‡é›†

### ä¸­æœŸæ‰©å±• (6-12ä¸ªæœˆ)
- **æœºå™¨å­¦ä¹ æ•°æ®è´¨é‡**: AIè¾…åŠ©æ•°æ®éªŒè¯å’Œæ¸…æ´—
- **çŸ¥è¯†å›¾è°±æ„å»º**: é±¼ç±»-è£…å¤‡-åœ°ç†çŸ¥è¯†å›¾è°±
- **é¢„æµ‹æ•°æ®é‡‡é›†**: åŸºäºå†å²æ•°æ®çš„é¢„æµ‹æ€§é‡‡é›†
- **åŒºå—é“¾æ•°æ®å­˜è¯**: å…³é”®æ•°æ®æ¥æºå’Œå˜æ›´è¿½æº¯

### é•¿æœŸæ‰©å±• (1-2å¹´)
- **å¤šæ¨¡æ€æ•°æ®**: æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ç»¼åˆé‡‡é›†
- **è”é‚¦å­¦ä¹ æ•°æ®**: åˆ†å¸ƒå¼æ•°æ®é‡‡é›†å’Œéšç§ä¿æŠ¤
- **è‡ªåŠ¨åŒ–æ•°æ®æ ‡æ³¨**: AIè¾…åŠ©æ•°æ®åˆ†ç±»å’Œæ ‡æ³¨
- **å®æ—¶æ•°æ®æµå¤„ç†**: æµå¼æ•°æ®å¤„ç†å’Œåˆ†æ

## ğŸ“ æ€»ç»“

æ•°æ®é‡‡é›†æˆ˜ç•¥å®æ–½ä¸ºæ™ºèƒ½é’“é±¼ç”Ÿæ€ç³»ç»Ÿæä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚é€šè¿‡åˆ†å±‚çš„æ¶æ„è®¾è®¡ã€å…¨é¢çš„æ•°æ®æºè¦†ç›–ã€ä¸¥æ ¼çš„è´¨é‡æ§åˆ¶å’Œä¸ç°æœ‰ç³»ç»Ÿçš„æ— ç¼é›†æˆï¼Œè¯¥æ–¹æ¡ˆèƒ½å¤Ÿï¼š

1. **å»ºç«‹å®Œæ•´çš„æ•°æ®é‡‡é›†ä½“ç³»**ï¼Œè¦†ç›–é±¼ç±»çŸ¥è¯†ã€è£…å¤‡ä¿¡æ¯ã€ç¯å¢ƒæ•°æ®ç­‰å„ä¸ªç»´åº¦
2. **ç¡®ä¿æ•°æ®è´¨é‡**ï¼Œé€šè¿‡å¤šç»´åº¦è´¨é‡ç›‘æ§å’Œè‡ªåŠ¨åŒ–å¤„ç†æµç¨‹
3. **å®ç°ä¸ç°æœ‰ç³»ç»Ÿçš„æ·±åº¦é›†æˆ**ï¼Œä¸ºæ™ºèƒ½ä½“æä¾›ä¸°å¯Œçš„æ•°æ®æ”¯æ’‘
4. **æ”¯æŒæŒç»­æ‰©å±•**ï¼Œä¸ºæœªæ¥æ–°æ•°æ®æºå’Œæ–°åŠŸèƒ½çš„æ¥å…¥æä¾›è‰¯å¥½åŸºç¡€

è¯¥å®æ–½æ–¹æ¡ˆå°†æˆä¸ºæ™ºèƒ½é’“é±¼ç”Ÿæ€ç³»ç»Ÿä»"è§„åˆ™é©±åŠ¨"å‘"æ•°æ®é©±åŠ¨"è½¬å‹çš„å…³é”®åŸºç¡€è®¾æ–½ï¼Œä¸ºç³»ç»Ÿçš„æ™ºèƒ½åŒ–å‡çº§å’Œç”¨æˆ·ä½“éªŒæå‡å¥ å®šåšå®åŸºç¡€ã€‚

---

*æœ¬æ–‡æ¡£æä¾›äº†æ•°æ®é‡‡é›†æˆ˜ç•¥çš„å®Œæ•´å®æ–½æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æŠ€æœ¯æ¶æ„ã€å…·ä½“å®ç°æ­¥éª¤ã€è´¨é‡æ§åˆ¶å’Œæ‰©å±•è§„åˆ’ï¼Œä¸ºé¡¹ç›®çš„æˆåŠŸå®æ–½æä¾›è¯¦ç»†æŒ‡å¯¼ã€‚*