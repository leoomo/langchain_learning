"""
LangChain æ™ºèƒ½ä½“æ—¥å¿—ä¸­é—´ä»¶

æä¾›å®Œæ•´çš„æ™ºèƒ½ä½“æ‰§è¡Œæ—¥å¿—è®°å½•åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- ç”¨æˆ·è¾“å…¥å’ŒAIå›å¤è®°å½•
- å·¥å…·è°ƒç”¨ç›‘æ§å’Œæ€§èƒ½ç»Ÿè®¡
- é”™è¯¯è¿½è¸ªå’Œè°ƒè¯•ä¿¡æ¯
- å¯é…ç½®çš„æ—¥å¿—çº§åˆ«å’Œè¾“å‡ºæ–¹å¼
"""

import json
import time
import uuid
import os
import logging
from typing import Any, Dict, Optional, List, Callable, TYPE_CHECKING
from datetime import datetime
from dataclasses import dataclass, asdict

# LangChain imports
if TYPE_CHECKING:
    from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse
    from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
    from langgraph.runtime import Runtime
    from langchain.agents import AgentState
else:
    # å¦‚æœæ²¡æœ‰å®‰è£…å®Œæ•´çš„LangChainï¼Œæä¾›åŸºç¡€ç±»å‹
    AgentMiddleware = object
    ModelRequest = object
    ModelResponse = object
    BaseMessage = object

from .config import MiddlewareConfig, default_config


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡ - æ”¯æŒåŠ¨æ€æ‰©å±•"""
    # æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
    request_duration_ms: float = 0.0      # è¯·æ±‚å¤„ç†æ—¶é—´
    inference_duration_ms: float = 0.0     # æ¨¡å‹æ¨ç†æ—¶é—´
    response_duration_ms: float = 0.0     # å“åº”ç”Ÿæˆæ—¶é—´
    network_duration_ms: float = 0.0      # ç½‘ç»œä¼ è¾“æ—¶é—´

    # æ‰©å±•æ€§èƒ½æŒ‡æ ‡
    custom_metrics: Optional[Dict[str, Any]] = None  # è‡ªå®šä¹‰æŒ‡æ ‡

    def __post_init__(self):
        if self.custom_metrics is None:
            self.custom_metrics = {}

    def add_metric(self, name: str, value: Any, metric_type: str = "custom", unit: str = ""):
        """åŠ¨æ€æ·»åŠ æ€§èƒ½æŒ‡æ ‡"""
        self.custom_metrics[name] = {
            "value": value,
            "type": metric_type,
            "unit": unit,
            "timestamp": time.time()
        }

    def get_total_duration(self) -> float:
        """è·å–æ€»è€—æ—¶"""
        return max(self.request_duration_ms,
                  self.inference_duration_ms + self.response_duration_ms)

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "request_duration_ms": self.request_duration_ms,
            "inference_duration_ms": self.inference_duration_ms,
            "response_duration_ms": self.response_duration_ms,
            "network_duration_ms": self.network_duration_ms,
            "total_duration_ms": self.get_total_duration(),
            "custom_metrics": self.custom_metrics
        }


@dataclass
class ModelCallRecord:
    """æ¨¡å‹è°ƒç”¨è®°å½• - å¢å¼ºç‰ˆï¼ˆæ”¯æŒæ€§èƒ½æ‰©å±•ï¼‰"""
    call_id: int
    timestamp: str
    model_name: str
    duration_ms: float
    token_usage: Dict[str, int]
    success: bool
    call_purpose: str = "unknown"  # è°ƒç”¨ç›®çš„
    intent_category: str = ""  # æ„å›¾åˆ†ç±»
    call_context_summary: str = ""  # è°ƒç”¨ä¸Šä¸‹æ–‡æ‘˜è¦
    key_points: Optional[List[str]] = None  # å…³é”®ä¿¡æ¯ç‚¹
    inference_method: str = "position_and_content_analysis"  # æ¨æ–­æ–¹æ³•
    error_message: Optional[str] = None

    # æ€§èƒ½æŒ‡æ ‡æ‰©å±•å­—æ®µ
    performance_metrics: Optional[PerformanceMetrics] = None
    resource_usage: Optional[Dict[str, Any]] = None  # èµ„æºä½¿ç”¨æƒ…å†µ

    def __post_init__(self):
        if self.key_points is None:
            self.key_points = []
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
        if not self.token_usage:
            self.token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        if self.performance_metrics is None:
            self.performance_metrics = PerformanceMetrics()
        if self.resource_usage is None:
            self.resource_usage = {}

    def get_detailed_performance(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„æ€§èƒ½ä¿¡æ¯"""
        return {
            "call_id": self.call_id,
            "model_name": self.model_name,
            "success": self.success,
            "basic_duration_ms": self.duration_ms,
            "performance_metrics": self.performance_metrics.to_dict(),
            "token_efficiency": {
                "tokens_per_second": self._calculate_tokens_per_second(),
                "ms_per_token": self._calculate_ms_per_token()
            },
            "resource_usage": self.resource_usage
        }

    def _calculate_tokens_per_second(self) -> float:
        """è®¡ç®—æ¯ç§’ç”Ÿæˆçš„tokenæ•°"""
        total_tokens = self.token_usage.get("total_tokens", 0)
        if total_tokens > 0 and self.duration_ms > 0:
            return (total_tokens / self.duration_ms) * 1000
        return 0.0

    def _calculate_ms_per_token(self) -> float:
        """è®¡ç®—æ¯ä¸ªtokençš„è€—æ—¶"""
        total_tokens = self.token_usage.get("total_tokens", 0)
        if total_tokens > 0:
            return self.duration_ms / total_tokens
        return 0.0


@dataclass
class ToolCallRecord:
    """å·¥å…·è°ƒç”¨è®°å½• - å¢å¼ºç‰ˆï¼ˆæ”¯æŒæ€§èƒ½æ‰©å±•ï¼‰"""
    tool_name: str
    tool_args: Dict[str, Any]
    result: Any
    duration_ms: float
    success: bool
    error_message: Optional[str] = None
    timestamp: str = ""

    # æ€§èƒ½æŒ‡æ ‡æ‰©å±•å­—æ®µ
    performance_metrics: Optional[PerformanceMetrics] = None
    operation_phases: Optional[Dict[str, float]] = None  # å„æ“ä½œé˜¶æ®µè€—æ—¶
    cache_hit: bool = False  # ç¼“å­˜å‘½ä¸­çŠ¶æ€
    retry_count: int = 0  # é‡è¯•æ¬¡æ•°
    resource_usage: Optional[Dict[str, Any]] = None  # èµ„æºä½¿ç”¨æƒ…å†µ

    def __post_init__(self):
        if not self.timestamp:
            self.timestamp = datetime.now().isoformat()
        if self.performance_metrics is None:
            self.performance_metrics = PerformanceMetrics()
        if self.operation_phases is None:
            self.operation_phases = {}
        if self.resource_usage is None:
            self.resource_usage = {}

    def add_phase_duration(self, phase_name: str, duration_ms: float):
        """æ·»åŠ æ“ä½œé˜¶æ®µè€—æ—¶"""
        self.operation_phases[phase_name] = duration_ms

    def get_detailed_performance(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†çš„æ€§èƒ½ä¿¡æ¯"""
        return {
            "tool_name": self.tool_name,
            "success": self.success,
            "basic_duration_ms": self.duration_ms,
            "performance_metrics": self.performance_metrics.to_dict(),
            "operation_phases": self.operation_phases,
            "cache_hit": self.cache_hit,
            "retry_count": self.retry_count,
            "resource_usage": self.resource_usage
        }


@dataclass
class AgentExecutionMetrics:
    """æ™ºèƒ½ä½“æ‰§è¡ŒæŒ‡æ ‡"""
    session_id: str
    timestamp: str
    execution_id: str
    total_duration_ms: float = 0.0
    model_calls_count: int = 0
    tool_calls_count: int = 0
    token_usage: Optional[Dict[str, int]] = None
    errors_count: int = 0
    success: bool = True
    model_name: str = ""
    model_calls: Optional[List[ModelCallRecord]] = None  # å¢å¼ºçš„æ¨¡å‹è°ƒç”¨è®°å½•
    tool_calls: Optional[List[ToolCallRecord]] = None    # å·¥å…·è°ƒç”¨è®°å½•

    def __post_init__(self):
        if self.token_usage is None:
            self.token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        if self.model_calls is None:
            self.model_calls = []
        if self.tool_calls is None:
            self.tool_calls = []

    def add_model_call(self, call_record: ModelCallRecord):
        """æ·»åŠ æ¨¡å‹è°ƒç”¨è®°å½•"""
        self.model_calls.append(call_record)
        self.model_calls_count += 1

    def get_model_calls_summary(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹è°ƒç”¨æ‘˜è¦"""
        if not self.model_calls:
            return {}

        purposes = {}
        intents = {}
        total_duration = 0

        for call in self.model_calls:
            # ç»Ÿè®¡è°ƒç”¨ç›®çš„
            purpose = call.call_purpose
            purposes[purpose] = purposes.get(purpose, 0) + 1

            # ç»Ÿè®¡æ„å›¾åˆ†ç±»
            intent = call.intent_category
            intents[intent] = intents.get(intent, 0) + 1

            total_duration += call.duration_ms

        return {
            "total_calls": len(self.model_calls),
            "purposes_distribution": purposes,
            "intents_distribution": intents,
            "average_duration_ms": total_duration / len(self.model_calls) if self.model_calls else 0,
            "total_model_duration_ms": total_duration
        }


class PerformanceTracker:
    """æ€§èƒ½è¿½è¸ªå™¨ - ç”¨äºè·Ÿè¸ªå„ç§æ“ä½œçš„è€—æ—¶å’ŒæŒ‡æ ‡"""

    def __init__(self):
        self.active_timings: Dict[str, Dict[str, Any]] = {}
        self.completed_operations: List[Dict[str, Any]] = []
        self.counters: Dict[str, int] = {}
        self.gauges: Dict[str, Any] = {}

    def start_timing(self, operation_id: str, operation_type: str, metadata: Dict[str, Any] = None):
        """å¼€å§‹è®¡æ—¶"""
        self.active_timings[operation_id] = {
            "start_time": time.time(),
            "type": operation_type,
            "metadata": metadata or {}
        }

    def end_timing(self, operation_id: str) -> Optional[float]:
        """ç»“æŸè®¡æ—¶å¹¶è¿”å›è€—æ—¶"""
        if operation_id in self.active_timings:
            timing_info = self.active_timings.pop(operation_id)
            duration = (time.time() - timing_info["start_time"]) * 1000

            # è®°å½•å®Œæˆçš„æ“ä½œ
            self.completed_operations.append({
                "operation_id": operation_id,
                "type": timing_info["type"],
                "duration_ms": duration,
                "metadata": timing_info["metadata"],
                "timestamp": time.time()
            })

            return duration
        return None

    def increment_counter(self, name: str, value: int = 1):
        """å¢åŠ è®¡æ•°å™¨"""
        self.counters[name] = self.counters.get(name, 0) + value

    def set_gauge(self, name: str, value: Any):
        """è®¾ç½®ä»ªè¡¨å€¼"""
        self.gauges[name] = value

    def get_metrics_summary(self) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡æ‘˜è¦"""
        # æŒ‰ç±»å‹ç»Ÿè®¡æ“ä½œè€—æ—¶
        type_stats = {}
        for op in self.completed_operations[-100:]:  # æœ€è¿‘100ä¸ªæ“ä½œ
            op_type = op["type"]
            if op_type not in type_stats:
                type_stats[op_type] = {
                    "count": 0,
                    "total_duration": 0.0,
                    "avg_duration": 0.0,
                    "min_duration": float('inf'),
                    "max_duration": 0.0
                }

            stats = type_stats[op_type]
            stats["count"] += 1
            stats["total_duration"] += op["duration_ms"]
            stats["min_duration"] = min(stats["min_duration"], op["duration_ms"])
            stats["max_duration"] = max(stats["max_duration"], op["duration_ms"])

        # è®¡ç®—å¹³å‡å€¼
        for stats in type_stats.values():
            if stats["count"] > 0:
                stats["avg_duration"] = stats["total_duration"] / stats["count"]

        return {
            "counters": self.counters,
            "gauges": self.gauges,
            "operation_stats": type_stats,
            "active_operations": len(self.active_timings),
            "completed_operations": len(self.completed_operations)
        }


class MetricRegistry:
    """æŒ‡æ ‡æ³¨å†Œè¡¨ - ç®¡ç†æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡çš„è§„èŒƒ"""

    def __init__(self):
        self.metrics: Dict[str, Dict[str, Any]] = {}

    def register_metric(self, name: str, metric_type: str, description: str = "", unit: str = ""):
        """æ³¨å†ŒæŒ‡æ ‡"""
        self.metrics[name] = {
            "type": metric_type,  # timing, count, gauge, rate
            "description": description,
            "unit": unit,
            "registered_at": time.time()
        }

    def get_metric_info(self, name: str) -> Optional[Dict[str, Any]]:
        """è·å–æŒ‡æ ‡ä¿¡æ¯"""
        return self.metrics.get(name)

    def list_metrics(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„æŒ‡æ ‡"""
        return list(self.metrics.keys())


# å…¨å±€æŒ‡æ ‡æ³¨å†Œè¡¨
metric_registry = MetricRegistry()

# æ³¨å†Œæ ¸å¿ƒæŒ‡æ ‡
metric_registry.register_metric("model_call_duration", "timing", "æ¨¡å‹è°ƒç”¨æ€»è€—æ—¶", "ms")
metric_registry.register_metric("model_inference_duration", "timing", "æ¨¡å‹æ¨ç†è€—æ—¶", "ms")
metric_registry.register_metric("model_token_usage", "count", "Tokenä½¿ç”¨é‡", "tokens")
metric_registry.register_metric("tool_call_duration", "timing", "å·¥å…·è°ƒç”¨è€—æ—¶", "ms")
metric_registry.register_metric("tool_cache_hit_rate", "rate", "å·¥å…·ç¼“å­˜å‘½ä¸­ç‡", "%")


class CallPurposeAnalyzer:
    """è°ƒç”¨ç›®çš„åˆ†æå™¨ - æ™ºèƒ½æ¨æ–­æ¨¡å‹è°ƒç”¨çš„ç›®çš„å’Œæ„å›¾"""

    # è°ƒç”¨ç›®çš„ç±»å‹å®šä¹‰
    CALL_PURPOSES = {
        "tool_selection": "å·¥å…·é€‰æ‹©",
        "result_generation": "ç»“æœç”Ÿæˆ",
        "intent_understanding": "æ„å›¾ç†è§£",
        "context_analysis": "ä¸Šä¸‹æ–‡åˆ†æ",
        "tool_execution": "å·¥å…·æ‰§è¡Œå¤„ç†",
        "final_response": "æœ€ç»ˆå›å¤ç”Ÿæˆ",
        "error_handling": "é”™è¯¯å¤„ç†",
        "unknown": "æœªçŸ¥ç›®çš„"
    }

    # æ„å›¾åˆ†ç±»å®šä¹‰
    INTENT_CATEGORIES = {
        "weather_query": "å¤©æ°”æŸ¥è¯¢",
        "weather_fishing_query": "é’“é±¼å¤©æ°”æŸ¥è¯¢",
        "fishing_advice_generation": "é’“é±¼å»ºè®®ç”Ÿæˆ",
        "fishing_data_processing": "é’“é±¼æ•°æ®å¤„ç†",
        "advice_generation": "å»ºè®®ç”Ÿæˆ",
        "data_processing": "æ•°æ®å¤„ç†",
        "location_query": "åœ°ç‚¹æŸ¥è¯¢",
        "time_query": "æ—¶é—´æŸ¥è¯¢",
        "calculation": "è®¡ç®—è¯·æ±‚",
        "information_search": "ä¿¡æ¯æœç´¢",
        "general_conversation": "ä¸€èˆ¬å¯¹è¯",
        "error_recovery": "é”™è¯¯æ¢å¤",
        "unknown": "æœªçŸ¥æ„å›¾"
    }

    # å…³é”®è¯æ˜ å°„
    INTENT_KEYWORDS = {
        "weather_query": ["å¤©æ°”", "æ°”æ¸©", "ä¸‹é›¨", "æ™´å¤©", "é˜´å¤©", "å¤šäº‘", "æ¸©åº¦"],
        "weather_fishing_query": ["é’“é±¼", "é’“é±¼å¤©æ°”", "é€‚åˆé’“é±¼", "é’“é±¼æ—¶é—´", "é’“é±¼æ¨è"],
        "location_query": ["åœ¨å“ª", "ä½ç½®", "åæ ‡", "åœ°å€", "æ€ä¹ˆå»"],
        "time_query": ["æ—¶é—´", "å‡ ç‚¹", "ä»€ä¹ˆæ—¶å€™", "æ˜å¤©", "åå¤©", "ä»Šå¤©"],
        "calculation": ["è®¡ç®—", "åŠ æ³•", "ä¹˜æ³•", "é™¤æ³•", "ç­‰äº"],
        "information_search": ["æœç´¢", "æŸ¥æ‰¾", "ä¿¡æ¯", "èµ„æ–™", "ä»€ä¹ˆæ˜¯"]
    }

    @classmethod
    def analyze_call_purpose(cls, messages: List[Any], call_position: int,
                            has_tool_calls: bool, response: Any = None,
                            compiled_patterns: Optional[Dict] = None,
                            execution_context: str = "unknown") -> Dict[str, str]:
        """
        åˆ†ææ¨¡å‹è°ƒç”¨çš„ç›®çš„ - å¢å¼ºç‰ˆï¼Œæ”¯æŒé¢„ç¼–è¯‘æ¨¡å¼å’Œæ‰§è¡Œé˜¶æ®µæ„ŸçŸ¥

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            call_position: è°ƒç”¨åœ¨å¯¹è¯ä¸­çš„ä½ç½®ï¼ˆä»1å¼€å§‹ï¼‰
            has_tool_calls: æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            response: æ¨¡å‹å“åº”ï¼ˆå¯é€‰ï¼‰
            compiled_patterns: é¢„ç¼–è¯‘çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
            execution_context: æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ˆtool_selection, result_generation, tool_executionï¼‰

        Returns:
            åŒ…å«è°ƒç”¨ç›®çš„åˆ†æçš„å­—å…¸
        """
        # åŸºäºè°ƒç”¨ä½ç½®çš„åŸºç¡€æ¨æ–­
        purpose = cls._infer_purpose_by_position(call_position, has_tool_calls)

        # å¦‚æœæ²¡æœ‰æ˜ç¡®æä¾›æ‰§è¡Œä¸Šä¸‹æ–‡ï¼ŒåŸºäºpurposeæ¨æ–­
        if execution_context == "unknown":
            execution_context = purpose

        # åŸºäºæ¶ˆæ¯å†…å®¹å’Œæ‰§è¡Œä¸Šä¸‹æ–‡çš„æ„å›¾åˆ†æ
        intent_category = cls._analyze_intent_from_messages(messages, call_position, has_tool_calls, execution_context)

        # æå–å…³é”®ä¿¡æ¯ç‚¹ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ¨¡å¼ï¼‰
        key_points = cls._extract_key_points(messages, response, compiled_patterns)

        # ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦
        context_summary = cls._generate_context_summary(messages, purpose, key_points)

        return {
            "call_purpose": purpose,
            "intent_category": intent_category,
            "key_points": key_points,
            "context_summary": context_summary,
            "execution_context": execution_context,
            "inference_method": "enhanced_position_content_analysis"
        }

    @classmethod
    def _infer_purpose_by_position(cls, position: int, has_tool_calls: bool) -> str:
        """åŸºäºè°ƒç”¨ä½ç½®æ¨æ–­ç›®çš„"""
        if position == 1:
            # é¦–æ¬¡è°ƒç”¨é€šå¸¸æ˜¯å·¥å…·é€‰æ‹©
            return "tool_selection"
        elif has_tool_calls:
            # åŒ…å«å·¥å…·è°ƒç”¨çš„å¯èƒ½æ˜¯å·¥å…·æ‰§è¡Œå¤„ç†
            return "tool_execution"
        else:
            # åç»­è°ƒç”¨é€šå¸¸æ˜¯ç»“æœç”Ÿæˆ
            return "result_generation"

    @classmethod
    def _analyze_intent_from_messages(cls, messages: List[Any], call_position: int = 1,
                                    has_tool_calls: bool = False, execution_context: str = "unknown") -> str:
        """ä»æ¶ˆæ¯ä¸­åˆ†æç”¨æˆ·æ„å›¾ - å¢å¼ºç‰ˆï¼Œæ”¯æŒæ‰§è¡Œé˜¶æ®µæ„ŸçŸ¥"""
        if not messages:
            return "unknown"

        # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        user_message = None
        for msg in reversed(messages):
            if hasattr(msg, 'type') and msg.type == 'human':
                user_message = msg
                break
            elif hasattr(msg, '__class__') and 'HumanMessage' in str(msg.__class__):
                user_message = msg
                break

        if not user_message or not hasattr(user_message, 'content'):
            return "unknown"

        content = str(user_message.content).lower()

        # åŸºäºæ‰§è¡Œé˜¶æ®µå’Œä¸Šä¸‹æ–‡çš„ç²¾ç»†åŒ–æ„å›¾åˆ†æ
        if execution_context == "tool_selection":
            # å·¥å…·é€‰æ‹©é˜¶æ®µï¼šæ›´æ³¨é‡æŸ¥è¯¢ç±»æ„å›¾
            if "é’“é±¼" in content and any(keyword in content for keyword in ["å¤©æ°”", "æ—¶é—´", "ä»€ä¹ˆæ—¶å€™", "æ˜å¤©", "åå¤©"]):
                return "weather_fishing_query"
            elif any(keyword in content for keyword in ["å¤©æ°”", "æ°”æ¸©", "ä¸‹é›¨", "æ™´å¤©"]):
                return "weather_query"
        elif execution_context == "result_generation":
            # ç»“æœç”Ÿæˆé˜¶æ®µï¼šæ›´æ³¨é‡åˆ†æå’Œå»ºè®®ç±»æ„å›¾
            if "é’“é±¼" in content:
                return "fishing_advice_generation"
            elif any(keyword in content for keyword in ["å»ºè®®", "æ¨è", "åˆ†æ"]):
                return "advice_generation"
        elif execution_context == "tool_execution":
            # å·¥å…·æ‰§è¡Œé˜¶æ®µï¼šæ•°æ®å¤„ç†ç±»æ„å›¾
            if "é’“é±¼" in content:
                return "fishing_data_processing"
            else:
                return "data_processing"

        # åŸºäºå…³é”®è¯åŒ¹é…æ„å›¾ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        for intent, keywords in cls.INTENT_KEYWORDS.items():
            if any(keyword in content for keyword in keywords):
                return intent

        # ç‰¹æ®Šæ£€æŸ¥ï¼šé’“é±¼ç›¸å…³æŸ¥è¯¢ï¼ˆåªåœ¨å…¶ä»–æ£€æŸ¥éƒ½æœªåŒ¹é…æ—¶ä½¿ç”¨ï¼‰
        if "é’“é±¼" in content:
            # é»˜è®¤æ ¹æ®æ‰§è¡Œä¸Šä¸‹æ–‡å†³å®šé’“é±¼æ„å›¾
            if execution_context == "result_generation":
                return "fishing_advice_generation"
            else:
                return "weather_fishing_query"

        return "general_conversation"

    @classmethod
    def _extract_key_points(cls, messages: List[Any], response: Any = None, compiled_patterns: Optional[Dict] = None) -> List[str]:
        """æå–å…³é”®ä¿¡æ¯ç‚¹ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒé¢„ç¼–è¯‘æ¨¡å¼"""
        key_points = []

        # ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–å…³é”®è¯
        for msg in messages:
            if hasattr(msg, 'type') and msg.type == 'human':
                content = str(msg.content)

                # ä½¿ç”¨é¢„ç¼–è¯‘æ¨¡å¼ï¼ˆå¦‚æœæä¾›ï¼‰
                if compiled_patterns:
                    # æ—¶é—´ç›¸å…³å…³é”®è¯
                    for word in compiled_patterns.get('time_words', []):
                        if word in content:
                            key_points.append(word)

                    # åœ°ç‚¹ç›¸å…³å…³é”®è¯ï¼ˆä½¿ç”¨é¢„ç¼–è¯‘æ­£åˆ™ï¼‰
                    location_regex = compiled_patterns.get('location')
                    if location_regex:
                        locations = location_regex.findall(content)
                        key_points.extend(locations)

                    # æ´»åŠ¨ç›¸å…³å…³é”®è¯
                    for word in compiled_patterns.get('activity_words', []):
                        if word in content:
                            key_points.append(word)
                else:
                    # å›é€€åˆ°åŸå§‹æ¨¡å¼
                    # æ—¶é—´ç›¸å…³å…³é”®è¯
                    time_words = ["æ˜å¤©", "åå¤©", "ä»Šå¤©", "æ—©ä¸Š", "ä¸Šåˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé—´"]
                    for word in time_words:
                        if word in content:
                            key_points.append(word)

                    # åœ°ç‚¹ç›¸å…³å…³é”®è¯
                    import re
                    location_pattern = r'(åŒ—äº¬|ä¸Šæµ·|å¹¿å·|æ·±åœ³|æ­å·|å—äº¬|è‹å·|æˆéƒ½|æ­¦æ±‰|è¥¿å®‰|éƒ‘å·|é’å²›|å¤§è¿|å¦é—¨|æ— é”¡|ç¦å·|æµå—|å“ˆå°”æ»¨|æ²ˆé˜³|é•¿æ˜¥|çŸ³å®¶åº„|å¤ªåŸ|å‘¼å’Œæµ©ç‰¹|é“¶å·|è¥¿å®|ä¹Œé²æœ¨é½|å…°å·|è¥¿å®‰|æˆéƒ½|è´µé˜³|æ˜†æ˜|å—å®|æ‹‰è¨|æ­å·|åˆè‚¥|å—æ˜Œ|é•¿æ²™|æ­¦æ±‰|éƒ‘å·|æµå—|é’å²›|å—äº¬|è‹å·|ä¸Šæµ·|ç¦å·|å¦é—¨|å°åŒ—|é¦™æ¸¯|æ¾³é—¨|å¤©æ´¥|é‡åº†|.[åŒºå¿])'
                    locations = re.findall(location_pattern, content)
                    key_points.extend(locations)

                    # æ´»åŠ¨ç›¸å…³å…³é”®è¯
                    activity_words = ["é’“é±¼", "å¤©æ°”", "æŸ¥è¯¢", "è®¡ç®—", "æœç´¢"]
                    for word in activity_words:
                        if word in content:
                            key_points.append(word)

        # å»é‡å¹¶é™åˆ¶æ•°é‡
        key_points = list(set(key_points))[:5]
        return key_points

    @classmethod
    def _generate_context_summary(cls, messages: List[Any], purpose: str, key_points: List[str]) -> str:
        """ç”Ÿæˆè°ƒç”¨ä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not messages:
            return "ç©ºæ¶ˆæ¯ä¸Šä¸‹æ–‡"

        # è·å–ç”¨æˆ·æ¶ˆæ¯å†…å®¹
        user_content = ""
        for msg in messages:
            if hasattr(msg, 'type') and msg.type == 'human':
                user_content = str(msg.content)[:100]  # é™åˆ¶é•¿åº¦
                break

        # ç”Ÿæˆæ‘˜è¦
        purpose_desc = cls.CALL_PURPOSES.get(purpose, purpose)

        if key_points:
            key_points_str = "ã€".join(key_points[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ªå…³é”®ç‚¹
            summary = f"{purpose_desc}ï¼Œå…³é”®ä¿¡æ¯ï¼š{key_points_str}"
        else:
            summary = f"{purpose_desc}ï¼Œç”¨æˆ·è¾“å…¥ï¼š{user_content}"

        return summary[:200]  # é™åˆ¶æ‘˜è¦é•¿åº¦


class SensitiveDataFilter:
    """æ•æ„Ÿæ•°æ®è¿‡æ»¤å™¨"""

    SENSITIVE_PATTERNS = [
        ('api_key', r'(?i)api[_-]?key["\']?\s*[:=]\s*["\']?([^"\'\\s]{10,})'),
        ('password', r'(?i)password["\']?\s*[:=]\s*["\']?([^"\'\\s]{6,})'),
        ('token', r'(?i)token["\']?\s*[:=]\s*["\']?([a-zA-Z0-9_-]{10,})'),
        ('secret', r'(?i)secret["\']?\s*[:=]\s*["\']?([a-zA-Z0-9_-]{10,})'),
    ]

    @classmethod
    def filter_text(cls, text: str) -> str:
        """è¿‡æ»¤æ–‡æœ¬ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        import re
        filtered_text = text
        for pattern_name, pattern in cls.SENSITIVE_PATTERNS:
            filtered_text = re.sub(pattern, f'{pattern_name}=***FILTERED***', filtered_text)
        return filtered_text

    @classmethod
    def filter_dict(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """é€’å½’è¿‡æ»¤å­—å…¸ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        if not isinstance(data, dict):
            return data

        filtered = {}
        for key, value in data.items():
            key_lower = str(key).lower()
            if any(sensitive in key_lower for sensitive in ['api_key', 'password', 'token', 'secret', 'key']):
                filtered[key] = "***FILTERED***"
            elif isinstance(value, dict):
                filtered[key] = cls.filter_dict(value)
            elif isinstance(value, str):
                filtered[key] = cls.filter_text(value)
            else:
                filtered[key] = value
        return filtered


class AgentLoggingMiddleware(AgentMiddleware):
    """
    LangChain æ™ºèƒ½ä½“æ—¥å¿—ä¸­é—´ä»¶

    æä¾›å®Œæ•´çš„æ™ºèƒ½ä½“æ‰§è¡Œæ—¥å¿—è®°å½•å’Œæ€§èƒ½ç›‘æ§åŠŸèƒ½ã€‚
    """

    def __init__(self, config: Optional[MiddlewareConfig] = None, logger: Optional[logging.Logger] = None):
        """
        åˆå§‹åŒ–æ—¥å¿—ä¸­é—´ä»¶

        Args:
            config: ä¸­é—´ä»¶é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            logger: è‡ªå®šä¹‰loggerï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤logger
        """
        self.config = config or default_config
        self.config.validate()

        # è®¾ç½®logger
        self.logger = logger or self._setup_logger()

        # ä¼šè¯ç®¡ç†
        self.session_id = self._generate_session_id()
        self.execution_start_time = None

        # æ¨¡å‹è°ƒç”¨è¿½è¸ª
        self.current_request_model_calls = 0  # å½“å‰è¯·æ±‚çš„æ¨¡å‹è°ƒç”¨æ¬¡æ•°
        self.total_model_calls = 0  # ä¼šè¯æ€»æ¨¡å‹è°ƒç”¨æ¬¡æ•°
        self.request_start_time = None  # å½“å‰è¯·æ±‚å¼€å§‹æ—¶é—´

        # æ‰§è¡Œç»Ÿè®¡
        self.metrics = AgentExecutionMetrics(
            session_id=self.session_id,
            timestamp=datetime.now().isoformat(),
            execution_id=str(uuid.uuid4())
        )

        # å·¥å…·è°ƒç”¨è®°å½•
        self.tool_calls: List[ToolCallRecord] = []

        # æ•æ„Ÿæ•°æ®è¿‡æ»¤å™¨
        self.sensitive_filter = SensitiveDataFilter() if self.config.enable_sensitive_filter else None

        # æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜æœºåˆ¶
        self._purpose_analysis_cache = {}
        self._max_cache_size = 100

        # æ€§èƒ½ä¼˜åŒ–ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self._compiled_patterns = {}
        if self.config.enable_call_purpose_analysis:
            self._compile_intent_patterns()

        # æ€§èƒ½è¿½è¸ªå™¨
        self.performance_tracker = PerformanceTracker()

        self.logger.info(f"ğŸ”§ AgentLoggingMiddleware åˆå§‹åŒ–å®Œæˆ (æ€§èƒ½å¢å¼ºç‰ˆ)", extra={
            'session_id': self.session_id,
            'config': self.config.to_dict(),
            'enhanced_features': {
                'call_purpose_analysis': self.config.enable_call_purpose_analysis,
                'enhanced_console_output': self.config.show_enhanced_console_output,
                'model_call_detail_level': self.config.model_call_detail_level,
                'performance_tracking': True,
                'extended_metrics': True
            }
        })

    def _compile_intent_patterns(self):
        """é¢„ç¼–è¯‘æ„å›¾è¯†åˆ«çš„æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½"""
        import re

        # é¢„ç¼–è¯‘åœ°ç‚¹è¯†åˆ«æ­£åˆ™
        location_pattern = r'(åŒ—äº¬|ä¸Šæµ·|å¹¿å·|æ·±åœ³|æ­å·|å—äº¬|è‹å·|æˆéƒ½|æ­¦æ±‰|è¥¿å®‰|éƒ‘å·|é’å²›|å¤§è¿|å¦é—¨|æ— é”¡|ç¦å·|æµå—|å“ˆå°”æ»¨|æ²ˆé˜³|é•¿æ˜¥|çŸ³å®¶åº„|å¤ªåŸ|å‘¼å’Œæµ©ç‰¹|é“¶å·|è¥¿å®|ä¹Œé²æœ¨é½|å…°å·|è¥¿å®‰|æˆéƒ½|è´µé˜³|æ˜†æ˜|å—å®|æ‹‰è¨|æ­å·|åˆè‚¥|å—æ˜Œ|é•¿æ²™|æ­¦æ±‰|éƒ‘å·|æµå—|é’å²›|å—äº¬|è‹å·|ä¸Šæµ·|ç¦å·|å¦é—¨|å°åŒ—|é¦™æ¸¯|æ¾³é—¨|å¤©æ´¥|é‡åº†|.[åŒºå¿])'
        self._compiled_patterns['location'] = re.compile(location_pattern)

        # é¢„ç¼–è¯‘æ—¶é—´è¯è¯†åˆ«åˆ—è¡¨
        time_words = ["æ˜å¤©", "åå¤©", "ä»Šå¤©", "æ—©ä¸Š", "ä¸Šåˆ", "ä¸‹åˆ", "æ™šä¸Š", "å¤œé—´"]
        self._compiled_patterns['time_words'] = time_words

        # é¢„ç¼–è¯‘æ´»åŠ¨è¯è¯†åˆ«åˆ—è¡¨
        activity_words = ["é’“é±¼", "å¤©æ°”", "æŸ¥è¯¢", "è®¡ç®—", "æœç´¢"]
        self._compiled_patterns['activity_words'] = activity_words

    def start_request_tracking(self, user_input: str = ""):
        """å¼€å§‹æ–°çš„è¯·æ±‚è¿½è¸ª"""
        import time

        self.request_start_time = time.time()
        self.current_request_model_calls = 0

        if self.config.show_enhanced_console_output and self.config.log_to_console:
            # æ˜¾ç¤ºè¯·æ±‚å¼€å§‹ä¿¡æ¯
            preview = user_input[:50] + "..." if len(user_input) > 50 else user_input
            print(f"\nğŸš€ æ–°è¯·æ±‚å¼€å§‹ [#{self.total_model_calls + 1}]")
            print(f"ğŸ“ ç”¨æˆ·è¾“å…¥: {preview}")
            print(f"â±ï¸  å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
            print(f"ğŸ¯ å†å²æ€»è°ƒç”¨: {self.total_model_calls} æ¬¡")
            print("â”€" * 80)

    def end_request_tracking(self):
        """ç»“æŸè¯·æ±‚è¿½è¸ªå¹¶æ˜¾ç¤ºç»Ÿè®¡"""
        import time

        if self.request_start_time and self.config.show_enhanced_console_output and self.config.log_to_console:
            request_duration = (time.time() - self.request_start_time) * 1000

            print("â”€" * 80)
            print(f"âœ… è¯·æ±‚å®Œæˆ | æœ¬æ¬¡è°ƒç”¨: {self.current_request_model_calls} æ¬¡ | ç´¯è®¡: {self.total_model_calls} æ¬¡")

            if self.current_request_model_calls > 0:
                avg_duration = request_duration / self.current_request_model_calls
                print(f"â±ï¸  æ€»è€—æ—¶: {request_duration:.1f}ms | å¹³å‡: {avg_duration:.1f}ms/æ¬¡")

                # æ˜¾ç¤ºè°ƒç”¨æ•ˆç‡è¯„çº§
                if self.current_request_model_calls == 1:
                    efficiency = "ğŸŸ¢ ä¼˜ç§€ (å•æ¬¡è°ƒç”¨)"
                elif self.current_request_model_calls <= 2:
                    efficiency = "ğŸŸ¡ è‰¯å¥½ (å¤šæ¬¡è°ƒç”¨)"
                else:
                    efficiency = "ğŸ”´ éœ€ä¼˜åŒ– (å¤šæ¬¡è°ƒç”¨)"
                print(f"ğŸ“Š æ•ˆç‡è¯„çº§: {efficiency}")

            print(f"ğŸ• å®Œæˆæ—¶é—´: {time.strftime('%H:%M:%S')}\n")

        self.request_start_time = None
        self.current_request_model_calls = 0

    def _get_purpose_analysis_cache_key(self, messages_str: str, call_position: int,
                                   has_tool_calls: bool, execution_context: str = "unknown") -> str:
        """ç”Ÿæˆç›®çš„åˆ†æç¼“å­˜é”® - å¢å¼ºç‰ˆï¼ŒåŒ…å«æ‰§è¡Œä¸Šä¸‹æ–‡å’Œæ¶ˆæ¯ç±»å‹"""
        import hashlib

        # è·å–æ›´è¯¦ç»†çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç”¨äºç¼“å­˜é”®
        try:
            # æå–æ¶ˆæ¯ç±»å‹åˆ†å¸ƒ
            message_types = []
            msg_count = 0

            # ç®€å•çš„æ¶ˆæ¯è§£ææ¥è·å–ç±»å‹ä¿¡æ¯
            if messages_str:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«AIå›å¤ã€å·¥å…·è°ƒç”¨ç­‰æ ‡è¯†
                if 'AIMessage' in messages_str or 'ai' in messages_str.lower():
                    message_types.append('ai')
                if 'HumanMessage' in messages_str or 'human' in messages_str.lower():
                    message_types.append('human')
                if 'ToolMessage' in messages_str or 'tool' in messages_str.lower():
                    message_types.append('tool')

                # è®¡ç®—æ¶ˆæ¯æ•°é‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
                msg_count = messages_str.count('content') or 1

            # æ„å»ºæ›´ç²¾ç¡®çš„ç¼“å­˜é”®
            context_info = {
                'content_hash': hashlib.md5(messages_str.encode()).hexdigest()[:8],
                'position': call_position,
                'has_tools': has_tool_calls,
                'context': execution_context,
                'msg_types': sorted(message_types),
                'msg_count': msg_count
            }

            # ç”Ÿæˆç¼“å­˜é”®
            cache_content = f"{context_info['content_hash']}_{context_info['position']}_{context_info['has_tools']}_{context_info['context']}_{context_info['msg_count']}_{'_'.join(context_info['msg_types'])}"

            return hashlib.md5(cache_content.encode()).hexdigest()[:16]

        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼Œå›é€€åˆ°ç®€å•ç‰ˆæœ¬
            content = f"{messages_str}_{call_position}_{has_tool_calls}_{execution_context}"
            return hashlib.md5(content.encode()).hexdigest()[:16]

    def _cache_purpose_analysis(self, cache_key: str, analysis: Dict[str, Any]):
        """ç¼“å­˜ç›®çš„åˆ†æç»“æœ"""
        if len(self._purpose_analysis_cache) >= self._max_cache_size:
            # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = next(iter(self._purpose_analysis_cache))
            del self._purpose_analysis_cache[oldest_key]

        self._purpose_analysis_cache[cache_key] = analysis

    def _get_cached_purpose_analysis(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„ç›®çš„åˆ†æç»“æœ"""
        return self._purpose_analysis_cache.get(cache_key)

    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®ä¸“ç”¨çš„agentæ—¥å¿—logger"""
        logger = logging.getLogger('agent.middleware')
        logger.setLevel(getattr(logging, self.config.log_level))

        # é¿å…é‡å¤æ·»åŠ handler
        if not logger.handlers:
            # åˆ›å»ºformatter
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )

            # æ§åˆ¶å°è¾“å‡º
            if self.config.log_to_console:
                console_handler = logging.StreamHandler()
                console_handler.setFormatter(formatter)
                logger.addHandler(console_handler)

            # æ–‡ä»¶è¾“å‡º
            if self.config.log_to_file:
                # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(self.config.log_file_path), exist_ok=True)
                file_handler = logging.FileHandler(self.config.log_file_path)
                file_handler.setFormatter(formatter)
                logger.addHandler(file_handler)

        return logger

    def _generate_session_id(self) -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        if self.config.session_id_generator == "uuid":
            return str(uuid.uuid4())
        elif self.config.session_id_generator == "timestamp":
            return str(int(time.time()))
        else:
            return f"session_{int(time.time())}"

    def _log_with_context(self, level: str, message: str, extra: Optional[Dict[str, Any]] = None):
        """å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—è®°å½•"""
        log_extra = {
            'session_id': self.session_id,
            'execution_id': self.metrics.execution_id,
            'component': 'AgentMiddleware'
        }

        if extra:
            log_extra.update(extra)

        # è¿‡æ»¤æ•æ„Ÿä¿¡æ¯
        if self.sensitive_filter and extra:
            log_extra = self.sensitive_filter.filter_dict(log_extra)

        # æˆªæ–­è¿‡é•¿çš„æ—¥å¿—
        if len(message) > self.config.max_log_length:
            message = message[:self.config.max_log_length] + "...[TRUNCATED]"

        getattr(self.logger, level.lower())(message, extra=log_extra)

    def _format_messages(self, messages: List[BaseMessage]) -> str:
        """æ ¼å¼åŒ–æ¶ˆæ¯åˆ—è¡¨"""
        if not messages:
            return "[]"

        formatted = []
        for msg in messages:
            msg_dict = {
                'type': msg.__class__.__name__,
                'content': str(msg.content)[:200] + ("..." if len(str(msg.content)) > 200 else "")
            }

            # æ·»åŠ å·¥å…·è°ƒç”¨ä¿¡æ¯
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                msg_dict['tool_calls'] = [
                    {
                        'name': tc.get('name', 'unknown'),
                        'args': tc.get('args', {})
                    }
                    for tc in msg.tool_calls
                ]

            formatted.append(msg_dict)

        return json.dumps(formatted, ensure_ascii=False, indent=2)

    def _extract_model_name(self, request: ModelRequest) -> str:
        """æå–æ¨¡å‹åç§°"""
        if hasattr(request, 'model') and request.model:
            if hasattr(request.model, 'model_name'):
                return request.model.model_name
            elif hasattr(request.model, 'name'):
                return request.model.name
            elif isinstance(request.model, str):
                return request.model
        return "unknown"

    def before_model(self, state: AgentState, runtime: Runtime) -> Optional[Dict[str, Any]]:
        """æ¨¡å‹è°ƒç”¨å‰çš„å¤„ç†"""
        if not self.execution_start_time:
            self.execution_start_time = time.time()

        self.metrics.model_calls_count += 1

        # è®°å½•è¾“å…¥ä¿¡æ¯
        messages = state.get('messages', [])

        self._log_with_context('INFO', "ğŸš€ å¼€å§‹æ¨¡å‹è°ƒç”¨", {
            'messages_count': len(messages),
            'messages_preview': self._format_messages(messages),
            'runtime_context': str(runtime.context) if hasattr(runtime, 'context') else None
        })

        return None

    def wrap_model_call(self, request: ModelRequest, handler: Callable) -> ModelResponse:
        """åŒ…è£…æ¨¡å‹è°ƒç”¨ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯å’Œè°ƒç”¨ç›®çš„åˆ†æï¼ˆæ€§èƒ½å¢å¼ºç‰ˆï¼‰"""
        # ç”Ÿæˆæ“ä½œIDç”¨äºæ€§èƒ½è¿½è¸ª
        operation_id = f"model_call_{self.metrics.model_calls_count + 1}_{int(time.time() * 1000)}"

        # å¼€å§‹æ€§èƒ½è¿½è¸ª
        request_start_time = time.time()
        self.performance_tracker.start_timing(operation_id, "model_call", {
            "model_name": self._extract_model_name(request),
            "call_position": self.metrics.model_calls_count + 1
        })

        self.metrics.model_name = self._extract_model_name(request)

        # è·å–è°ƒç”¨ä¿¡æ¯ç”¨äºç›®çš„åˆ†æ
        messages = getattr(request, 'messages', [])
        call_position = self.metrics.model_calls_count + 1  # è°ƒç”¨ä½ç½®ï¼ˆä»1å¼€å§‹ï¼‰

        try:
            # å¼€å§‹æ¨ç†é˜¶æ®µè®¡æ—¶
            inference_start_time = time.time()

            # æ‰§è¡Œæ¨¡å‹è°ƒç”¨
            response = handler(request)

            # ç»“æŸæ¨ç†é˜¶æ®µè®¡æ—¶
            inference_duration_ms = (time.time() - inference_start_time) * 1000

            # è®¡ç®—æ€»è€—æ—¶
            total_duration_ms = (time.time() - request_start_time) * 1000
            self.metrics.total_duration_ms += total_duration_ms

            # æå–tokenä½¿ç”¨ä¿¡æ¯ (å¢å¼ºå…¼å®¹æ€§)
            token_usage = {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}

            # å°è¯•å¤šç§æ–¹å¼è·å–Tokenä½¿ç”¨é‡
            token_extracted = False

            # æ–¹æ³•1: usage_metadata (æ ‡å‡†LangChain)
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                token_usage.update(response.usage_metadata)
                token_extracted = True
                self.metrics.token_usage.update(response.usage_metadata)

            # æ–¹æ³•2: response.usage (æŸäº›æ¨¡å‹æä¾›å•†)
            elif hasattr(response, 'usage') and response.usage:
                if hasattr(response.usage, 'prompt_tokens'):
                    token_usage["prompt_tokens"] = response.usage.prompt_tokens
                if hasattr(response.usage, 'completion_tokens'):
                    token_usage["completion_tokens"] = response.usage.completion_tokens
                if hasattr(response.usage, 'total_tokens'):
                    token_usage["total_tokens"] = response.usage.total_tokens
                token_extracted = True
                self.metrics.token_usage.update(token_usage)

            # æ–¹æ³•3: ä»responseå¯¹è±¡ä¸­ç›´æ¥æŸ¥æ‰¾ (å…¼å®¹æ›´å¤šæä¾›å•†)
            else:
                # å°è¯•ä»response.response_metadataä¸­æŸ¥æ‰¾
                if hasattr(response, 'response_metadata') and response.response_metadata:
                    if 'token_usage' in response.response_metadata:
                        token_usage.update(response.response_metadata['token_usage'])
                        token_extracted = True
                        self.metrics.token_usage.update(token_usage)
                    elif 'usage' in response.response_metadata:
                        token_usage.update(response.response_metadata['usage'])
                        token_extracted = True
                        self.metrics.token_usage.update(token_usage)

            # æ–¹æ³•4: ä¼°ç®—Tokenæ•°é‡ (åŸºäºæ–‡æœ¬é•¿åº¦)
            if not token_extracted:
                # å¦‚æœæ— æ³•è·å–Tokenï¼Œè¿›è¡Œç®€å•ä¼°ç®—
                messages = getattr(request, 'messages', [])
                if messages:
                    # ä¼°ç®—è¾“å…¥Token (ä¸­æ–‡çº¦1.5å­—ç¬¦=1Token, è‹±æ–‡çº¦4å­—ç¬¦=1Token)
                    input_text = ""
                    for msg in messages:
                        if hasattr(msg, 'content'):
                            input_text += str(msg.content) + " "

                    # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦ / 1.5 + è‹±æ–‡å•è¯ / 1
                    chinese_chars = len([c for c in input_text if '\u4e00' <= c <= '\u9fff'])
                    other_chars = len(input_text) - chinese_chars

                    estimated_input_tokens = int(chinese_chars / 1.5 + other_chars / 4)

                    # ä¼°ç®—è¾“å‡ºToken (å‡è®¾è¾“å‡ºé•¿åº¦ä¸è¾“å…¥ç›¸ä¼¼)
                    estimated_output_tokens = estimated_input_tokens // 3

                    token_usage = {
                        "prompt_tokens": estimated_input_tokens,
                        "completion_tokens": estimated_output_tokens,
                        "total_tokens": estimated_input_tokens + estimated_output_tokens,
                        "estimated": True  # æ ‡è®°è¿™æ˜¯ä¼°ç®—å€¼
                    }
                    self.metrics.token_usage.update(token_usage)

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨
            has_tool_calls = False
            if hasattr(response, 'tool_calls') and response.tool_calls:
                has_tool_calls = True

            # åˆ†æè°ƒç”¨ç›®çš„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            purpose_analysis = {}
            if self.config.enable_call_purpose_analysis:
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰æ„å›¾åˆ†æç»“æœï¼ˆæ¥è‡ªæ„å›¾ä¸­é—´ä»¶ï¼‰
                existing_intent_analysis = None
                if hasattr(request, 'metadata') and request.metadata:
                    existing_intent_analysis = request.metadata.get('intent_analysis')

                if existing_intent_analysis:
                    # ä½¿ç”¨å·²æœ‰çš„æ„å›¾åˆ†æç»“æœï¼Œé¿å…é‡å¤åˆ†æ
                    purpose_analysis = existing_intent_analysis
                else:
                    # æ¨æ–­æ‰§è¡Œä¸Šä¸‹æ–‡
                    execution_context = CallPurposeAnalyzer._infer_purpose_by_position(call_position, has_tool_calls)

                    # å°è¯•ä»ç¼“å­˜è·å–åˆ†æç»“æœ
                    messages_str = str([str(getattr(msg, 'content', '')) for msg in messages[-3:]])  # åªä½¿ç”¨æœ€è¿‘3æ¡æ¶ˆæ¯ç”Ÿæˆç¼“å­˜é”®
                    cache_key = self._get_purpose_analysis_cache_key(messages_str, call_position, has_tool_calls, execution_context)

                    cached_analysis = self._get_cached_purpose_analysis(cache_key)
                    if cached_analysis:
                        purpose_analysis = cached_analysis
                    else:
                        # æ‰§è¡Œåˆ†æå¹¶ç¼“å­˜ç»“æœ
                        purpose_analysis = CallPurposeAnalyzer.analyze_call_purpose(
                            messages=messages,
                            call_position=call_position,
                            has_tool_calls=has_tool_calls,
                            response=response,
                            compiled_patterns=self._compiled_patterns,
                            execution_context=execution_context
                        )
                        self._cache_purpose_analysis(cache_key, purpose_analysis)

            # åˆ›å»ºå¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡
            performance_metrics = PerformanceMetrics(
                request_duration_ms=total_duration_ms,
                inference_duration_ms=inference_duration_ms,
                response_duration_ms=total_duration_ms - inference_duration_ms
            )

            # æ·»åŠ è¯¦ç»†æ€§èƒ½æŒ‡æ ‡
            performance_metrics.add_metric("messages_count", len(messages), "count")
            performance_metrics.add_metric("tokens_per_second",
                                          (total_duration_ms > 0) and (token_usage.get("total_tokens", 0) / total_duration_ms * 1000) or 0,
                                          "rate", "tokens/sec")
            performance_metrics.add_metric("response_size", len(str(response)), "count", "chars")

            # è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ
            resource_usage = {
                "memory_usage_mb": 0,  # å¯ä»¥é›†æˆå®é™…çš„å†…å­˜ç›‘æ§
                "cpu_usage_percent": 0,  # å¯ä»¥é›†æˆå®é™…çš„CPUç›‘æ§
                "network_io_bytes": len(str(request)) + len(str(response))  # ç®€å•çš„ç½‘ç»œIOä¼°ç®—
            }

            # åˆ›å»ºå¢å¼ºçš„æ¨¡å‹è°ƒç”¨è®°å½•
            call_record = ModelCallRecord(
                call_id=call_position,
                timestamp=datetime.now().isoformat(),
                model_name=self.metrics.model_name,
                duration_ms=total_duration_ms,  # ä¿æŒå‘åå…¼å®¹
                token_usage=token_usage.copy(),
                success=True,
                call_purpose=purpose_analysis.get("call_purpose", "unknown"),
                intent_category=purpose_analysis.get("intent_category", ""),
                call_context_summary=purpose_analysis.get("context_summary", ""),
                key_points=purpose_analysis.get("key_points", []),
                inference_method=purpose_analysis.get("inference_method", "position_and_content_analysis"),
                performance_metrics=performance_metrics,
                resource_usage=resource_usage
            )

            # ç»“æŸæ€§èƒ½è¿½è¸ª
            self.performance_tracker.end_timing(operation_id)
            self.performance_tracker.increment_counter("model_calls_success")

            # æ·»åŠ åˆ°æŒ‡æ ‡ä¸­
            self.metrics.add_model_call(call_record)

            # è®°å½•å¢å¼ºçš„è¯·æ±‚ä¿¡æ¯
            self._log_enhanced_model_request(request, purpose_analysis)

            # è®°å½•å¢å¼ºçš„å“åº”ä¿¡æ¯
            self._log_enhanced_model_response(response, call_record, purpose_analysis)

            return response

        except Exception as e:
            self.metrics.errors_count += 1
            self.metrics.success = False
            error_duration_ms = (time.time() - request_start_time) * 1000

            # ç»“æŸæ€§èƒ½è¿½è¸ª
            self.performance_tracker.end_timing(operation_id)

            # å³ä½¿å¤±è´¥ä¹Ÿåˆ›å»ºè°ƒç”¨è®°å½•
            purpose_analysis = {}
            if self.config.enable_call_purpose_analysis:
                purpose_analysis = CallPurposeAnalyzer.analyze_call_purpose(
                    messages=messages,
                    call_position=call_position,
                    has_tool_calls=False,
                    response=None,
                    compiled_patterns=self._compiled_patterns
                )

            # åˆ›å»ºå¤±è´¥è®°å½•çš„æ€§èƒ½æŒ‡æ ‡
            error_performance_metrics = PerformanceMetrics(
                request_duration_ms=error_duration_ms,
                inference_duration_ms=error_duration_ms  # æ•´ä¸ªè¿‡ç¨‹éƒ½ç®—æ¨ç†æ—¶é—´
            )
            error_performance_metrics.add_metric("error_type", type(e).__name__, "custom")
            error_performance_metrics.add_metric("error_recovery", False, "boolean")

            error_call_record = ModelCallRecord(
                call_id=call_position,
                timestamp=datetime.now().isoformat(),
                model_name=self.metrics.model_name,
                duration_ms=error_duration_ms,
                token_usage=self.metrics.token_usage.copy(),
                success=False,
                call_purpose=purpose_analysis.get("call_purpose", "error_handling"),
                intent_category=purpose_analysis.get("intent_category", "error_recovery"),
                call_context_summary=purpose_analysis.get("context_summary", "æ¨¡å‹è°ƒç”¨å¤±è´¥"),
                key_points=purpose_analysis.get("key_points", []),
                inference_method=purpose_analysis.get("inference_method", "position_and_content_analysis"),
                error_message=str(e),
                performance_metrics=error_performance_metrics
            )

            self.metrics.add_model_call(error_call_record)

            # è®°å½•é”™è¯¯ä¿¡æ¯
            self._log_with_context('ERROR', f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}", {
                'duration_ms': round(error_duration_ms, 2),
                'error_type': type(e).__name__,
                'error_details': str(e),
                'call_purpose': error_call_record.call_purpose,
                'call_id': call_position
            })

            raise

    def _log_enhanced_model_request(self, request: ModelRequest, purpose_analysis: Dict[str, str]):
        """è®°å½•å¢å¼ºçš„æ¨¡å‹è¯·æ±‚ä¿¡æ¯"""
        messages = getattr(request, 'messages', [])

        # æ„å»ºæ˜“è¯»çš„æ§åˆ¶å°è¾“å‡ºï¼ˆå¦‚æœå¯ç”¨å¢å¼ºè¾“å‡ºï¼‰
        if self.config.show_enhanced_console_output and purpose_analysis:
            purpose_desc = CallPurposeAnalyzer.CALL_PURPOSES.get(
                purpose_analysis.get("call_purpose", "unknown"),
                "æœªçŸ¥ç›®çš„"
            )

            key_points = purpose_analysis.get("key_points", [])
            key_points_str = "ã€".join(key_points[:3]) if key_points else "æ— "

            console_msg = f"ğŸ¤– æ¨¡å‹è°ƒç”¨ #{self.metrics.model_calls_count + 1} [{purpose_desc}]"

            if self.config.log_to_console:
                print(f"\n{console_msg}")
                print(f"â”œâ”€â”€ ç›®çš„: {purpose_desc}")
                print(f"â”œâ”€â”€ æ„å›¾: {CallPurposeAnalyzer.INTENT_CATEGORIES.get(purpose_analysis.get('intent_category', ''), purpose_analysis.get('intent_category', ''))}")
                if key_points:
                    print(f"â”œâ”€â”€ å…³é”®ç‚¹: [{key_points_str}]")
                print(f"â””â”€â”€ æ¨¡å‹: {self.metrics.model_name}")

        # è®°å½•è°ƒè¯•ä¿¡æ¯
        self._log_with_context('DEBUG', "ğŸ“¤ æ¨¡å‹è¯·æ±‚è¯¦æƒ…", {
            'call_id': self.metrics.model_calls_count + 1,
            'model': self.metrics.model_name,
            'tools_count': len(getattr(request, 'tools', [])),
            'messages_count': len(messages),
            'call_purpose': purpose_analysis.get("call_purpose"),
            'intent_category': purpose_analysis.get("intent_category"),
            'key_points': key_points,
            'context_summary': purpose_analysis.get("context_summary", "")[:100]
        })

    def _log_enhanced_model_response(self, response: ModelResponse, call_record: ModelCallRecord, purpose_analysis: Dict[str, str]):
        """è®°å½•å¢å¼ºçš„æ¨¡å‹å“åº”ä¿¡æ¯"""

        # æ„å»ºæ˜“è¯»çš„æ§åˆ¶å°è¾“å‡ºï¼ˆå¦‚æœå¯ç”¨å¢å¼ºè¾“å‡ºï¼‰
        if self.config.show_enhanced_console_output:
            purpose_desc = CallPurposeAnalyzer.CALL_PURPOSES.get(call_record.call_purpose, call_record.call_purpose)

            # è·å–è¯¦ç»†æ€§èƒ½ä¿¡æ¯
            perf_metrics = call_record.performance_metrics
            total_duration = perf_metrics.get_total_duration()
            inference_duration = perf_metrics.inference_duration_ms
            tokens_per_sec = call_record._calculate_tokens_per_second()

            if self.config.log_to_console:
                # æ›´æ–°è°ƒç”¨è®¡æ•°
                self.current_request_model_calls += 1
                self.total_model_calls += 1

                # é€‰æ‹©åˆé€‚çš„emoji
                if call_record.call_purpose == "tool_selection":
                    emoji = "ğŸ¯"
                elif call_record.call_purpose == "result_generation":
                    emoji = "âœ¨"
                elif call_record.call_purpose == "tool_execution":
                    emoji = "âš™ï¸"
                else:
                    emoji = "âš¡"

                # æ˜¾ç¤ºè°ƒç”¨æ¬¡æ•°å’Œæ€§èƒ½ä¿¡æ¯
                call_info = f"ç¬¬{self.total_model_calls}æ¬¡"
                if self.current_request_model_calls > 1:
                    call_info += f" (æœ¬æ¬¡ç¬¬{self.current_request_model_calls}æ¬¡)"

                # æ£€æŸ¥Tokenæ˜¯å¦ä¸ºä¼°ç®—å€¼
                total_tokens = call_record.token_usage.get('total_tokens', 0)
                is_estimated = call_record.token_usage.get('estimated', False)
                token_display = f"{total_tokens}{' (ä¼°ç®—)' if is_estimated else ''}"

                print(f"{emoji} æ¨¡å‹è°ƒç”¨[{call_info}]: {total_duration:.1f}ms | Tokens: {token_display} | é€Ÿç‡: {tokens_per_sec:.1f} t/s")

                # æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½åˆ†è§£
                if total_duration > 100:  # åªä¸ºè¾ƒæ…¢çš„è°ƒç”¨æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                    print(f"â”œâ”€â”€ æ¨ç†: {inference_duration:.1f}ms | å“åº”: {perf_metrics.response_duration_ms:.1f}ms")
                    if perf_metrics.custom_metrics.get("tokens_per_second"):
                        print(f"â”œâ”€â”€ æ•ˆç‡: {tokens_per_sec:.1f} tokens/sec | {call_record._calculate_ms_per_token():.2f} ms/token")

                    if call_record.resource_usage.get("network_io_bytes", 0) > 0:
                        network_kb = call_record.resource_usage["network_io_bytes"] / 1024
                        print(f"â”œâ”€â”€ ç½‘ç»œ: {network_kb:.1f}KB")

                if call_record.key_points:
                    print(f"â””â”€â”€ æ‘˜è¦: {call_record.call_context_summary[:80]}...")

        # è®°å½•å®Œæ•´çš„å“åº”ä¿¡æ¯ï¼ˆåŒ…å«è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ï¼‰
        self._log_with_context('INFO', "ğŸ“¥ æ¨¡å‹å“åº”è¯¦æƒ…", {
            'call_id': call_record.call_id,
            'call_purpose': call_record.call_purpose,
            'purpose_desc': purpose_desc,
            'basic_duration_ms': round(call_record.duration_ms, 2),
            'detailed_performance': call_record.performance_metrics.to_dict(),
            'token_usage': call_record.token_usage,
            'token_efficiency': {
                'tokens_per_second': call_record._calculate_tokens_per_second(),
                'ms_per_token': call_record._calculate_ms_per_token()
            },
            'intent_category': call_record.intent_category,
            'key_points': call_record.key_points,
            'context_summary': call_record.call_context_summary,
            'resource_usage': call_record.resource_usage,
            'success': call_record.success,
            'response_preview': str(response)[:200] + "..." if len(str(response)) > 200 else str(response)
        })

    def wrap_tool_call(self, request, handler) -> Any:
        """åŒ…è£…å·¥å…·è°ƒç”¨ï¼Œè®°å½•å·¥å…·æ‰§è¡Œè¯¦æƒ…ï¼ˆæ€§èƒ½å¢å¼ºç‰ˆï¼‰"""
        if not self.config.enable_tool_tracking:
            return handler(request)

        # æå–å·¥å…·ä¿¡æ¯
        tool_name = "unknown"
        tool_args = {}

        if hasattr(request, 'tool_call'):
            tool_call = request.tool_call
            tool_name = tool_call.get('name', 'unknown')
            tool_args = tool_call.get('args', {})
        elif hasattr(request, 'name'):
            tool_name = request.name
            tool_args = getattr(request, 'args', {})

        # ç”Ÿæˆæ“ä½œIDç”¨äºæ€§èƒ½è¿½è¸ª
        tool_operation_id = f"tool_{tool_name}_{int(time.time() * 1000)}"

        # å¼€å§‹æ€§èƒ½è¿½è¸ª
        self.performance_tracker.start_timing(tool_operation_id, "tool_call", {
            "tool_name": tool_name,
            "args_count": len(tool_args) if isinstance(tool_args, dict) else 0
        })

        # å¼€å§‹å„é˜¶æ®µè®¡æ—¶
        tool_start_time = time.time()

        self.metrics.tool_calls_count += 1

        try:
            # è®°å½•å·¥å…·è°ƒç”¨å¼€å§‹
            self._log_with_context('INFO', f"ğŸ”§ å¼€å§‹å·¥å…·è°ƒç”¨: {tool_name}", {
                'tool_name': tool_name,
                'tool_args': tool_args,
                'args_count': len(tool_args) if isinstance(tool_args, dict) else 0
            })

            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            result = handler(request)

            # è®¡ç®—æ€»è€—æ—¶
            total_duration_ms = (time.time() - tool_start_time) * 1000

            # åˆ›å»ºå·¥å…·æ€§èƒ½æŒ‡æ ‡
            tool_performance_metrics = PerformanceMetrics(
                request_duration_ms=total_duration_ms,
                inference_duration_ms=total_duration_ms,  # å·¥å…·æ‰§è¡Œæ—¶é—´ä½œä¸ºä¸»è¦è€—æ—¶
                response_duration_ms=0  # å·¥å…·é€šå¸¸æ²¡æœ‰ç‹¬ç«‹çš„å“åº”ç”Ÿæˆé˜¶æ®µ
            )

            # æ·»åŠ å·¥å…·ç‰¹å®šçš„æ€§èƒ½æŒ‡æ ‡
            tool_performance_metrics.add_metric("execution_duration_ms", total_duration_ms, "timing", "ms")
            tool_performance_metrics.add_metric("args_count", len(tool_args) if isinstance(tool_args, dict) else 0, "count")
            tool_performance_metrics.add_metric("result_size", len(str(result)), "count", "chars")

            # è®°å½•ç¼“å­˜å‘½ä¸­çŠ¶æ€ï¼ˆå¦‚æœå¯ä»¥æ£€æµ‹ï¼‰
            cache_hit = self._detect_cache_hit(tool_name, tool_args, result)
            if cache_hit is not None:
                tool_performance_metrics.add_metric("cache_hit", cache_hit, "boolean")

            # è®°å½•èµ„æºä½¿ç”¨æƒ…å†µ
            resource_usage = {
                "input_size_bytes": len(str(tool_args)),
                "output_size_bytes": len(str(result)),
                "total_io_bytes": len(str(tool_args)) + len(str(result))
            }

            # è®°å½•å·¥å…·è°ƒç”¨æˆåŠŸ
            tool_record = ToolCallRecord(
                tool_name=tool_name,
                tool_args=tool_args,
                result=result,
                duration_ms=total_duration_ms,
                success=True,
                performance_metrics=tool_performance_metrics,
                operation_phases={
                    "execution": total_duration_ms,
                    "total": total_duration_ms
                },
                cache_hit=cache_hit or False,
                resource_usage=resource_usage
            )
            self.tool_calls.append(tool_record)

            # ç»“æŸæ€§èƒ½è¿½è¸ª
            self.performance_tracker.end_timing(tool_operation_id)
            self.performance_tracker.increment_counter("tool_calls_success")
            if cache_hit:
                self.performance_tracker.increment_counter("tool_cache_hits")

            # è®°å½•è¯¦ç»†æ€§èƒ½ä¿¡æ¯
            self._log_with_context('INFO', f"âœ… å·¥å…·è°ƒç”¨å®Œæˆ: {tool_name}", {
                'tool_name': tool_name,
                'duration_ms': round(total_duration_ms, 2),
                'performance_breakdown': tool_record.get_detailed_performance(),
                'cache_hit': cache_hit,
                'result_preview': str(result)[:200] + "..." if len(str(result)) > 200 else str(result)
            })

            return result

        except Exception as e:
            self.metrics.errors_count += 1
            duration_ms = (time.time() - tool_start_time) * 1000

            # è®°å½•å·¥å…·è°ƒç”¨å¤±è´¥
            tool_record = ToolCallRecord(
                tool_name=tool_name,
                tool_args=tool_args,
                result=None,
                duration_ms=duration_ms,
                success=False,
                error_message=str(e)
            )
            self.tool_calls.append(tool_record)

            self._log_with_context('ERROR', f"âŒ å·¥å…·è°ƒç”¨å¤±è´¥: {tool_name}", {
                'tool_name': tool_name,
                'duration_ms': round(duration_ms, 2),
                'error_type': type(e).__name__,
                'error_message': str(e)
            })

            raise

    def _detect_cache_hit(self, tool_name: str, tool_args: Dict[str, Any], result: Any) -> Optional[bool]:
        """
        æ£€æµ‹å·¥å…·è°ƒç”¨æ˜¯å¦å‘½ä¸­ç¼“å­˜

        è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å¯å‘å¼å®ç°ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œæ‰©å±•
        """
        try:
            # å¯¹äºå¤©æ°”æŸ¥è¯¢å·¥å…·ï¼Œæ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«ç¼“å­˜æ ‡è¯†
            if tool_name in ['query_current_weather', 'query_weather_by_date', 'query_fishing_recommendation']:
                result_str = str(result).lower()
                # ç®€å•çš„ç¼“å­˜æ£€æµ‹é€»è¾‘
                cache_indicators = ['cache', 'cached', 'from cache', 'ç¼“å­˜']
                return any(indicator in result_str for indicator in cache_indicators)

            # å¯¹äºåæ ‡æŸ¥è¯¢å·¥å…·ï¼Œæ£€æŸ¥æ˜¯å¦å¿«é€Ÿè¿”å›ï¼ˆé€šå¸¸è¡¨ç¤ºç¼“å­˜å‘½ä¸­ï¼‰
            elif tool_name in ['get_coordinate']:
                total_duration_ms = (time.time() - time.time())  # è¿™ä¼šåœ¨åé¢é‡ç½®
                # å¦‚æœè€—æ—¶éå¸¸çŸ­ï¼Œå¯èƒ½æ˜¯ç¼“å­˜å‘½ä¸­
                return False  # éœ€è¦å®é™…çš„ç¼“å­˜æœºåˆ¶æ¥æ”¯æŒ

            return None
        except Exception:
            return None

    def get_performance_summary(self) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½ç»Ÿè®¡æ‘˜è¦

        Returns:
            åŒ…å«è¯¦ç»†æ€§èƒ½ç»Ÿè®¡çš„å­—å…¸
        """
        # åŸºç¡€ç»Ÿè®¡
        basic_summary = {
            "session_id": self.session_id,
            "total_model_calls": self.metrics.model_calls_count,
            "total_tool_calls": self.metrics.tool_calls_count,
            "total_duration_ms": self.metrics.total_duration_ms,
            "total_errors": self.metrics.errors_count,
            "success_rate": (self.metrics.model_calls_count - self.metrics.errors_count) / max(self.metrics.model_calls_count, 1)
        }

        # æ¨¡å‹è°ƒç”¨æ€§èƒ½ç»Ÿè®¡
        model_calls_summary = self.metrics.get_model_calls_summary()

        # æ€§èƒ½è¿½è¸ªå™¨ç»Ÿè®¡
        tracker_summary = self.performance_tracker.get_metrics_summary()

        # å·¥å…·è°ƒç”¨æ€§èƒ½ç»Ÿè®¡
        tool_performance = {}
        total_tool_duration = 0
        cache_hits = 0
        if self.tool_calls:
            for tool_call in self.tool_calls:
                tool_name = tool_call.tool_name
                if tool_name not in tool_performance:
                    tool_performance[tool_name] = {
                        "count": 0,
                        "total_duration_ms": 0,
                        "avg_duration_ms": 0,
                        "cache_hits": 0,
                        "success_rate": 0
                    }

                stats = tool_performance[tool_name]
                stats["count"] += 1
                stats["total_duration_ms"] += tool_call.duration_ms
                if tool_call.cache_hit:
                    stats["cache_hits"] += 1
                if tool_call.success:
                    stats["success_rate"] += 1

        # è®¡ç®—æ€»å’Œ
        total_tool_duration = sum(tc.duration_ms for tc in self.tool_calls)
        cache_hits = sum(1 for tc in self.tool_calls if tc.cache_hit)

        # è®¡ç®—å¹³å‡å€¼å’ŒæˆåŠŸç‡
        for stats in tool_performance.values():
            if stats["count"] > 0:
                stats["avg_duration_ms"] = stats["total_duration_ms"] / stats["count"]
                stats["success_rate"] = (stats["success_rate"] / stats["count"]) * 100

        # æ•´åˆæ‘˜è¦
        return {
            **basic_summary,
            "model_performance": model_calls_summary,
            "tool_performance": tool_performance,
            "tracker_performance": tracker_summary,
            "performance_metrics": {
                "avg_model_call_duration": model_calls_summary.get("average_duration_ms", 0),
                "total_tool_duration_ms": total_tool_duration,
                "cache_hit_rate": (cache_hits / max(len(self.tool_calls), 1)) * 100 if self.tool_calls else 0,
                "error_rate": (self.metrics.errors_count / max(self.metrics.model_calls_count + self.metrics.tool_calls_count, 1)) * 100
            }
        }

    def after_model(self, state: AgentState, runtime: Runtime) -> Optional[Dict[str, Any]]:
        """æ¨¡å‹è°ƒç”¨åçš„å¤„ç†"""
        # æ›´æ–°æœ€ç»ˆæŒ‡æ ‡
        if self.execution_start_time:
            self.metrics.total_duration_ms = (time.time() - self.execution_start_time) * 1000

        # è®°å½•æ‰§è¡Œå®Œæˆ
        self._log_with_context('INFO', "ğŸ æ¨¡å‹è°ƒç”¨å®Œæˆ", {
            'final_metrics': asdict(self.metrics),
            'tool_calls_summary': [
                {
                    'tool_name': tc.tool_name,
                    'duration_ms': round(tc.duration_ms, 2),
                    'success': tc.success
                }
                for tc in self.tool_calls[-5:]  # åªæ˜¾ç¤ºæœ€è¿‘5ä¸ªå·¥å…·è°ƒç”¨
            ]
        })

        return None

    def get_execution_summary(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        return {
            'session_id': self.session_id,
            'execution_id': self.metrics.execution_id,
            'metrics': asdict(self.metrics),
            'tool_calls': [asdict(tc) for tc in self.tool_calls],
            'timestamp': datetime.now().isoformat()
        }

    def reset_metrics(self):
        """é‡ç½®æ‰§è¡ŒæŒ‡æ ‡"""
        self.metrics = AgentExecutionMetrics(
            session_id=self.session_id,
            timestamp=datetime.now().isoformat(),
            execution_id=str(uuid.uuid4())
        )
        self.tool_calls = []
        self.execution_start_time = None